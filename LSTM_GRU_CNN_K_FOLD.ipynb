{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppressing Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "import string\n",
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, LSTM, Embedding, Flatten, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv1D,GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import random as python_random\n",
    "np.random.seed(123) \n",
    "python_random.seed(123)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>product_category</th>\n",
       "      <th>description</th>\n",
       "      <th>product_category_wide</th>\n",
       "      <th>mrp</th>\n",
       "      <th>price</th>\n",
       "      <th>%discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"I DO\" Signature Lace Cheeky Hipster with Gift...</td>\n",
       "      <td>hanky panky</td>\n",
       "      <td>Collections</td>\n",
       "      <td>With \"I DO\" emblazoned on this Hanky Panky lac...</td>\n",
       "      <td>panties</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I DO\" Signature Lace Cheeky Hipster with Gift...</td>\n",
       "      <td>hanky panky</td>\n",
       "      <td>Panties</td>\n",
       "      <td>With \"I DO\" emblazoned on this Hanky Panky lac...</td>\n",
       "      <td>panties</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'After Midnight' Lace Open Gusset G-String</td>\n",
       "      <td>hanky panky</td>\n",
       "      <td>Bridal Lingerie</td>\n",
       "      <td>Stretch signature lace fashions an alluring do...</td>\n",
       "      <td>other</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'After Midnight' Lace Open Gusset G-String</td>\n",
       "      <td>hanky panky</td>\n",
       "      <td>Sexy Lingerie</td>\n",
       "      <td>Stretch signature lace fashions an alluring do...</td>\n",
       "      <td>other</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'After Midnight' Lace Open Gusset G-String</td>\n",
       "      <td>hanky panky</td>\n",
       "      <td>Women's Panties</td>\n",
       "      <td>Stretch signature lace fashions an alluring do...</td>\n",
       "      <td>other</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        product_name   brand_name product_category                                        description product_category_wide   mrp  price  %discount\n",
       "0  \"I DO\" Signature Lace Cheeky Hipster with Gift...  hanky panky      Collections  With \"I DO\" emblazoned on this Hanky Panky lac...               panties  40.0   40.0        0.0\n",
       "1  \"I DO\" Signature Lace Cheeky Hipster with Gift...  hanky panky          Panties  With \"I DO\" emblazoned on this Hanky Panky lac...               panties  40.0   40.0        0.0\n",
       "2         'After Midnight' Lace Open Gusset G-String  hanky panky  Bridal Lingerie  Stretch signature lace fashions an alluring do...                 other  27.0   27.0        0.0\n",
       "3         'After Midnight' Lace Open Gusset G-String  hanky panky    Sexy Lingerie  Stretch signature lace fashions an alluring do...                 other  27.0   27.0        0.0\n",
       "4         'After Midnight' Lace Open Gusset G-String  hanky panky  Women's Panties  Stretch signature lace fashions an alluring do...                 other  27.0   27.0        0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_data = pd.read_csv('Dataset/Intermediate/all_products.csv')\n",
    "products_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>product_category</th>\n",
       "      <th>description</th>\n",
       "      <th>product_category_wide</th>\n",
       "      <th>mrp</th>\n",
       "      <th>price</th>\n",
       "      <th>%discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"i do\" signature lace cheeky hipster with gift...</td>\n",
       "      <td>hanky panky</td>\n",
       "      <td>Collections</td>\n",
       "      <td>with \"i do\" emblazoned on this hanky panky lac...</td>\n",
       "      <td>panties</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"i do\" signature lace cheeky hipster with gift...</td>\n",
       "      <td>hanky panky</td>\n",
       "      <td>Panties</td>\n",
       "      <td>with \"i do\" emblazoned on this hanky panky lac...</td>\n",
       "      <td>panties</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'after midnight' lace open gusset g-string</td>\n",
       "      <td>hanky panky</td>\n",
       "      <td>Bridal Lingerie</td>\n",
       "      <td>stretch signature lace fashions an alluring do...</td>\n",
       "      <td>other</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'after midnight' lace open gusset g-string</td>\n",
       "      <td>hanky panky</td>\n",
       "      <td>Sexy Lingerie</td>\n",
       "      <td>stretch signature lace fashions an alluring do...</td>\n",
       "      <td>other</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'after midnight' lace open gusset g-string</td>\n",
       "      <td>hanky panky</td>\n",
       "      <td>Women's Panties</td>\n",
       "      <td>stretch signature lace fashions an alluring do...</td>\n",
       "      <td>other</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        product_name   brand_name product_category                                        description product_category_wide   mrp  price  %discount\n",
       "0  \"i do\" signature lace cheeky hipster with gift...  hanky panky      Collections  with \"i do\" emblazoned on this hanky panky lac...               panties  40.0   40.0        0.0\n",
       "1  \"i do\" signature lace cheeky hipster with gift...  hanky panky          Panties  with \"i do\" emblazoned on this hanky panky lac...               panties  40.0   40.0        0.0\n",
       "2         'after midnight' lace open gusset g-string  hanky panky  Bridal Lingerie  stretch signature lace fashions an alluring do...                 other  27.0   27.0        0.0\n",
       "3         'after midnight' lace open gusset g-string  hanky panky    Sexy Lingerie  stretch signature lace fashions an alluring do...                 other  27.0   27.0        0.0\n",
       "4         'after midnight' lace open gusset g-string  hanky panky  Women's Panties  stretch signature lace fashions an alluring do...                 other  27.0   27.0        0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_data['description'] = products_data['description'].str.lower()\n",
    "products_data['product_name'] = products_data['product_name'].str.lower()\n",
    "products_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5982, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3446, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>product_category_wide</th>\n",
       "      <th>description</th>\n",
       "      <th>mrp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"i do\" signature lace cheeky hipster with gift...</td>\n",
       "      <td>hanky panky</td>\n",
       "      <td>panties</td>\n",
       "      <td>with \"i do\" emblazoned on this hanky panky lac...</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'after midnight' lace open gusset g-string</td>\n",
       "      <td>hanky panky</td>\n",
       "      <td>other</td>\n",
       "      <td>stretch signature lace fashions an alluring do...</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'after midnight' open gusset lace thong</td>\n",
       "      <td>hanky panky</td>\n",
       "      <td>panties</td>\n",
       "      <td>leopard-spotted lace adds to the bold open-gus...</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'annabelle' lace camisole</td>\n",
       "      <td>hanky panky</td>\n",
       "      <td>camisoles</td>\n",
       "      <td>scalloped trim adorned with a little blue bow ...</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'annabelle' lace garter</td>\n",
       "      <td>hanky panky</td>\n",
       "      <td>other</td>\n",
       "      <td>scalloped lace adorned with a little blue bow ...</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        product_name   brand_name product_category_wide                                        description   mrp\n",
       "0  \"i do\" signature lace cheeky hipster with gift...  hanky panky               panties  with \"i do\" emblazoned on this hanky panky lac...  40.0\n",
       "2         'after midnight' lace open gusset g-string  hanky panky                 other  stretch signature lace fashions an alluring do...  27.0\n",
       "5            'after midnight' open gusset lace thong  hanky panky               panties  leopard-spotted lace adds to the bold open-gus...  24.0\n",
       "8                          'annabelle' lace camisole  hanky panky             camisoles  scalloped trim adorned with a little blue bow ...  64.0\n",
       "9                            'annabelle' lace garter  hanky panky                 other  scalloped lace adorned with a little blue bow ...  23.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_data = products_data[['product_name', 'brand_name', 'product_category_wide', 'description', 'mrp']]\n",
    "products_data.drop_duplicates(inplace = True)\n",
    "print(products_data.shape)\n",
    "products_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords removal\n",
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \n",
    "             \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\",\n",
    "             \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \n",
    "             \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\",\n",
    "             \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\",\n",
    "             \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \n",
    "             \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\",\n",
    "             \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n",
    "             \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\",\n",
    "             \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\",\n",
    "             \"your\", \"yours\", \"yourself\", \"yourselves\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>product_category_wide</th>\n",
       "      <th>description</th>\n",
       "      <th>mrp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"i do\" signature lace cheeky hipster with gift...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>with \"i do\" emblazoned on this hanky panky lac...</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'after midnight' lace open gusset g-string</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>stretch signature lace fashions an alluring do...</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'after midnight' open gusset lace thong</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>leopard-spotted lace adds to the bold open-gus...</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'annabelle' lace camisole</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>scalloped trim adorned with a little blue bow ...</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'annabelle' lace garter</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>scalloped lace adorned with a little blue bow ...</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        product_name  brand_name  product_category_wide                                        description   mrp\n",
       "0  \"i do\" signature lace cheeky hipster with gift...           6                      7  with \"i do\" emblazoned on this hanky panky lac...  40.0\n",
       "2         'after midnight' lace open gusset g-string           6                      6  stretch signature lace fashions an alluring do...  27.0\n",
       "5            'after midnight' open gusset lace thong           6                      7  leopard-spotted lace adds to the bold open-gus...  24.0\n",
       "8                          'annabelle' lace camisole           6                      5  scalloped trim adorned with a little blue bow ...  64.0\n",
       "9                            'annabelle' lace garter           6                      6  scalloped lace adorned with a little blue bow ...  23.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(np.hstack([products_data.product_category_wide]))\n",
    "products_data.product_category_wide = le.transform(products_data.product_category_wide)\n",
    "\n",
    "le.fit(np.hstack([products_data.brand_name]))\n",
    "products_data.brand_name = le.transform(products_data.brand_name)\n",
    "del le\n",
    "\n",
    "products_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>product_category_wide</th>\n",
       "      <th>mrp</th>\n",
       "      <th>clean_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"i do\" signature lace cheeky hipster with gift...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>40.0</td>\n",
       "      <td>i do  emblazoned hanky panky lace hipster swa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'after midnight' lace open gusset g-string</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>27.0</td>\n",
       "      <td>stretch signature lace fashions alluring doubl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'after midnight' open gusset lace thong</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>24.0</td>\n",
       "      <td>leopard spotted lace adds bold open gusset des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'annabelle' lace camisole</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>64.0</td>\n",
       "      <td>scalloped trim adorned little blue bow flirts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'annabelle' lace garter</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>scalloped lace adorned little blue bow defines...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        product_name  brand_name  product_category_wide   mrp                                  clean_description\n",
       "0  \"i do\" signature lace cheeky hipster with gift...           6                      7  40.0   i do  emblazoned hanky panky lace hipster swa...\n",
       "2         'after midnight' lace open gusset g-string           6                      6  27.0  stretch signature lace fashions alluring doubl...\n",
       "5            'after midnight' open gusset lace thong           6                      7  24.0  leopard spotted lace adds bold open gusset des...\n",
       "8                          'annabelle' lace camisole           6                      5  64.0  scalloped trim adorned little blue bow flirts ...\n",
       "9                            'annabelle' lace garter           6                      6  23.0  scalloped lace adorned little blue bow defines..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(data):\n",
    "  data['clean_description'] = data['description'].apply(lambda x : ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "  return data\n",
    "\n",
    "data_cleaned = remove_stopwords(products_data)\n",
    "data_cleaned['clean_description'] = data_cleaned['clean_description'].str.replace('[{}]'.format(string.punctuation), ' ')\n",
    "data_cleaned.drop('description', inplace = True, axis = 1)\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Value of Sequences:  91 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>product_category_wide</th>\n",
       "      <th>mrp</th>\n",
       "      <th>clean_description</th>\n",
       "      <th>seq_description</th>\n",
       "      <th>seq_product_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"i do\" signature lace cheeky hipster with gift...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>40.0</td>\n",
       "      <td>i do  emblazoned hanky panky lace hipster swa...</td>\n",
       "      <td>[540, 541, 1760, 83, 90, 1, 98, 1201, 557, 296...</td>\n",
       "      <td>[540, 541, 70, 1, 175, 98, 283, 439, 561]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'after midnight' lace open gusset g-string</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>27.0</td>\n",
       "      <td>stretch signature lace fashions alluring doubl...</td>\n",
       "      <td>[34, 70, 1, 1035, 612, 542, 174, 415, 246, 109...</td>\n",
       "      <td>[2569, 2570, 1, 211, 397, 415, 246]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'after midnight' open gusset lace thong</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>24.0</td>\n",
       "      <td>leopard spotted lace adds bold open gusset des...</td>\n",
       "      <td>[182, 691, 1, 416, 724, 211, 397, 135, 644, 49...</td>\n",
       "      <td>[2569, 2570, 211, 397, 1, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'annabelle' lace camisole</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>64.0</td>\n",
       "      <td>scalloped trim adorned little blue bow flirts ...</td>\n",
       "      <td>[302, 75, 1406, 340, 285, 156, 2620, 521, 77, ...</td>\n",
       "      <td>[2571, 1, 426]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'annabelle' lace garter</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>scalloped lace adorned little blue bow defines...</td>\n",
       "      <td>[302, 1, 1406, 340, 285, 156, 1990, 1407, 1109...</td>\n",
       "      <td>[2571, 1, 288]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        product_name  brand_name  product_category_wide   mrp                                  clean_description                                    seq_description                           seq_product_name\n",
       "0  \"i do\" signature lace cheeky hipster with gift...           6                      7  40.0   i do  emblazoned hanky panky lace hipster swa...  [540, 541, 1760, 83, 90, 1, 98, 1201, 557, 296...  [540, 541, 70, 1, 175, 98, 283, 439, 561]\n",
       "2         'after midnight' lace open gusset g-string           6                      6  27.0  stretch signature lace fashions alluring doubl...  [34, 70, 1, 1035, 612, 542, 174, 415, 246, 109...        [2569, 2570, 1, 211, 397, 415, 246]\n",
       "5            'after midnight' open gusset lace thong           6                      7  24.0  leopard spotted lace adds bold open gusset des...  [182, 691, 1, 416, 724, 211, 397, 135, 644, 49...               [2569, 2570, 211, 397, 1, 8]\n",
       "8                          'annabelle' lace camisole           6                      5  64.0  scalloped trim adorned little blue bow flirts ...  [302, 75, 1406, 340, 285, 156, 2620, 521, 77, ...                             [2571, 1, 426]\n",
       "9                            'annabelle' lace garter           6                      6  23.0  scalloped lace adorned little blue bow defines...  [302, 1, 1406, 340, 285, 156, 1990, 1407, 1109...                             [2571, 1, 288]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing\n",
    "raw_text = np.hstack([data_cleaned.clean_description.str.lower(), data_cleaned.product_name.str.lower()])\n",
    "tok_raw = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tok_raw.fit_on_texts(raw_text)\n",
    "data_cleaned[\"seq_description\"] = tok_raw.texts_to_sequences(data_cleaned.clean_description.str.lower())\n",
    "data_cleaned[\"seq_product_name\"] = tok_raw.texts_to_sequences(data_cleaned.product_name.str.lower())\n",
    "max_seq_description = np.max(data_cleaned.seq_description.apply(lambda x: len(x)))\n",
    "max_seq_product_name = np.max(data_cleaned.seq_product_name.apply(lambda x: len(x)))\n",
    "print(\"Maximum Value of Sequences: \", max_seq_description, max_seq_product_name)\n",
    "\n",
    "MAX_TEXT = np.max([np.max(data_cleaned.seq_description.apply(lambda x : max(x))),\n",
    "                   np.max(data_cleaned.seq_product_name.apply(lambda x : max(x)))])  + 3\n",
    "\n",
    "MAX_BRAND = np.max(data_cleaned.brand_name.max()) + 3\n",
    "MAX_CAT = np.max(data_cleaned.product_category_wide.max()) + 3\n",
    "\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'target'}>]], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Min Max Scaler for target\n",
    "data_cleaned[\"target\"] = np.log(data_cleaned.mrp+1)\n",
    "target_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "data_cleaned[\"target\"] = target_scaler.fit_transform(data_cleaned.target.values.reshape(-1,1))\n",
    "pd.DataFrame(data_cleaned.target).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3101, 8)\n",
      "(345, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>product_category_wide</th>\n",
       "      <th>mrp</th>\n",
       "      <th>clean_description</th>\n",
       "      <th>seq_description</th>\n",
       "      <th>seq_product_name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>seamless hipster briefs</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>13.00</td>\n",
       "      <td>ultrasoft  stretchy microfiber shapes comforta...</td>\n",
       "      <td>[1688, 49, 389, 694, 95, 98, 208, 1933, 303, 3...</td>\n",
       "      <td>[48, 98, 208]</td>\n",
       "      <td>-0.690375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>bridget pushup cotton bra</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>32.95</td>\n",
       "      <td>flirt everyday perkiness every girl  style  97...</td>\n",
       "      <td>[569, 73, 630, 89, 159, 5, 1070, 1639, 9, 933,...</td>\n",
       "      <td>[643, 66, 7, 2]</td>\n",
       "      <td>-0.294680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>aerie everyday loves lace thong</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>12.50</td>\n",
       "      <td>introducing everyday loves™  made love  everyd...</td>\n",
       "      <td>[356, 73, 357, 32, 39, 73, 116, 258, 234, 140,...</td>\n",
       "      <td>[14, 73, 116, 1, 8]</td>\n",
       "      <td>-0.706620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>aerie bikini</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>9.50</td>\n",
       "      <td>peek cheek  lot wow  soft comfy cotton love fi...</td>\n",
       "      <td>[27, 26, 230, 207, 18, 20, 7, 39, 23, 65, 87, ...</td>\n",
       "      <td>[14, 38]</td>\n",
       "      <td>-0.818881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>id logo mesh-panel hipster qf1780</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>22.00</td>\n",
       "      <td>kick style notch logo enhanced cooling hipster...</td>\n",
       "      <td>[1690, 5, 2943, 35, 701, 1889, 98, 54, 59]</td>\n",
       "      <td>[406, 35, 36, 364, 98, 3654]</td>\n",
       "      <td>-0.468620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           product_name  brand_name  product_category_wide    mrp                                  clean_description                                    seq_description              seq_product_name    target\n",
       "4479            seamless hipster briefs           9                      7  13.00  ultrasoft  stretchy microfiber shapes comforta...  [1688, 49, 389, 694, 95, 98, 208, 1933, 303, 3...                 [48, 98, 208] -0.690375\n",
       "2199          bridget pushup cotton bra           0                      4  32.95  flirt everyday perkiness every girl  style  97...  [569, 73, 630, 89, 159, 5, 1070, 1639, 9, 933,...               [643, 66, 7, 2] -0.294680\n",
       "843     aerie everyday loves lace thong           0                      7  12.50  introducing everyday loves™  made love  everyd...  [356, 73, 357, 32, 39, 73, 116, 258, 234, 140,...           [14, 73, 116, 1, 8] -0.706620\n",
       "324                        aerie bikini           0                      6   9.50  peek cheek  lot wow  soft comfy cotton love fi...  [27, 26, 230, 207, 18, 20, 7, 39, 23, 65, 87, ...                      [14, 38] -0.818881\n",
       "3247  id logo mesh-panel hipster qf1780           2                      7  22.00  kick style notch logo enhanced cooling hipster...         [1690, 5, 2943, 35, 701, 1889, 98, 54, 59]  [406, 35, 36, 364, 98, 3654] -0.468620"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Test Split\n",
    "dtrain, dtest = train_test_split(data_cleaned, random_state=123, train_size=0.90)\n",
    "print(dtrain.shape)\n",
    "print(dtest.shape)\n",
    "dtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item_name': array([[   0,    0,    0, ...,   48,   98,  208],\n",
       "        [   0,    0,    0, ...,   66,    7,    2],\n",
       "        [   0,    0,    0, ...,  116,    1,    8],\n",
       "        ...,\n",
       "        [   0,  165,  133, ...,  288,  452, 3534],\n",
       "        [   0,    0,    0, ...,  833,   19, 3582],\n",
       "        [   0,    0,    0, ...,  118,  132,    2]]),\n",
       " 'item_desc': array([[   0,    0,    0, ...,   62,  304,   23],\n",
       "        [   0,    0,    0, ...,    3,    3,   11],\n",
       "        [   0,    0,    0, ...,    3,    3,   11],\n",
       "        ...,\n",
       "        [   0,    0,    0, ...,  133,   54,   59],\n",
       "        [   0,    0,    0, ...,  238,  243,  135],\n",
       "        [   0,    0,    0, ...,  264,  837, 1243]]),\n",
       " 'brand_name': array([9, 0, 0, ..., 2, 6, 2]),\n",
       " 'product_category': array([ 7,  4,  7, ..., 12,  3,  4])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequence Padding\n",
    "def get_keras_data(dataset):\n",
    "    X = {\n",
    "        'item_name':  pad_sequences(dataset.seq_product_name, maxlen=10),\n",
    "        'item_desc': pad_sequences(dataset.seq_description, maxlen=75),\n",
    "        'brand_name': np.array(dataset.brand_name),\n",
    "        'product_category': np.array(dataset.product_category_wide)\n",
    "    }\n",
    "    return X\n",
    "\n",
    "X_train = get_keras_data(dtrain)\n",
    "X_test = get_keras_data(dtest)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " item_desc (InputLayer)         [(None, 75)]         0           []                               \n",
      "                                                                                                  \n",
      " item_name (InputLayer)         [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " brand_name (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " product_category (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 75, 50)       195500      ['item_desc[0][0]']              \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 10, 50)       195500      ['item_name[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 1, 10)        160         ['brand_name[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 1, 10)        160         ['product_category[0][0]']       \n",
      "                                                                                                  \n",
      " gru (GRU)                      (None, 16)           3264        ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 8)            1440        ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 10)           0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 10)           0           ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 44)           0           ['gru[0][0]',                    \n",
      "                                                                  'gru_1[0][0]',                  \n",
      "                                                                  'flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          5760        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            65          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 410,105\n",
      "Trainable params: 410,105\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "def rmsle_cust(y_true, y_pred):\n",
    "    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n",
    "    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n",
    "    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n",
    "\n",
    "def get_gru_model(data):\n",
    "    # Params\n",
    "    dr_r = 0.1\n",
    "    \n",
    "    # Inputs\n",
    "    item_name = Input(shape=[data[\"item_name\"].shape[1]], name=\"item_name\")\n",
    "    item_desc = Input(shape=[data[\"item_desc\"].shape[1]], name=\"item_desc\")\n",
    "    brand_name = Input(shape=[1], name=\"brand_name\")\n",
    "    product_category = Input(shape=[1], name=\"product_category\")\n",
    "    \n",
    "    # Embeddings Layers\n",
    "    emb_item_name = Embedding(MAX_TEXT, 50)(item_name)\n",
    "    emb_item_desc = Embedding(MAX_TEXT, 50)(item_desc)\n",
    "    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n",
    "    emb_product_category = Embedding(MAX_CAT, 10)(product_category)\n",
    "    \n",
    "    # RNN Layer\n",
    "    rnn_layer1 = GRU(16) (emb_item_desc)\n",
    "    rnn_layer2 = GRU(8) (emb_item_name)\n",
    "    \n",
    "    # Main Layer\n",
    "    main_l = concatenate([\n",
    "        rnn_layer1,\n",
    "        rnn_layer2,\n",
    "        Flatten() (emb_brand_name)\n",
    "        , Flatten() (emb_product_category)\n",
    "    ])\n",
    "    main_l = Dropout(dr_r) (Dense(128) (main_l))\n",
    "    main_l = Dropout(dr_r) (Dense(64) (main_l))\n",
    "    \n",
    "    # Output Layer\n",
    "    output = Dense(1, activation=\"linear\") (main_l)\n",
    "    \n",
    "    # Init Model\n",
    "    model = Model([item_name, item_desc, brand_name, product_category], output)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", rmsle_cust])\n",
    "    \n",
    "    return model\n",
    "\n",
    "    \n",
    "gru_model = get_gru_model(X_train)\n",
    "\n",
    "# Calculation of Loss History\n",
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "history = LossHistory()\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2/2 [==============================] - 8s 2s/step - loss: 0.1951 - mae: 0.3718 - rmsle_cust: 0.0211 - val_loss: 0.1163 - val_mae: 0.2708 - val_rmsle_cust: 0.0077\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 1s 198ms/step - loss: 0.1211 - mae: 0.2801 - rmsle_cust: 0.0101 - val_loss: 0.0669 - val_mae: 0.2008 - val_rmsle_cust: 0.0077\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 1s 216ms/step - loss: 0.0706 - mae: 0.2079 - rmsle_cust: 0.0101 - val_loss: 0.0471 - val_mae: 0.1800 - val_rmsle_cust: 0.0077\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 215ms/step - loss: 0.0513 - mae: 0.1858 - rmsle_cust: 0.0101 - val_loss: 0.0563 - val_mae: 0.2031 - val_rmsle_cust: 0.0077\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 217ms/step - loss: 0.0591 - mae: 0.2033 - rmsle_cust: 0.0101 - val_loss: 0.0557 - val_mae: 0.1993 - val_rmsle_cust: 0.0077\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 217ms/step - loss: 0.0555 - mae: 0.1942 - rmsle_cust: 0.0101 - val_loss: 0.0401 - val_mae: 0.1664 - val_rmsle_cust: 0.0077\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 1s 223ms/step - loss: 0.0403 - mae: 0.1621 - rmsle_cust: 0.0101 - val_loss: 0.0295 - val_mae: 0.1385 - val_rmsle_cust: 0.0076\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 217ms/step - loss: 0.0309 - mae: 0.1360 - rmsle_cust: 0.0100 - val_loss: 0.0277 - val_mae: 0.1295 - val_rmsle_cust: 0.0074\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 224ms/step - loss: 0.0298 - mae: 0.1321 - rmsle_cust: 0.0094 - val_loss: 0.0277 - val_mae: 0.1300 - val_rmsle_cust: 0.0072\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 202ms/step - loss: 0.0288 - mae: 0.1323 - rmsle_cust: 0.0086 - val_loss: 0.0244 - val_mae: 0.1228 - val_rmsle_cust: 0.0071\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 204ms/step - loss: 0.0251 - mae: 0.1242 - rmsle_cust: 0.0090 - val_loss: 0.0194 - val_mae: 0.1076 - val_rmsle_cust: 0.0073\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 190ms/step - loss: 0.0195 - mae: 0.1052 - rmsle_cust: 0.0091 - val_loss: 0.0172 - val_mae: 0.0972 - val_rmsle_cust: 0.0073\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 198ms/step - loss: 0.0173 - mae: 0.0947 - rmsle_cust: 0.0091 - val_loss: 0.0185 - val_mae: 0.0993 - val_rmsle_cust: 0.0072\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 209ms/step - loss: 0.0186 - mae: 0.0986 - rmsle_cust: 0.0088 - val_loss: 0.0185 - val_mae: 0.1000 - val_rmsle_cust: 0.0073\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 187ms/step - loss: 0.0181 - mae: 0.0979 - rmsle_cust: 0.0088 - val_loss: 0.0159 - val_mae: 0.0928 - val_rmsle_cust: 0.0078\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 214ms/step - loss: 0.0149 - mae: 0.0882 - rmsle_cust: 0.0092 - val_loss: 0.0143 - val_mae: 0.0900 - val_rmsle_cust: 0.0084\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 178ms/step - loss: 0.0126 - mae: 0.0833 - rmsle_cust: 0.0095 - val_loss: 0.0143 - val_mae: 0.0925 - val_rmsle_cust: 0.0085\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 214ms/step - loss: 0.0123 - mae: 0.0842 - rmsle_cust: 0.0092 - val_loss: 0.0140 - val_mae: 0.0912 - val_rmsle_cust: 0.0076\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 183ms/step - loss: 0.0112 - mae: 0.0799 - rmsle_cust: 0.0080 - val_loss: 0.0125 - val_mae: 0.0844 - val_rmsle_cust: 0.0063\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 196ms/step - loss: 0.0095 - mae: 0.0707 - rmsle_cust: 0.0067 - val_loss: 0.0116 - val_mae: 0.0797 - val_rmsle_cust: 0.0061\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 176ms/step - loss: 0.0084 - mae: 0.0653 - rmsle_cust: 0.0060 - val_loss: 0.0118 - val_mae: 0.0812 - val_rmsle_cust: 0.0060\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 197ms/step - loss: 0.0081 - mae: 0.0651 - rmsle_cust: 0.0060 - val_loss: 0.0118 - val_mae: 0.0816 - val_rmsle_cust: 0.0059\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 183ms/step - loss: 0.0082 - mae: 0.0667 - rmsle_cust: 0.0057 - val_loss: 0.0111 - val_mae: 0.0788 - val_rmsle_cust: 0.0059\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 200ms/step - loss: 0.0074 - mae: 0.0619 - rmsle_cust: 0.0057 - val_loss: 0.0109 - val_mae: 0.0775 - val_rmsle_cust: 0.0059\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 187ms/step - loss: 0.0069 - mae: 0.0598 - rmsle_cust: 0.0056 - val_loss: 0.0111 - val_mae: 0.0785 - val_rmsle_cust: 0.0061\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 184ms/step - loss: 0.0069 - mae: 0.0605 - rmsle_cust: 0.0061 - val_loss: 0.0109 - val_mae: 0.0773 - val_rmsle_cust: 0.0062\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 215ms/step - loss: 0.0067 - mae: 0.0591 - rmsle_cust: 0.0060 - val_loss: 0.0102 - val_mae: 0.0738 - val_rmsle_cust: 0.0061\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 185ms/step - loss: 0.0061 - mae: 0.0561 - rmsle_cust: 0.0060 - val_loss: 0.0099 - val_mae: 0.0724 - val_rmsle_cust: 0.0059\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 208ms/step - loss: 0.0059 - mae: 0.0550 - rmsle_cust: 0.0055 - val_loss: 0.0097 - val_mae: 0.0717 - val_rmsle_cust: 0.0058\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 185ms/step - loss: 0.0058 - mae: 0.0547 - rmsle_cust: 0.0056 - val_loss: 0.0094 - val_mae: 0.0697 - val_rmsle_cust: 0.0057\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 1s 204ms/step - loss: 0.0054 - mae: 0.0519 - rmsle_cust: 0.0053 - val_loss: 0.0094 - val_mae: 0.0691 - val_rmsle_cust: 0.0058\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 183ms/step - loss: 0.0052 - mae: 0.0511 - rmsle_cust: 0.0051 - val_loss: 0.0094 - val_mae: 0.0695 - val_rmsle_cust: 0.0058\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 204ms/step - loss: 0.0049 - mae: 0.0500 - rmsle_cust: 0.0051 - val_loss: 0.0092 - val_mae: 0.0687 - val_rmsle_cust: 0.0056\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 179ms/step - loss: 0.0049 - mae: 0.0490 - rmsle_cust: 0.0046 - val_loss: 0.0090 - val_mae: 0.0671 - val_rmsle_cust: 0.0055\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 1s 205ms/step - loss: 0.0047 - mae: 0.0477 - rmsle_cust: 0.0044 - val_loss: 0.0089 - val_mae: 0.0666 - val_rmsle_cust: 0.0055\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 181ms/step - loss: 0.0046 - mae: 0.0474 - rmsle_cust: 0.0044 - val_loss: 0.0088 - val_mae: 0.0664 - val_rmsle_cust: 0.0054\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 1s 200ms/step - loss: 0.0045 - mae: 0.0466 - rmsle_cust: 0.0042 - val_loss: 0.0088 - val_mae: 0.0661 - val_rmsle_cust: 0.0054\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 1s 213ms/step - loss: 0.0044 - mae: 0.0466 - rmsle_cust: 0.0042 - val_loss: 0.0089 - val_mae: 0.0664 - val_rmsle_cust: 0.0054\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 1s 184ms/step - loss: 0.0042 - mae: 0.0455 - rmsle_cust: 0.0040 - val_loss: 0.0088 - val_mae: 0.0660 - val_rmsle_cust: 0.0055\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 1s 214ms/step - loss: 0.0043 - mae: 0.0459 - rmsle_cust: 0.0040 - val_loss: 0.0087 - val_mae: 0.0653 - val_rmsle_cust: 0.0055\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 1s 185ms/step - loss: 0.0042 - mae: 0.0453 - rmsle_cust: 0.0040 - val_loss: 0.0086 - val_mae: 0.0652 - val_rmsle_cust: 0.0054\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 182ms/step - loss: 0.0040 - mae: 0.0442 - rmsle_cust: 0.0039 - val_loss: 0.0085 - val_mae: 0.0648 - val_rmsle_cust: 0.0054\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 183ms/step - loss: 0.0038 - mae: 0.0436 - rmsle_cust: 0.0037 - val_loss: 0.0085 - val_mae: 0.0646 - val_rmsle_cust: 0.0054\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 204ms/step - loss: 0.0038 - mae: 0.0429 - rmsle_cust: 0.0040 - val_loss: 0.0086 - val_mae: 0.0648 - val_rmsle_cust: 0.0054\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 178ms/step - loss: 0.0037 - mae: 0.0429 - rmsle_cust: 0.0040 - val_loss: 0.0085 - val_mae: 0.0644 - val_rmsle_cust: 0.0053\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 200ms/step - loss: 0.0036 - mae: 0.0423 - rmsle_cust: 0.0036 - val_loss: 0.0084 - val_mae: 0.0644 - val_rmsle_cust: 0.0053\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 186ms/step - loss: 0.0037 - mae: 0.0423 - rmsle_cust: 0.0035 - val_loss: 0.0083 - val_mae: 0.0645 - val_rmsle_cust: 0.0053\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 203ms/step - loss: 0.0036 - mae: 0.0426 - rmsle_cust: 0.0036 - val_loss: 0.0083 - val_mae: 0.0642 - val_rmsle_cust: 0.0053\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 184ms/step - loss: 0.0034 - mae: 0.0406 - rmsle_cust: 0.0036 - val_loss: 0.0083 - val_mae: 0.0639 - val_rmsle_cust: 0.0055\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 193ms/step - loss: 0.0034 - mae: 0.0413 - rmsle_cust: 0.0035 - val_loss: 0.0083 - val_mae: 0.0639 - val_rmsle_cust: 0.0055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23aa74b4e50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Fitting\n",
    "\n",
    "BATCH_SIZE = 2500\n",
    "epochs = 50\n",
    "gru_model = get_gru_model(X_train)\n",
    "gru_model.fit(X_train, dtrain.target, epochs=epochs, batch_size=BATCH_SIZE\n",
    "          , validation_data=(X_test, dtest.target)\n",
    "          , verbose=1, callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_range=np.log(history.losses)\n",
    "sns.distplot(history.losses, hist=False)\n",
    "plt.title('Training Loss')\n",
    "plt.grid()\n",
    "plt.legend('top')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 11ms/step\n",
      " RMSLE error for GRU Based Model on Test Data: 0.22893180024344015\n",
      " RMSE error for GRU Based Model on Test Data: 8.495720831382716\n"
     ]
    }
   ],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n",
    "    return (sum(to_sum) * (1.0/len(y))) ** 0.5\n",
    "\n",
    "val_preds = gru_model.predict(X_test)\n",
    "val_preds = target_scaler.inverse_transform(val_preds)\n",
    "val_preds = np.exp(val_preds)+1\n",
    "\n",
    "# mean_absolute_error, mean_squared_log_error.\n",
    "y_true = np.array(dtest.mrp.values)\n",
    "y_pred = val_preds[:,0]\n",
    "v_rmsle = rmsle(y_true, y_pred)\n",
    "v_rmse = mean_squared_error(y_true, y_pred , squared=False)\n",
    "print(\" RMSLE error for GRU Based Model on Test Data: \"+str(v_rmsle))\n",
    "print(\" RMSE error for GRU Based Model on Test Data: \"+str(v_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>product_category_wide</th>\n",
       "      <th>mrp</th>\n",
       "      <th>clean_description</th>\n",
       "      <th>seq_description</th>\n",
       "      <th>seq_product_name</th>\n",
       "      <th>target</th>\n",
       "      <th>predicted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3619</th>\n",
       "      <td>logo to go low rise thong 631581</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>22.00</td>\n",
       "      <td>soft  sheer lace wraps around waist lightweigh...</td>\n",
       "      <td>[18, 72, 1, 1261, 521, 161, 264, 20, 8, 83, 90]</td>\n",
       "      <td>[35, 87, 65, 21, 16, 8, 1980]</td>\n",
       "      <td>-0.468620</td>\n",
       "      <td>30.259712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3444</th>\n",
       "      <td>lace plunge bra</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>35.00</td>\n",
       "      <td>elegant black plunge bra prettified gorgeous l...</td>\n",
       "      <td>[488, 133, 80, 2, 2973, 238, 1, 711, 109, 324,...</td>\n",
       "      <td>[1, 80, 2]</td>\n",
       "      <td>-0.268490</td>\n",
       "      <td>28.239491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517</th>\n",
       "      <td>classic mesh triangle bralette</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>54.00</td>\n",
       "      <td>classic silhouettes airy stretch mesh</td>\n",
       "      <td>[100, 682, 616, 34, 36]</td>\n",
       "      <td>[100, 36, 68, 19]</td>\n",
       "      <td>-0.079174</td>\n",
       "      <td>52.706375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>happy unlined bandeau bra</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>26.95</td>\n",
       "      <td>hello  happy bras  happiness feeling like real...</td>\n",
       "      <td>[240, 52, 92, 148, 269, 150, 12, 235, 360, 52,...</td>\n",
       "      <td>[52, 169, 312, 2]</td>\n",
       "      <td>-0.381549</td>\n",
       "      <td>27.374535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5635</th>\n",
       "      <td>b.provocative contrast-lace bra 951222</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>40.00</td>\n",
       "      <td>flattering sheer nude panels decorated intrica...</td>\n",
       "      <td>[215, 72, 393, 781, 999, 571, 1250, 325, 3311,...</td>\n",
       "      <td>[67, 686, 216, 1, 2, 3892]</td>\n",
       "      <td>-0.210396</td>\n",
       "      <td>37.524208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                product_name  brand_name  product_category_wide    mrp                                  clean_description                                    seq_description               seq_product_name    target  predicted_price\n",
       "3619        logo to go low rise thong 631581           6                      7  22.00  soft  sheer lace wraps around waist lightweigh...    [18, 72, 1, 1261, 521, 161, 264, 20, 8, 83, 90]  [35, 87, 65, 21, 16, 8, 1980] -0.468620        30.259712\n",
       "3444                         lace plunge bra          12                      4  35.00  elegant black plunge bra prettified gorgeous l...  [488, 133, 80, 2, 2973, 238, 1, 711, 109, 324,...                     [1, 80, 2] -0.268490        28.239491\n",
       "2517          classic mesh triangle bralette           6                      3  54.00             classic silhouettes airy stretch mesh                             [100, 682, 616, 34, 36]              [100, 36, 68, 19] -0.079174        52.706375\n",
       "3076               happy unlined bandeau bra           0                      4  26.95  hello  happy bras  happiness feeling like real...  [240, 52, 92, 148, 269, 150, 12, 235, 360, 52,...              [52, 169, 312, 2] -0.381549        27.374535\n",
       "5635  b.provocative contrast-lace bra 951222           1                      4  40.00  flattering sheer nude panels decorated intrica...  [215, 72, 393, 781, 999, 571, 1250, 325, 3311,...     [67, 686, 216, 1, 2, 3892] -0.210396        37.524208"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the price.\n",
    "preds = gru_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "preds = target_scaler.inverse_transform(preds)\n",
    "preds = np.exp(preds)-1\n",
    "dtest[\"predicted_price\"] = preds\n",
    "dtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [   0    1    2 ... 3443 3444 3445] Validation: [   3    6    7   12   13   28   40   45   75   89   93   94   98   99\n",
      "  102  110  126  136  161  165  171  187  188  201  203  205  224  230\n",
      "  254  255  262  268  280  291  297  301  306  318  321  329  330  331\n",
      "  349  351  366  368  382  393  397  403  420  442  453  464  473  481\n",
      "  487  494  495  517  534  540  541  546  557  560  563  581  582  588\n",
      "  592  598  600  605  610  613  632  644  650  671  678  702  705  709\n",
      "  719  725  730  748  772  774  777  784  790  792  796  799  802  870\n",
      "  876  899  902  913  925  927  929  943  946  955  957  968  979  984\n",
      "  988  993  994 1002 1004 1020 1045 1060 1065 1074 1098 1100 1103 1128\n",
      " 1135 1138 1141 1142 1153 1163 1164 1171 1173 1177 1179 1196 1201 1232\n",
      " 1247 1265 1268 1270 1271 1273 1291 1294 1301 1305 1335 1396 1413 1421\n",
      " 1436 1450 1452 1459 1503 1509 1517 1520 1526 1537 1545 1548 1554 1584\n",
      " 1611 1614 1621 1647 1649 1661 1672 1674 1685 1687 1697 1723 1732 1744\n",
      " 1754 1759 1765 1805 1817 1828 1832 1863 1906 1907 1908 1913 1919 1928\n",
      " 1929 1941 1945 1956 1957 1977 1987 1988 1993 1994 2005 2010 2017 2019\n",
      " 2034 2062 2066 2073 2078 2097 2105 2113 2114 2119 2128 2146 2161 2188\n",
      " 2231 2238 2243 2256 2274 2285 2287 2291 2294 2296 2323 2331 2347 2351\n",
      " 2352 2357 2359 2375 2379 2383 2391 2396 2410 2416 2423 2435 2447 2448\n",
      " 2471 2475 2497 2509 2535 2560 2576 2607 2613 2623 2633 2646 2655 2667\n",
      " 2676 2688 2710 2720 2727 2733 2736 2745 2746 2771 2790 2800 2817 2824\n",
      " 2827 2839 2849 2866 2867 2885 2888 2893 2928 2952 2964 2971 2984 2994\n",
      " 3001 3017 3026 3035 3036 3043 3044 3055 3066 3076 3077 3081 3088 3093\n",
      " 3107 3117 3130 3138 3145 3149 3157 3165 3167 3180 3185 3190 3193 3203\n",
      " 3204 3214 3221 3233 3259 3260 3262 3269 3294 3296 3323 3324 3329 3333\n",
      " 3337 3350 3362 3375 3378 3381 3402 3408 3430]\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 4s 913ms/step - loss: 0.1748 - mae: 0.3455 - rmsle_cust: 0.0139 - val_loss: 0.1112 - val_mae: 0.2661 - val_rmsle_cust: 0.0100\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 1s 201ms/step - loss: 0.1094 - mae: 0.2630 - rmsle_cust: 0.0099 - val_loss: 0.0667 - val_mae: 0.1958 - val_rmsle_cust: 0.0100\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 1s 197ms/step - loss: 0.0667 - mae: 0.2023 - rmsle_cust: 0.0099 - val_loss: 0.0537 - val_mae: 0.1847 - val_rmsle_cust: 0.0100\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 186ms/step - loss: 0.0539 - mae: 0.1912 - rmsle_cust: 0.0099 - val_loss: 0.0588 - val_mae: 0.1992 - val_rmsle_cust: 0.0100\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 218ms/step - loss: 0.0556 - mae: 0.1946 - rmsle_cust: 0.0099 - val_loss: 0.0482 - val_mae: 0.1742 - val_rmsle_cust: 0.0100\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 197ms/step - loss: 0.0431 - mae: 0.1652 - rmsle_cust: 0.0099 - val_loss: 0.0325 - val_mae: 0.1352 - val_rmsle_cust: 0.0097\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 1s 207ms/step - loss: 0.0281 - mae: 0.1276 - rmsle_cust: 0.0098 - val_loss: 0.0271 - val_mae: 0.1252 - val_rmsle_cust: 0.0100\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 204ms/step - loss: 0.0240 - mae: 0.1192 - rmsle_cust: 0.0096 - val_loss: 0.0287 - val_mae: 0.1331 - val_rmsle_cust: 0.0105\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 197ms/step - loss: 0.0251 - mae: 0.1270 - rmsle_cust: 0.0108 - val_loss: 0.0266 - val_mae: 0.1268 - val_rmsle_cust: 0.0120\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 191ms/step - loss: 0.0225 - mae: 0.1196 - rmsle_cust: 0.0124 - val_loss: 0.0219 - val_mae: 0.1067 - val_rmsle_cust: 0.0112\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 193ms/step - loss: 0.0181 - mae: 0.1010 - rmsle_cust: 0.0117 - val_loss: 0.0210 - val_mae: 0.1022 - val_rmsle_cust: 0.0100\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 215ms/step - loss: 0.0175 - mae: 0.0968 - rmsle_cust: 0.0098 - val_loss: 0.0211 - val_mae: 0.1039 - val_rmsle_cust: 0.0091\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 210ms/step - loss: 0.0171 - mae: 0.0962 - rmsle_cust: 0.0089 - val_loss: 0.0181 - val_mae: 0.0934 - val_rmsle_cust: 0.0087\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 233ms/step - loss: 0.0140 - mae: 0.0856 - rmsle_cust: 0.0081 - val_loss: 0.0149 - val_mae: 0.0804 - val_rmsle_cust: 0.0086\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 199ms/step - loss: 0.0111 - mae: 0.0760 - rmsle_cust: 0.0078 - val_loss: 0.0142 - val_mae: 0.0825 - val_rmsle_cust: 0.0085\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 208ms/step - loss: 0.0107 - mae: 0.0756 - rmsle_cust: 0.0072 - val_loss: 0.0141 - val_mae: 0.0850 - val_rmsle_cust: 0.0087\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 202ms/step - loss: 0.0102 - mae: 0.0751 - rmsle_cust: 0.0066 - val_loss: 0.0131 - val_mae: 0.0802 - val_rmsle_cust: 0.0084\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 204ms/step - loss: 0.0091 - mae: 0.0689 - rmsle_cust: 0.0061 - val_loss: 0.0122 - val_mae: 0.0753 - val_rmsle_cust: 0.0082\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 192ms/step - loss: 0.0081 - mae: 0.0636 - rmsle_cust: 0.0055 - val_loss: 0.0122 - val_mae: 0.0750 - val_rmsle_cust: 0.0083\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 192ms/step - loss: 0.0080 - mae: 0.0640 - rmsle_cust: 0.0057 - val_loss: 0.0121 - val_mae: 0.0755 - val_rmsle_cust: 0.0085\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 186ms/step - loss: 0.0077 - mae: 0.0637 - rmsle_cust: 0.0055 - val_loss: 0.0116 - val_mae: 0.0735 - val_rmsle_cust: 0.0088\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 191ms/step - loss: 0.0071 - mae: 0.0601 - rmsle_cust: 0.0054 - val_loss: 0.0115 - val_mae: 0.0734 - val_rmsle_cust: 0.0092\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 236ms/step - loss: 0.0065 - mae: 0.0582 - rmsle_cust: 0.0058 - val_loss: 0.0116 - val_mae: 0.0744 - val_rmsle_cust: 0.0095\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 200ms/step - loss: 0.0064 - mae: 0.0574 - rmsle_cust: 0.0060 - val_loss: 0.0114 - val_mae: 0.0731 - val_rmsle_cust: 0.0093\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 1s 221ms/step - loss: 0.0060 - mae: 0.0564 - rmsle_cust: 0.0058 - val_loss: 0.0111 - val_mae: 0.0719 - val_rmsle_cust: 0.0089\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 199ms/step - loss: 0.0056 - mae: 0.0535 - rmsle_cust: 0.0054 - val_loss: 0.0110 - val_mae: 0.0723 - val_rmsle_cust: 0.0085\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 202ms/step - loss: 0.0055 - mae: 0.0536 - rmsle_cust: 0.0051 - val_loss: 0.0109 - val_mae: 0.0711 - val_rmsle_cust: 0.0083\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 185ms/step - loss: 0.0053 - mae: 0.0521 - rmsle_cust: 0.0048 - val_loss: 0.0108 - val_mae: 0.0696 - val_rmsle_cust: 0.0083\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 194ms/step - loss: 0.0051 - mae: 0.0503 - rmsle_cust: 0.0048 - val_loss: 0.0108 - val_mae: 0.0700 - val_rmsle_cust: 0.0083\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 184ms/step - loss: 0.0050 - mae: 0.0499 - rmsle_cust: 0.0047 - val_loss: 0.0108 - val_mae: 0.0699 - val_rmsle_cust: 0.0083\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 1s 198ms/step - loss: 0.0047 - mae: 0.0486 - rmsle_cust: 0.0045 - val_loss: 0.0107 - val_mae: 0.0686 - val_rmsle_cust: 0.0082\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 1s 189ms/step - loss: 0.0047 - mae: 0.0483 - rmsle_cust: 0.0044 - val_loss: 0.0107 - val_mae: 0.0677 - val_rmsle_cust: 0.0080\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 209ms/step - loss: 0.0044 - mae: 0.0468 - rmsle_cust: 0.0044 - val_loss: 0.0106 - val_mae: 0.0674 - val_rmsle_cust: 0.0079\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 1s 185ms/step - loss: 0.0045 - mae: 0.0472 - rmsle_cust: 0.0042 - val_loss: 0.0106 - val_mae: 0.0671 - val_rmsle_cust: 0.0079\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 1s 184ms/step - loss: 0.0042 - mae: 0.0452 - rmsle_cust: 0.0043 - val_loss: 0.0106 - val_mae: 0.0671 - val_rmsle_cust: 0.0080\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 1s 207ms/step - loss: 0.0042 - mae: 0.0458 - rmsle_cust: 0.0043 - val_loss: 0.0106 - val_mae: 0.0670 - val_rmsle_cust: 0.0079\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 211ms/step - loss: 0.0041 - mae: 0.0451 - rmsle_cust: 0.0040 - val_loss: 0.0105 - val_mae: 0.0668 - val_rmsle_cust: 0.0078\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 1s 227ms/step - loss: 0.0039 - mae: 0.0445 - rmsle_cust: 0.0040 - val_loss: 0.0104 - val_mae: 0.0665 - val_rmsle_cust: 0.0078\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 1s 188ms/step - loss: 0.0039 - mae: 0.0442 - rmsle_cust: 0.0039 - val_loss: 0.0104 - val_mae: 0.0663 - val_rmsle_cust: 0.0077\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 1s 220ms/step - loss: 0.0038 - mae: 0.0436 - rmsle_cust: 0.0038 - val_loss: 0.0103 - val_mae: 0.0660 - val_rmsle_cust: 0.0076\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 1s 196ms/step - loss: 0.0039 - mae: 0.0435 - rmsle_cust: 0.0039 - val_loss: 0.0103 - val_mae: 0.0660 - val_rmsle_cust: 0.0076\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 1s 197ms/step - loss: 0.0037 - mae: 0.0425 - rmsle_cust: 0.0037 - val_loss: 0.0103 - val_mae: 0.0660 - val_rmsle_cust: 0.0075\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 1s 179ms/step - loss: 0.0035 - mae: 0.0419 - rmsle_cust: 0.0037 - val_loss: 0.0103 - val_mae: 0.0660 - val_rmsle_cust: 0.0075\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 193ms/step - loss: 0.0035 - mae: 0.0421 - rmsle_cust: 0.0036 - val_loss: 0.0103 - val_mae: 0.0657 - val_rmsle_cust: 0.0074\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 1s 183ms/step - loss: 0.0035 - mae: 0.0419 - rmsle_cust: 0.0036 - val_loss: 0.0103 - val_mae: 0.0658 - val_rmsle_cust: 0.0074\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 1s 186ms/step - loss: 0.0034 - mae: 0.0415 - rmsle_cust: 0.0036 - val_loss: 0.0103 - val_mae: 0.0658 - val_rmsle_cust: 0.0074\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 1s 221ms/step - loss: 0.0034 - mae: 0.0415 - rmsle_cust: 0.0035 - val_loss: 0.0104 - val_mae: 0.0657 - val_rmsle_cust: 0.0073\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 208ms/step - loss: 0.0034 - mae: 0.0421 - rmsle_cust: 0.0035 - val_loss: 0.0103 - val_mae: 0.0656 - val_rmsle_cust: 0.0073\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 218ms/step - loss: 0.0032 - mae: 0.0406 - rmsle_cust: 0.0034 - val_loss: 0.0103 - val_mae: 0.0655 - val_rmsle_cust: 0.0073\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 1s 199ms/step - loss: 0.0031 - mae: 0.0401 - rmsle_cust: 0.0033 - val_loss: 0.0103 - val_mae: 0.0654 - val_rmsle_cust: 0.0073\n",
      "11/11 [==============================] - 1s 8ms/step\n",
      "Train: [   0    1    2 ... 3443 3444 3445] Validation: [   4   10   41   50   65   87  107  111  123  133  137  157  172  192\n",
      "  195  204  219  220  227  228  231  248  273  282  289  292  299  314\n",
      "  340  346  364  367  383  394  396  432  439  440  460  462  470  486\n",
      "  490  516  521  535  544  547  551  558  569  574  577  586  595  623\n",
      "  630  666  700  724  749  756  757  761  771  797  804  808  813  816\n",
      "  825  839  854  855  872  874  880  893  895  906  926  933  941  953\n",
      "  965  973  975  986  996 1017 1021 1032 1036 1039 1041 1043 1044 1064\n",
      " 1071 1073 1087 1104 1114 1125 1126 1131 1144 1145 1167 1169 1178 1189\n",
      " 1195 1217 1218 1221 1230 1234 1237 1239 1263 1316 1328 1333 1344 1349\n",
      " 1351 1373 1385 1386 1387 1399 1415 1417 1419 1426 1427 1432 1441 1453\n",
      " 1454 1456 1463 1477 1490 1501 1502 1504 1507 1535 1546 1562 1564 1565\n",
      " 1578 1607 1631 1636 1650 1659 1660 1669 1670 1675 1680 1690 1698 1713\n",
      " 1719 1720 1751 1760 1772 1773 1777 1780 1792 1816 1818 1819 1822 1823\n",
      " 1827 1829 1848 1858 1862 1881 1896 1903 1917 1924 1925 1930 1938 1950\n",
      " 1960 1965 2015 2027 2031 2038 2039 2079 2084 2085 2095 2099 2104 2106\n",
      " 2115 2130 2133 2149 2155 2159 2173 2175 2177 2182 2194 2206 2209 2236\n",
      " 2250 2259 2295 2297 2299 2322 2337 2348 2349 2361 2368 2373 2384 2385\n",
      " 2389 2390 2395 2414 2440 2500 2511 2522 2526 2532 2539 2571 2578 2580\n",
      " 2589 2595 2596 2601 2610 2627 2629 2637 2644 2656 2672 2689 2695 2702\n",
      " 2722 2732 2735 2740 2742 2750 2764 2778 2785 2802 2813 2821 2830 2837\n",
      " 2838 2857 2860 2863 2869 2875 2883 2894 2901 2908 2911 2917 2922 2927\n",
      " 2942 2950 2963 2980 2983 2988 3005 3006 3032 3038 3045 3053 3064 3078\n",
      " 3084 3086 3106 3111 3112 3113 3116 3126 3134 3162 3166 3171 3179 3191\n",
      " 3197 3205 3215 3219 3240 3241 3257 3285 3299 3313 3320 3332 3343 3345\n",
      " 3359 3366 3367 3393 3404 3405 3409 3420 3424]\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 5s 962ms/step - loss: 0.2032 - mae: 0.3725 - rmsle_cust: 0.0234 - val_loss: 0.1378 - val_mae: 0.2901 - val_rmsle_cust: 0.0095\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 1s 225ms/step - loss: 0.1373 - mae: 0.2936 - rmsle_cust: 0.0099 - val_loss: 0.0925 - val_mae: 0.2370 - val_rmsle_cust: 0.0095\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 1s 227ms/step - loss: 0.0915 - mae: 0.2369 - rmsle_cust: 0.0099 - val_loss: 0.0698 - val_mae: 0.2186 - val_rmsle_cust: 0.0095\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 224ms/step - loss: 0.0687 - mae: 0.2171 - rmsle_cust: 0.0099 - val_loss: 0.0700 - val_mae: 0.2294 - val_rmsle_cust: 0.0095\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 226ms/step - loss: 0.0677 - mae: 0.2219 - rmsle_cust: 0.0099 - val_loss: 0.0698 - val_mae: 0.2263 - val_rmsle_cust: 0.0095\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 225ms/step - loss: 0.0647 - mae: 0.2136 - rmsle_cust: 0.0099 - val_loss: 0.0562 - val_mae: 0.2011 - val_rmsle_cust: 0.0095\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 1s 243ms/step - loss: 0.0513 - mae: 0.1879 - rmsle_cust: 0.0099 - val_loss: 0.0421 - val_mae: 0.1698 - val_rmsle_cust: 0.0095\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 251ms/step - loss: 0.0390 - mae: 0.1587 - rmsle_cust: 0.0099 - val_loss: 0.0353 - val_mae: 0.1479 - val_rmsle_cust: 0.0095\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 276ms/step - loss: 0.0326 - mae: 0.1403 - rmsle_cust: 0.0099 - val_loss: 0.0325 - val_mae: 0.1395 - val_rmsle_cust: 0.0096\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 242ms/step - loss: 0.0295 - mae: 0.1339 - rmsle_cust: 0.0099 - val_loss: 0.0283 - val_mae: 0.1317 - val_rmsle_cust: 0.0100\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 246ms/step - loss: 0.0247 - mae: 0.1235 - rmsle_cust: 0.0099 - val_loss: 0.0225 - val_mae: 0.1153 - val_rmsle_cust: 0.0103\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 245ms/step - loss: 0.0193 - mae: 0.1065 - rmsle_cust: 0.0102 - val_loss: 0.0189 - val_mae: 0.0999 - val_rmsle_cust: 0.0105\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 245ms/step - loss: 0.0162 - mae: 0.0938 - rmsle_cust: 0.0100 - val_loss: 0.0193 - val_mae: 0.1009 - val_rmsle_cust: 0.0104\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 267ms/step - loss: 0.0173 - mae: 0.0971 - rmsle_cust: 0.0095 - val_loss: 0.0192 - val_mae: 0.1028 - val_rmsle_cust: 0.0105\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 297ms/step - loss: 0.0167 - mae: 0.0957 - rmsle_cust: 0.0094 - val_loss: 0.0168 - val_mae: 0.0947 - val_rmsle_cust: 0.0107\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 252ms/step - loss: 0.0139 - mae: 0.0864 - rmsle_cust: 0.0093 - val_loss: 0.0151 - val_mae: 0.0898 - val_rmsle_cust: 0.0108\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 274ms/step - loss: 0.0117 - mae: 0.0801 - rmsle_cust: 0.0093 - val_loss: 0.0151 - val_mae: 0.0927 - val_rmsle_cust: 0.0102\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 241ms/step - loss: 0.0110 - mae: 0.0777 - rmsle_cust: 0.0086 - val_loss: 0.0145 - val_mae: 0.0912 - val_rmsle_cust: 0.0092\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 241ms/step - loss: 0.0103 - mae: 0.0744 - rmsle_cust: 0.0074 - val_loss: 0.0134 - val_mae: 0.0855 - val_rmsle_cust: 0.0083\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 285ms/step - loss: 0.0089 - mae: 0.0683 - rmsle_cust: 0.0065 - val_loss: 0.0128 - val_mae: 0.0829 - val_rmsle_cust: 0.0080\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 258ms/step - loss: 0.0084 - mae: 0.0658 - rmsle_cust: 0.0060 - val_loss: 0.0128 - val_mae: 0.0823 - val_rmsle_cust: 0.0079\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 245ms/step - loss: 0.0085 - mae: 0.0668 - rmsle_cust: 0.0060 - val_loss: 0.0126 - val_mae: 0.0809 - val_rmsle_cust: 0.0078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.0079 - mae: 0.0635 - rmsle_cust: 0.0056 - val_loss: 0.0121 - val_mae: 0.0783 - val_rmsle_cust: 0.0076\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 285ms/step - loss: 0.0073 - mae: 0.0615 - rmsle_cust: 0.0054 - val_loss: 0.0118 - val_mae: 0.0766 - val_rmsle_cust: 0.0076\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 1s 292ms/step - loss: 0.0071 - mae: 0.0601 - rmsle_cust: 0.0055 - val_loss: 0.0116 - val_mae: 0.0756 - val_rmsle_cust: 0.0078\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 233ms/step - loss: 0.0067 - mae: 0.0580 - rmsle_cust: 0.0055 - val_loss: 0.0113 - val_mae: 0.0739 - val_rmsle_cust: 0.0080\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 257ms/step - loss: 0.0065 - mae: 0.0573 - rmsle_cust: 0.0056 - val_loss: 0.0109 - val_mae: 0.0718 - val_rmsle_cust: 0.0081\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 241ms/step - loss: 0.0061 - mae: 0.0552 - rmsle_cust: 0.0058 - val_loss: 0.0107 - val_mae: 0.0711 - val_rmsle_cust: 0.0083\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 241ms/step - loss: 0.0060 - mae: 0.0549 - rmsle_cust: 0.0054 - val_loss: 0.0106 - val_mae: 0.0710 - val_rmsle_cust: 0.0083\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 268ms/step - loss: 0.0056 - mae: 0.0534 - rmsle_cust: 0.0053 - val_loss: 0.0106 - val_mae: 0.0707 - val_rmsle_cust: 0.0084\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 1s 244ms/step - loss: 0.0055 - mae: 0.0524 - rmsle_cust: 0.0052 - val_loss: 0.0106 - val_mae: 0.0706 - val_rmsle_cust: 0.0084\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 1s 263ms/step - loss: 0.0052 - mae: 0.0507 - rmsle_cust: 0.0049 - val_loss: 0.0106 - val_mae: 0.0709 - val_rmsle_cust: 0.0084\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 281ms/step - loss: 0.0051 - mae: 0.0501 - rmsle_cust: 0.0048 - val_loss: 0.0107 - val_mae: 0.0709 - val_rmsle_cust: 0.0083\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 1s 253ms/step - loss: 0.0050 - mae: 0.0490 - rmsle_cust: 0.0045 - val_loss: 0.0107 - val_mae: 0.0705 - val_rmsle_cust: 0.0083\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 1s 245ms/step - loss: 0.0049 - mae: 0.0491 - rmsle_cust: 0.0046 - val_loss: 0.0107 - val_mae: 0.0700 - val_rmsle_cust: 0.0083\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 1s 240ms/step - loss: 0.0047 - mae: 0.0480 - rmsle_cust: 0.0042 - val_loss: 0.0107 - val_mae: 0.0698 - val_rmsle_cust: 0.0083\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 1s 242ms/step - loss: 0.0045 - mae: 0.0475 - rmsle_cust: 0.0043 - val_loss: 0.0107 - val_mae: 0.0697 - val_rmsle_cust: 0.0083\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 1s 238ms/step - loss: 0.0044 - mae: 0.0466 - rmsle_cust: 0.0041 - val_loss: 0.0107 - val_mae: 0.0696 - val_rmsle_cust: 0.0084\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 1s 307ms/step - loss: 0.0044 - mae: 0.0459 - rmsle_cust: 0.0041 - val_loss: 0.0107 - val_mae: 0.0694 - val_rmsle_cust: 0.0085\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 1s 254ms/step - loss: 0.0043 - mae: 0.0459 - rmsle_cust: 0.0041 - val_loss: 0.0107 - val_mae: 0.0692 - val_rmsle_cust: 0.0085\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 1s 247ms/step - loss: 0.0043 - mae: 0.0462 - rmsle_cust: 0.0041 - val_loss: 0.0107 - val_mae: 0.0691 - val_rmsle_cust: 0.0085\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 1s 239ms/step - loss: 0.0041 - mae: 0.0448 - rmsle_cust: 0.0040 - val_loss: 0.0107 - val_mae: 0.0685 - val_rmsle_cust: 0.0085\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 1s 264ms/step - loss: 0.0041 - mae: 0.0447 - rmsle_cust: 0.0039 - val_loss: 0.0107 - val_mae: 0.0682 - val_rmsle_cust: 0.0085\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 242ms/step - loss: 0.0039 - mae: 0.0431 - rmsle_cust: 0.0038 - val_loss: 0.0108 - val_mae: 0.0687 - val_rmsle_cust: 0.0085\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 1s 244ms/step - loss: 0.0039 - mae: 0.0436 - rmsle_cust: 0.0037 - val_loss: 0.0108 - val_mae: 0.0689 - val_rmsle_cust: 0.0086\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 1s 244ms/step - loss: 0.0039 - mae: 0.0434 - rmsle_cust: 0.0036 - val_loss: 0.0108 - val_mae: 0.0688 - val_rmsle_cust: 0.0086\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 1s 228ms/step - loss: 0.0038 - mae: 0.0427 - rmsle_cust: 0.0036 - val_loss: 0.0108 - val_mae: 0.0688 - val_rmsle_cust: 0.0086\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 213ms/step - loss: 0.0038 - mae: 0.0425 - rmsle_cust: 0.0034 - val_loss: 0.0108 - val_mae: 0.0687 - val_rmsle_cust: 0.0086\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 289ms/step - loss: 0.0036 - mae: 0.0422 - rmsle_cust: 0.0034 - val_loss: 0.0108 - val_mae: 0.0686 - val_rmsle_cust: 0.0087\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 1s 217ms/step - loss: 0.0036 - mae: 0.0422 - rmsle_cust: 0.0035 - val_loss: 0.0108 - val_mae: 0.0686 - val_rmsle_cust: 0.0087\n",
      "11/11 [==============================] - 1s 7ms/step\n",
      "Train: [   0    1    2 ... 3441 3444 3445] Validation: [  17   19   34   44   47   54   58   72   79   81   91  108  120  138\n",
      "  151  169  177  180  181  186  194  218  221  233  236  241  250  256\n",
      "  260  270  275  285  303  304  335  341  359  370  415  422  427  428\n",
      "  438  445  457  471  485  499  525  527  533  549  553  555  571  573\n",
      "  576  579  584  587  589  599  602  607  616  620  627  635  636  664\n",
      "  675  681  685  688  693  707  713  720  729  751  773  781  783  791\n",
      "  812  829  836  846  857  866  885  896  898  909  918  921  931  932\n",
      "  936  945  958  959  990 1008 1010 1012 1026 1028 1030 1038 1040 1047\n",
      " 1050 1051 1070 1078 1080 1082 1089 1105 1120 1132 1143 1149 1156 1162\n",
      " 1176 1194 1197 1208 1215 1219 1236 1238 1248 1266 1281 1296 1299 1322\n",
      " 1329 1330 1336 1342 1352 1357 1359 1377 1379 1380 1381 1391 1395 1403\n",
      " 1404 1410 1435 1443 1448 1466 1498 1506 1515 1525 1532 1549 1551 1556\n",
      " 1559 1563 1592 1604 1637 1657 1663 1678 1684 1716 1721 1728 1735 1736\n",
      " 1747 1749 1758 1768 1787 1794 1804 1813 1815 1826 1836 1837 1846 1852\n",
      " 1856 1882 1884 1892 1909 1916 1921 1931 1934 1944 1947 1958 1962 1966\n",
      " 1973 1986 1992 1995 1999 2001 2006 2018 2024 2032 2046 2074 2098 2117\n",
      " 2118 2121 2142 2150 2160 2162 2165 2181 2185 2191 2198 2207 2240 2253\n",
      " 2271 2275 2276 2290 2293 2306 2313 2315 2318 2329 2332 2334 2346 2353\n",
      " 2356 2377 2380 2388 2399 2401 2411 2424 2431 2451 2454 2456 2457 2469\n",
      " 2473 2512 2528 2544 2550 2552 2557 2568 2569 2577 2603 2618 2639 2652\n",
      " 2654 2660 2668 2673 2684 2698 2701 2717 2731 2743 2747 2749 2755 2774\n",
      " 2776 2786 2798 2807 2820 2823 2825 2851 2859 2864 2886 2913 2915 2932\n",
      " 2938 2939 2954 2956 2960 2968 3004 3011 3020 3025 3052 3074 3105 3108\n",
      " 3158 3163 3173 3175 3201 3202 3223 3235 3280 3281 3290 3298 3306 3308\n",
      " 3318 3371 3395 3400 3407 3416 3440 3442 3443]\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 5s 837ms/step - loss: 0.1855 - mae: 0.3579 - rmsle_cust: 0.0133 - val_loss: 0.1325 - val_mae: 0.2939 - val_rmsle_cust: 0.0114\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 1s 189ms/step - loss: 0.1249 - mae: 0.2833 - rmsle_cust: 0.0097 - val_loss: 0.0859 - val_mae: 0.2285 - val_rmsle_cust: 0.0114\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 1s 203ms/step - loss: 0.0800 - mae: 0.2192 - rmsle_cust: 0.0097 - val_loss: 0.0594 - val_mae: 0.1953 - val_rmsle_cust: 0.0114\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 222ms/step - loss: 0.0560 - mae: 0.1884 - rmsle_cust: 0.0097 - val_loss: 0.0581 - val_mae: 0.2005 - val_rmsle_cust: 0.0114\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 203ms/step - loss: 0.0567 - mae: 0.1985 - rmsle_cust: 0.0097 - val_loss: 0.0604 - val_mae: 0.2034 - val_rmsle_cust: 0.0114\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 199ms/step - loss: 0.0575 - mae: 0.1988 - rmsle_cust: 0.0097 - val_loss: 0.0496 - val_mae: 0.1822 - val_rmsle_cust: 0.0114\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 1s 193ms/step - loss: 0.0454 - mae: 0.1730 - rmsle_cust: 0.0097 - val_loss: 0.0379 - val_mae: 0.1568 - val_rmsle_cust: 0.0114\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 208ms/step - loss: 0.0341 - mae: 0.1443 - rmsle_cust: 0.0097 - val_loss: 0.0334 - val_mae: 0.1431 - val_rmsle_cust: 0.0114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 196ms/step - loss: 0.0305 - mae: 0.1339 - rmsle_cust: 0.0096 - val_loss: 0.0327 - val_mae: 0.1408 - val_rmsle_cust: 0.0118\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 195ms/step - loss: 0.0300 - mae: 0.1344 - rmsle_cust: 0.0098 - val_loss: 0.0301 - val_mae: 0.1351 - val_rmsle_cust: 0.0121\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 183ms/step - loss: 0.0272 - mae: 0.1297 - rmsle_cust: 0.0099 - val_loss: 0.0249 - val_mae: 0.1209 - val_rmsle_cust: 0.0123\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 200ms/step - loss: 0.0221 - mae: 0.1144 - rmsle_cust: 0.0104 - val_loss: 0.0210 - val_mae: 0.1068 - val_rmsle_cust: 0.0122\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 200ms/step - loss: 0.0189 - mae: 0.1024 - rmsle_cust: 0.0102 - val_loss: 0.0206 - val_mae: 0.1036 - val_rmsle_cust: 0.0118\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 214ms/step - loss: 0.0187 - mae: 0.0984 - rmsle_cust: 0.0099 - val_loss: 0.0210 - val_mae: 0.1046 - val_rmsle_cust: 0.0114\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 195ms/step - loss: 0.0189 - mae: 0.1002 - rmsle_cust: 0.0097 - val_loss: 0.0190 - val_mae: 0.0982 - val_rmsle_cust: 0.0108\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 203ms/step - loss: 0.0167 - mae: 0.0940 - rmsle_cust: 0.0098 - val_loss: 0.0163 - val_mae: 0.0894 - val_rmsle_cust: 0.0103\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 214ms/step - loss: 0.0136 - mae: 0.0843 - rmsle_cust: 0.0095 - val_loss: 0.0152 - val_mae: 0.0887 - val_rmsle_cust: 0.0099\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 211ms/step - loss: 0.0121 - mae: 0.0814 - rmsle_cust: 0.0091 - val_loss: 0.0149 - val_mae: 0.0888 - val_rmsle_cust: 0.0092\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 226ms/step - loss: 0.0119 - mae: 0.0807 - rmsle_cust: 0.0084 - val_loss: 0.0141 - val_mae: 0.0860 - val_rmsle_cust: 0.0088\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 215ms/step - loss: 0.0101 - mae: 0.0730 - rmsle_cust: 0.0071 - val_loss: 0.0131 - val_mae: 0.0812 - val_rmsle_cust: 0.0084\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 239ms/step - loss: 0.0091 - mae: 0.0678 - rmsle_cust: 0.0063 - val_loss: 0.0127 - val_mae: 0.0794 - val_rmsle_cust: 0.0083\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 384ms/step - loss: 0.0088 - mae: 0.0671 - rmsle_cust: 0.0059 - val_loss: 0.0128 - val_mae: 0.0798 - val_rmsle_cust: 0.0081\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 222ms/step - loss: 0.0084 - mae: 0.0669 - rmsle_cust: 0.0058 - val_loss: 0.0124 - val_mae: 0.0780 - val_rmsle_cust: 0.0079\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 229ms/step - loss: 0.0081 - mae: 0.0658 - rmsle_cust: 0.0057 - val_loss: 0.0119 - val_mae: 0.0767 - val_rmsle_cust: 0.0079\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 1s 221ms/step - loss: 0.0073 - mae: 0.0612 - rmsle_cust: 0.0058 - val_loss: 0.0119 - val_mae: 0.0771 - val_rmsle_cust: 0.0079\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 216ms/step - loss: 0.0070 - mae: 0.0602 - rmsle_cust: 0.0056 - val_loss: 0.0118 - val_mae: 0.0770 - val_rmsle_cust: 0.0080\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 250ms/step - loss: 0.0068 - mae: 0.0590 - rmsle_cust: 0.0057 - val_loss: 0.0115 - val_mae: 0.0748 - val_rmsle_cust: 0.0079\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 246ms/step - loss: 0.0061 - mae: 0.0556 - rmsle_cust: 0.0055 - val_loss: 0.0112 - val_mae: 0.0725 - val_rmsle_cust: 0.0078\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 246ms/step - loss: 0.0061 - mae: 0.0555 - rmsle_cust: 0.0054 - val_loss: 0.0112 - val_mae: 0.0724 - val_rmsle_cust: 0.0078\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 209ms/step - loss: 0.0058 - mae: 0.0545 - rmsle_cust: 0.0051 - val_loss: 0.0111 - val_mae: 0.0718 - val_rmsle_cust: 0.0078\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 1s 209ms/step - loss: 0.0055 - mae: 0.0521 - rmsle_cust: 0.0048 - val_loss: 0.0110 - val_mae: 0.0710 - val_rmsle_cust: 0.0078\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 1s 220ms/step - loss: 0.0053 - mae: 0.0509 - rmsle_cust: 0.0049 - val_loss: 0.0111 - val_mae: 0.0716 - val_rmsle_cust: 0.0077\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 197ms/step - loss: 0.0050 - mae: 0.0497 - rmsle_cust: 0.0048 - val_loss: 0.0111 - val_mae: 0.0718 - val_rmsle_cust: 0.0077\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 1s 220ms/step - loss: 0.0049 - mae: 0.0495 - rmsle_cust: 0.0047 - val_loss: 0.0110 - val_mae: 0.0707 - val_rmsle_cust: 0.0076\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 1s 220ms/step - loss: 0.0047 - mae: 0.0480 - rmsle_cust: 0.0046 - val_loss: 0.0109 - val_mae: 0.0695 - val_rmsle_cust: 0.0076\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 1s 212ms/step - loss: 0.0045 - mae: 0.0471 - rmsle_cust: 0.0043 - val_loss: 0.0108 - val_mae: 0.0690 - val_rmsle_cust: 0.0075\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 1s 208ms/step - loss: 0.0043 - mae: 0.0458 - rmsle_cust: 0.0042 - val_loss: 0.0107 - val_mae: 0.0687 - val_rmsle_cust: 0.0074\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 1s 219ms/step - loss: 0.0043 - mae: 0.0464 - rmsle_cust: 0.0043 - val_loss: 0.0107 - val_mae: 0.0685 - val_rmsle_cust: 0.0073\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 1s 202ms/step - loss: 0.0041 - mae: 0.0454 - rmsle_cust: 0.0042 - val_loss: 0.0107 - val_mae: 0.0688 - val_rmsle_cust: 0.0073\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 1s 205ms/step - loss: 0.0041 - mae: 0.0444 - rmsle_cust: 0.0041 - val_loss: 0.0108 - val_mae: 0.0692 - val_rmsle_cust: 0.0073\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 1s 202ms/step - loss: 0.0038 - mae: 0.0435 - rmsle_cust: 0.0040 - val_loss: 0.0108 - val_mae: 0.0695 - val_rmsle_cust: 0.0073\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 1s 207ms/step - loss: 0.0038 - mae: 0.0439 - rmsle_cust: 0.0040 - val_loss: 0.0109 - val_mae: 0.0698 - val_rmsle_cust: 0.0073\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 1s 217ms/step - loss: 0.0037 - mae: 0.0430 - rmsle_cust: 0.0038 - val_loss: 0.0109 - val_mae: 0.0698 - val_rmsle_cust: 0.0073\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 210ms/step - loss: 0.0036 - mae: 0.0421 - rmsle_cust: 0.0038 - val_loss: 0.0109 - val_mae: 0.0697 - val_rmsle_cust: 0.0073\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 1s 220ms/step - loss: 0.0037 - mae: 0.0429 - rmsle_cust: 0.0037 - val_loss: 0.0109 - val_mae: 0.0693 - val_rmsle_cust: 0.0073\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 1s 220ms/step - loss: 0.0035 - mae: 0.0418 - rmsle_cust: 0.0037 - val_loss: 0.0109 - val_mae: 0.0688 - val_rmsle_cust: 0.0074\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 1s 266ms/step - loss: 0.0034 - mae: 0.0409 - rmsle_cust: 0.0035 - val_loss: 0.0109 - val_mae: 0.0687 - val_rmsle_cust: 0.0074\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 204ms/step - loss: 0.0033 - mae: 0.0408 - rmsle_cust: 0.0036 - val_loss: 0.0109 - val_mae: 0.0688 - val_rmsle_cust: 0.0074\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 219ms/step - loss: 0.0034 - mae: 0.0406 - rmsle_cust: 0.0033 - val_loss: 0.0109 - val_mae: 0.0689 - val_rmsle_cust: 0.0075\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 1s 207ms/step - loss: 0.0032 - mae: 0.0394 - rmsle_cust: 0.0035 - val_loss: 0.0109 - val_mae: 0.0689 - val_rmsle_cust: 0.0075\n",
      "11/11 [==============================] - 1s 8ms/step\n",
      "Train: [   0    1    2 ... 3441 3442 3443] Validation: [  23   32   35   39   56   64   82  103  119  124  130  132  134  148\n",
      "  154  166  182  191  196  199  208  216  217  226  242  257  283  287\n",
      "  288  302  338  348  365  390  401  414  418  425  429  444  446  452\n",
      "  459  468  476  477  478  480  492  501  503  508  510  514  526  543\n",
      "  556  559  561  572  578  590  608  612  615  638  645  653  654  655\n",
      "  660  670  674  679  680  683  692  703  708  711  712  733  735  736\n",
      "  737  743  760  764  767  768  785  788  817  840  858  877  911  916\n",
      "  924  934  935  937  954  961  987 1000 1009 1011 1016 1068 1077 1081\n",
      " 1117 1140 1151 1159 1188 1216 1223 1224 1227 1228 1241 1246 1255 1260\n",
      " 1264 1282 1307 1334 1338 1339 1353 1363 1364 1392 1394 1398 1400 1408\n",
      " 1412 1433 1447 1457 1467 1468 1475 1488 1496 1513 1536 1540 1544 1555\n",
      " 1566 1569 1577 1579 1587 1595 1596 1603 1623 1624 1625 1626 1642 1644\n",
      " 1653 1695 1696 1703 1707 1708 1724 1730 1734 1742 1766 1776 1778 1783\n",
      " 1784 1785 1791 1802 1807 1808 1850 1859 1860 1870 1872 1893 1897 1902\n",
      " 1915 1922 1926 1937 1946 1951 1959 1967 1990 2008 2013 2026 2028 2029\n",
      " 2030 2035 2043 2064 2068 2093 2107 2111 2132 2136 2152 2166 2201 2211\n",
      " 2223 2227 2241 2248 2273 2282 2284 2300 2303 2328 2336 2343 2344 2345\n",
      " 2354 2369 2371 2387 2392 2406 2409 2421 2422 2427 2467 2472 2487 2503\n",
      " 2513 2515 2519 2545 2554 2567 2597 2635 2640 2641 2645 2647 2649 2651\n",
      " 2664 2679 2680 2681 2683 2707 2724 2726 2738 2753 2758 2759 2782 2789\n",
      " 2794 2811 2812 2834 2846 2848 2850 2873 2876 2879 2891 2900 2905 2923\n",
      " 2976 2986 2993 2995 3000 3007 3008 3024 3048 3051 3054 3059 3060 3062\n",
      " 3070 3092 3094 3101 3103 3114 3119 3122 3127 3129 3135 3148 3164 3182\n",
      " 3183 3186 3187 3189 3199 3211 3226 3246 3256 3271 3283 3289 3304 3309\n",
      " 3325 3341 3355 3357 3369 3370 3390 3444 3445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2/2 [==============================] - 5s 904ms/step - loss: 0.1808 - mae: 0.3478 - rmsle_cust: 0.0115 - val_loss: 0.1169 - val_mae: 0.2673 - val_rmsle_cust: 0.0105\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 1s 198ms/step - loss: 0.1200 - mae: 0.2720 - rmsle_cust: 0.0098 - val_loss: 0.0749 - val_mae: 0.2104 - val_rmsle_cust: 0.0105\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 1s 184ms/step - loss: 0.0772 - mae: 0.2162 - rmsle_cust: 0.0098 - val_loss: 0.0587 - val_mae: 0.1975 - val_rmsle_cust: 0.0105\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 191ms/step - loss: 0.0606 - mae: 0.2036 - rmsle_cust: 0.0098 - val_loss: 0.0619 - val_mae: 0.2083 - val_rmsle_cust: 0.0105\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 181ms/step - loss: 0.0605 - mae: 0.2077 - rmsle_cust: 0.0098 - val_loss: 0.0552 - val_mae: 0.1933 - val_rmsle_cust: 0.0105\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 191ms/step - loss: 0.0518 - mae: 0.1899 - rmsle_cust: 0.0098 - val_loss: 0.0391 - val_mae: 0.1553 - val_rmsle_cust: 0.0105\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 1s 183ms/step - loss: 0.0361 - mae: 0.1525 - rmsle_cust: 0.0098 - val_loss: 0.0288 - val_mae: 0.1283 - val_rmsle_cust: 0.0105\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 201ms/step - loss: 0.0277 - mae: 0.1267 - rmsle_cust: 0.0098 - val_loss: 0.0271 - val_mae: 0.1298 - val_rmsle_cust: 0.0106\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 186ms/step - loss: 0.0262 - mae: 0.1250 - rmsle_cust: 0.0095 - val_loss: 0.0256 - val_mae: 0.1297 - val_rmsle_cust: 0.0108\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 187ms/step - loss: 0.0237 - mae: 0.1220 - rmsle_cust: 0.0101 - val_loss: 0.0208 - val_mae: 0.1135 - val_rmsle_cust: 0.0111\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 177ms/step - loss: 0.0180 - mae: 0.1033 - rmsle_cust: 0.0104 - val_loss: 0.0184 - val_mae: 0.1011 - val_rmsle_cust: 0.0107\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 194ms/step - loss: 0.0157 - mae: 0.0920 - rmsle_cust: 0.0096 - val_loss: 0.0200 - val_mae: 0.1074 - val_rmsle_cust: 0.0103\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 177ms/step - loss: 0.0173 - mae: 0.0975 - rmsle_cust: 0.0090 - val_loss: 0.0190 - val_mae: 0.1047 - val_rmsle_cust: 0.0100\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 180ms/step - loss: 0.0155 - mae: 0.0914 - rmsle_cust: 0.0088 - val_loss: 0.0159 - val_mae: 0.0945 - val_rmsle_cust: 0.0098\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 204ms/step - loss: 0.0121 - mae: 0.0816 - rmsle_cust: 0.0091 - val_loss: 0.0149 - val_mae: 0.0914 - val_rmsle_cust: 0.0090\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 203ms/step - loss: 0.0112 - mae: 0.0797 - rmsle_cust: 0.0091 - val_loss: 0.0150 - val_mae: 0.0928 - val_rmsle_cust: 0.0083\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 204ms/step - loss: 0.0110 - mae: 0.0789 - rmsle_cust: 0.0078 - val_loss: 0.0140 - val_mae: 0.0880 - val_rmsle_cust: 0.0078\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 202ms/step - loss: 0.0095 - mae: 0.0718 - rmsle_cust: 0.0067 - val_loss: 0.0130 - val_mae: 0.0809 - val_rmsle_cust: 0.0076\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 213ms/step - loss: 0.0084 - mae: 0.0655 - rmsle_cust: 0.0060 - val_loss: 0.0129 - val_mae: 0.0788 - val_rmsle_cust: 0.0078\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 202ms/step - loss: 0.0084 - mae: 0.0659 - rmsle_cust: 0.0058 - val_loss: 0.0130 - val_mae: 0.0799 - val_rmsle_cust: 0.0078\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 186ms/step - loss: 0.0080 - mae: 0.0642 - rmsle_cust: 0.0056 - val_loss: 0.0124 - val_mae: 0.0783 - val_rmsle_cust: 0.0076\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 195ms/step - loss: 0.0073 - mae: 0.0609 - rmsle_cust: 0.0054 - val_loss: 0.0120 - val_mae: 0.0766 - val_rmsle_cust: 0.0074\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 234ms/step - loss: 0.0068 - mae: 0.0578 - rmsle_cust: 0.0052 - val_loss: 0.0119 - val_mae: 0.0771 - val_rmsle_cust: 0.0072\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 195ms/step - loss: 0.0064 - mae: 0.0568 - rmsle_cust: 0.0053 - val_loss: 0.0117 - val_mae: 0.0760 - val_rmsle_cust: 0.0071\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 1s 204ms/step - loss: 0.0065 - mae: 0.0572 - rmsle_cust: 0.0057 - val_loss: 0.0112 - val_mae: 0.0728 - val_rmsle_cust: 0.0070\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 223ms/step - loss: 0.0059 - mae: 0.0547 - rmsle_cust: 0.0055 - val_loss: 0.0110 - val_mae: 0.0712 - val_rmsle_cust: 0.0069\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 205ms/step - loss: 0.0058 - mae: 0.0539 - rmsle_cust: 0.0053 - val_loss: 0.0109 - val_mae: 0.0715 - val_rmsle_cust: 0.0068\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 213ms/step - loss: 0.0055 - mae: 0.0533 - rmsle_cust: 0.0051 - val_loss: 0.0107 - val_mae: 0.0704 - val_rmsle_cust: 0.0068\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 193ms/step - loss: 0.0053 - mae: 0.0516 - rmsle_cust: 0.0050 - val_loss: 0.0105 - val_mae: 0.0694 - val_rmsle_cust: 0.0068\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 207ms/step - loss: 0.0050 - mae: 0.0510 - rmsle_cust: 0.0048 - val_loss: 0.0105 - val_mae: 0.0699 - val_rmsle_cust: 0.0069\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 1s 205ms/step - loss: 0.0047 - mae: 0.0491 - rmsle_cust: 0.0049 - val_loss: 0.0104 - val_mae: 0.0699 - val_rmsle_cust: 0.0069\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 1s 201ms/step - loss: 0.0048 - mae: 0.0484 - rmsle_cust: 0.0046 - val_loss: 0.0103 - val_mae: 0.0689 - val_rmsle_cust: 0.0069\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 202ms/step - loss: 0.0045 - mae: 0.0471 - rmsle_cust: 0.0043 - val_loss: 0.0102 - val_mae: 0.0682 - val_rmsle_cust: 0.0070\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 1s 191ms/step - loss: 0.0045 - mae: 0.0467 - rmsle_cust: 0.0043 - val_loss: 0.0102 - val_mae: 0.0680 - val_rmsle_cust: 0.0069\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 1s 195ms/step - loss: 0.0044 - mae: 0.0466 - rmsle_cust: 0.0042 - val_loss: 0.0101 - val_mae: 0.0676 - val_rmsle_cust: 0.0067\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 1s 195ms/step - loss: 0.0043 - mae: 0.0464 - rmsle_cust: 0.0041 - val_loss: 0.0100 - val_mae: 0.0669 - val_rmsle_cust: 0.0066\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 1s 195ms/step - loss: 0.0042 - mae: 0.0460 - rmsle_cust: 0.0041 - val_loss: 0.0100 - val_mae: 0.0663 - val_rmsle_cust: 0.0064\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 1s 221ms/step - loss: 0.0039 - mae: 0.0442 - rmsle_cust: 0.0039 - val_loss: 0.0100 - val_mae: 0.0661 - val_rmsle_cust: 0.0063\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 1s 213ms/step - loss: 0.0039 - mae: 0.0442 - rmsle_cust: 0.0041 - val_loss: 0.0099 - val_mae: 0.0660 - val_rmsle_cust: 0.0063\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 1s 233ms/step - loss: 0.0038 - mae: 0.0437 - rmsle_cust: 0.0040 - val_loss: 0.0099 - val_mae: 0.0658 - val_rmsle_cust: 0.0063\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 1s 207ms/step - loss: 0.0036 - mae: 0.0423 - rmsle_cust: 0.0037 - val_loss: 0.0099 - val_mae: 0.0655 - val_rmsle_cust: 0.0063\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 1s 197ms/step - loss: 0.0036 - mae: 0.0426 - rmsle_cust: 0.0035 - val_loss: 0.0099 - val_mae: 0.0658 - val_rmsle_cust: 0.0064\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 1s 242ms/step - loss: 0.0035 - mae: 0.0421 - rmsle_cust: 0.0037 - val_loss: 0.0099 - val_mae: 0.0659 - val_rmsle_cust: 0.0065\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 193ms/step - loss: 0.0034 - mae: 0.0407 - rmsle_cust: 0.0035 - val_loss: 0.0099 - val_mae: 0.0657 - val_rmsle_cust: 0.0065\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 1s 227ms/step - loss: 0.0034 - mae: 0.0411 - rmsle_cust: 0.0035 - val_loss: 0.0098 - val_mae: 0.0654 - val_rmsle_cust: 0.0065\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 1s 194ms/step - loss: 0.0032 - mae: 0.0401 - rmsle_cust: 0.0035 - val_loss: 0.0098 - val_mae: 0.0654 - val_rmsle_cust: 0.0064\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 231ms/step - loss: 0.0032 - mae: 0.0402 - rmsle_cust: 0.0035 - val_loss: 0.0097 - val_mae: 0.0654 - val_rmsle_cust: 0.0063\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 195ms/step - loss: 0.0032 - mae: 0.0401 - rmsle_cust: 0.0034 - val_loss: 0.0096 - val_mae: 0.0648 - val_rmsle_cust: 0.0063\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 217ms/step - loss: 0.0031 - mae: 0.0396 - rmsle_cust: 0.0034 - val_loss: 0.0095 - val_mae: 0.0644 - val_rmsle_cust: 0.0063\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 1s 214ms/step - loss: 0.0031 - mae: 0.0396 - rmsle_cust: 0.0033 - val_loss: 0.0095 - val_mae: 0.0643 - val_rmsle_cust: 0.0063\n",
      "11/11 [==============================] - 1s 9ms/step\n",
      "Train: [   0    2    3 ... 3443 3444 3445] Validation: [   1    8   11   16   24   30   37   38   48   62   76   90  101  115\n",
      "  128  140  173  189  190  198  200  206  207  212  223  229  258  261\n",
      "  293  294  325  332  345  350  353  363  372  375  385  389  398  400\n",
      "  406  421  441  443  447  448  484  493  520  523  583  585  614  628\n",
      "  631  639  640  642  669  689  697  698  718  745  747  750  763  769\n",
      "  778  787  807  820  821  823  828  852  859  864  881  891  894  900\n",
      "  917  940  972  977  983  985  992  997 1001 1013 1015 1027 1053 1058\n",
      " 1066 1069 1091 1094 1102 1112 1123 1129 1136 1146 1147 1161 1165 1185\n",
      " 1186 1193 1204 1205 1210 1213 1225 1226 1231 1233 1249 1287 1292 1297\n",
      " 1308 1325 1355 1365 1375 1382 1384 1388 1401 1407 1414 1430 1438 1442\n",
      " 1470 1471 1483 1508 1519 1524 1533 1534 1539 1542 1550 1552 1561 1575\n",
      " 1588 1589 1593 1605 1606 1608 1609 1610 1613 1641 1645 1651 1658 1664\n",
      " 1668 1673 1688 1725 1727 1729 1748 1761 1781 1789 1790 1796 1810 1825\n",
      " 1833 1847 1854 1857 1865 1873 1885 1895 1900 1911 1918 1933 1943 1964\n",
      " 2000 2011 2016 2037 2040 2049 2054 2059 2065 2070 2088 2096 2101 2102\n",
      " 2108 2112 2124 2129 2135 2164 2174 2183 2190 2208 2215 2217 2225 2226\n",
      " 2229 2234 2242 2249 2264 2270 2289 2292 2298 2301 2307 2314 2320 2325\n",
      " 2333 2338 2372 2374 2402 2404 2418 2420 2425 2426 2429 2433 2443 2444\n",
      " 2458 2462 2463 2484 2489 2491 2494 2496 2510 2521 2527 2534 2538 2551\n",
      " 2553 2575 2579 2600 2604 2606 2612 2632 2659 2661 2665 2671 2686 2687\n",
      " 2694 2696 2715 2718 2741 2748 2767 2779 2805 2809 2828 2845 2870 2871\n",
      " 2874 2890 2909 2918 2931 2946 2966 2972 2977 2997 3003 3033 3058 3061\n",
      " 3063 3075 3099 3102 3118 3123 3132 3139 3153 3154 3155 3169 3177 3194\n",
      " 3196 3198 3218 3249 3261 3264 3286 3295 3297 3305 3315 3317 3319 3328\n",
      " 3344 3360 3386 3388 3398 3421 3426 3433 3434]\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.1754 - mae: 0.3449 - rmsle_cust: 0.0121 - val_loss: 0.1160 - val_mae: 0.2669 - val_rmsle_cust: 0.0101\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 1s 248ms/step - loss: 0.1118 - mae: 0.2607 - rmsle_cust: 0.0098 - val_loss: 0.0714 - val_mae: 0.2005 - val_rmsle_cust: 0.0101\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 1s 262ms/step - loss: 0.0703 - mae: 0.2050 - rmsle_cust: 0.0098 - val_loss: 0.0566 - val_mae: 0.1934 - val_rmsle_cust: 0.0101\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 2s 605ms/step - loss: 0.0579 - mae: 0.2020 - rmsle_cust: 0.0098 - val_loss: 0.0622 - val_mae: 0.2100 - val_rmsle_cust: 0.0101\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 287ms/step - loss: 0.0619 - mae: 0.2136 - rmsle_cust: 0.0098 - val_loss: 0.0558 - val_mae: 0.1961 - val_rmsle_cust: 0.0101\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 272ms/step - loss: 0.0536 - mae: 0.1965 - rmsle_cust: 0.0098 - val_loss: 0.0418 - val_mae: 0.1650 - val_rmsle_cust: 0.0101\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 2s 429ms/step - loss: 0.0392 - mae: 0.1642 - rmsle_cust: 0.0098 - val_loss: 0.0333 - val_mae: 0.1387 - val_rmsle_cust: 0.0100\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 307ms/step - loss: 0.0309 - mae: 0.1395 - rmsle_cust: 0.0094 - val_loss: 0.0310 - val_mae: 0.1298 - val_rmsle_cust: 0.0096\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 0.0280 - mae: 0.1289 - rmsle_cust: 0.0083 - val_loss: 0.0294 - val_mae: 0.1267 - val_rmsle_cust: 0.0093\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 259ms/step - loss: 0.0259 - mae: 0.1245 - rmsle_cust: 0.0078 - val_loss: 0.0256 - val_mae: 0.1184 - val_rmsle_cust: 0.0096\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 311ms/step - loss: 0.0211 - mae: 0.1129 - rmsle_cust: 0.0080 - val_loss: 0.0214 - val_mae: 0.1054 - val_rmsle_cust: 0.0099\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 302ms/step - loss: 0.0162 - mae: 0.0967 - rmsle_cust: 0.0085 - val_loss: 0.0202 - val_mae: 0.0994 - val_rmsle_cust: 0.0100\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 0.0146 - mae: 0.0888 - rmsle_cust: 0.0088 - val_loss: 0.0221 - val_mae: 0.1046 - val_rmsle_cust: 0.0101\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 2s 377ms/step - loss: 0.0160 - mae: 0.0927 - rmsle_cust: 0.0086 - val_loss: 0.0222 - val_mae: 0.1060 - val_rmsle_cust: 0.0101\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 298ms/step - loss: 0.0157 - mae: 0.0916 - rmsle_cust: 0.0087 - val_loss: 0.0195 - val_mae: 0.0985 - val_rmsle_cust: 0.0099\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 263ms/step - loss: 0.0134 - mae: 0.0843 - rmsle_cust: 0.0086 - val_loss: 0.0173 - val_mae: 0.0942 - val_rmsle_cust: 0.0097\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 290ms/step - loss: 0.0114 - mae: 0.0782 - rmsle_cust: 0.0083 - val_loss: 0.0166 - val_mae: 0.0940 - val_rmsle_cust: 0.0093\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 292ms/step - loss: 0.0106 - mae: 0.0763 - rmsle_cust: 0.0073 - val_loss: 0.0160 - val_mae: 0.0920 - val_rmsle_cust: 0.0087\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 272ms/step - loss: 0.0100 - mae: 0.0739 - rmsle_cust: 0.0064 - val_loss: 0.0149 - val_mae: 0.0868 - val_rmsle_cust: 0.0081\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 440ms/step - loss: 0.0088 - mae: 0.0672 - rmsle_cust: 0.0056 - val_loss: 0.0142 - val_mae: 0.0828 - val_rmsle_cust: 0.0081\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 466ms/step - loss: 0.0083 - mae: 0.0645 - rmsle_cust: 0.0055 - val_loss: 0.0142 - val_mae: 0.0820 - val_rmsle_cust: 0.0081\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 282ms/step - loss: 0.0082 - mae: 0.0647 - rmsle_cust: 0.0054 - val_loss: 0.0141 - val_mae: 0.0817 - val_rmsle_cust: 0.0080\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 295ms/step - loss: 0.0080 - mae: 0.0643 - rmsle_cust: 0.0053 - val_loss: 0.0135 - val_mae: 0.0805 - val_rmsle_cust: 0.0078\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 294ms/step - loss: 0.0074 - mae: 0.0612 - rmsle_cust: 0.0051 - val_loss: 0.0129 - val_mae: 0.0801 - val_rmsle_cust: 0.0076\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 1s 266ms/step - loss: 0.0069 - mae: 0.0582 - rmsle_cust: 0.0048 - val_loss: 0.0127 - val_mae: 0.0803 - val_rmsle_cust: 0.0073\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 268ms/step - loss: 0.0067 - mae: 0.0575 - rmsle_cust: 0.0049 - val_loss: 0.0125 - val_mae: 0.0798 - val_rmsle_cust: 0.0072\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 352ms/step - loss: 0.0067 - mae: 0.0577 - rmsle_cust: 0.0051 - val_loss: 0.0120 - val_mae: 0.0774 - val_rmsle_cust: 0.0071\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 244ms/step - loss: 0.0064 - mae: 0.0561 - rmsle_cust: 0.0050 - val_loss: 0.0116 - val_mae: 0.0747 - val_rmsle_cust: 0.0070\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 238ms/step - loss: 0.0061 - mae: 0.0545 - rmsle_cust: 0.0050 - val_loss: 0.0114 - val_mae: 0.0740 - val_rmsle_cust: 0.0069\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 271ms/step - loss: 0.0060 - mae: 0.0537 - rmsle_cust: 0.0049 - val_loss: 0.0111 - val_mae: 0.0732 - val_rmsle_cust: 0.0068\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 1s 251ms/step - loss: 0.0059 - mae: 0.0533 - rmsle_cust: 0.0049 - val_loss: 0.0108 - val_mae: 0.0723 - val_rmsle_cust: 0.0067\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 1s 250ms/step - loss: 0.0056 - mae: 0.0514 - rmsle_cust: 0.0049 - val_loss: 0.0107 - val_mae: 0.0725 - val_rmsle_cust: 0.0067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 264ms/step - loss: 0.0052 - mae: 0.0501 - rmsle_cust: 0.0047 - val_loss: 0.0106 - val_mae: 0.0726 - val_rmsle_cust: 0.0067\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 1s 244ms/step - loss: 0.0052 - mae: 0.0504 - rmsle_cust: 0.0047 - val_loss: 0.0105 - val_mae: 0.0718 - val_rmsle_cust: 0.0066\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 1s 260ms/step - loss: 0.0050 - mae: 0.0492 - rmsle_cust: 0.0045 - val_loss: 0.0103 - val_mae: 0.0708 - val_rmsle_cust: 0.0066\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 1s 253ms/step - loss: 0.0049 - mae: 0.0484 - rmsle_cust: 0.0041 - val_loss: 0.0102 - val_mae: 0.0700 - val_rmsle_cust: 0.0066\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 1s 250ms/step - loss: 0.0048 - mae: 0.0480 - rmsle_cust: 0.0041 - val_loss: 0.0101 - val_mae: 0.0696 - val_rmsle_cust: 0.0066\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 1s 249ms/step - loss: 0.0047 - mae: 0.0478 - rmsle_cust: 0.0043 - val_loss: 0.0100 - val_mae: 0.0694 - val_rmsle_cust: 0.0065\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 1s 244ms/step - loss: 0.0045 - mae: 0.0462 - rmsle_cust: 0.0041 - val_loss: 0.0099 - val_mae: 0.0695 - val_rmsle_cust: 0.0065\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 1s 240ms/step - loss: 0.0045 - mae: 0.0463 - rmsle_cust: 0.0041 - val_loss: 0.0098 - val_mae: 0.0691 - val_rmsle_cust: 0.0065\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 1s 247ms/step - loss: 0.0044 - mae: 0.0457 - rmsle_cust: 0.0040 - val_loss: 0.0096 - val_mae: 0.0685 - val_rmsle_cust: 0.0064\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 1s 234ms/step - loss: 0.0043 - mae: 0.0449 - rmsle_cust: 0.0040 - val_loss: 0.0095 - val_mae: 0.0682 - val_rmsle_cust: 0.0064\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 1s 261ms/step - loss: 0.0041 - mae: 0.0448 - rmsle_cust: 0.0041 - val_loss: 0.0094 - val_mae: 0.0680 - val_rmsle_cust: 0.0064\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 253ms/step - loss: 0.0042 - mae: 0.0445 - rmsle_cust: 0.0038 - val_loss: 0.0094 - val_mae: 0.0677 - val_rmsle_cust: 0.0064\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 1s 246ms/step - loss: 0.0039 - mae: 0.0433 - rmsle_cust: 0.0037 - val_loss: 0.0093 - val_mae: 0.0678 - val_rmsle_cust: 0.0064\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 1s 285ms/step - loss: 0.0040 - mae: 0.0439 - rmsle_cust: 0.0039 - val_loss: 0.0093 - val_mae: 0.0678 - val_rmsle_cust: 0.0064\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 1s 232ms/step - loss: 0.0037 - mae: 0.0423 - rmsle_cust: 0.0037 - val_loss: 0.0092 - val_mae: 0.0672 - val_rmsle_cust: 0.0065\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 261ms/step - loss: 0.0039 - mae: 0.0432 - rmsle_cust: 0.0036 - val_loss: 0.0091 - val_mae: 0.0668 - val_rmsle_cust: 0.0065\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 237ms/step - loss: 0.0036 - mae: 0.0420 - rmsle_cust: 0.0035 - val_loss: 0.0091 - val_mae: 0.0665 - val_rmsle_cust: 0.0066\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 1s 249ms/step - loss: 0.0036 - mae: 0.0419 - rmsle_cust: 0.0035 - val_loss: 0.0091 - val_mae: 0.0663 - val_rmsle_cust: 0.0067\n",
      "11/11 [==============================] - 1s 7ms/step\n",
      "Train: [   1    2    3 ... 3443 3444 3445] Validation: [   0   21   49   51   53   59   60   67   68   73   80   83  104  125\n",
      "  139  146  149  158  160  167  176  202  209  210  259  264  267  281\n",
      "  305  309  336  347  361  369  380  386  399  407  416  419  424  434\n",
      "  435  436  456  498  509  512  513  518  529  536  537  575  596  619\n",
      "  625  634  643  661  662  672  701  710  726  739  758  766  779  793\n",
      "  800  801  805  809  834  845  849  851  853  887  904  919  920  948\n",
      "  970  976  981  982  995 1006 1034 1035 1037 1054 1059 1083 1084 1093\n",
      " 1097 1108 1109 1115 1118 1122 1139 1152 1157 1172 1183 1206 1212 1220\n",
      " 1244 1245 1252 1262 1269 1272 1276 1283 1285 1286 1289 1293 1310 1317\n",
      " 1323 1326 1347 1348 1354 1362 1369 1397 1402 1406 1409 1422 1446 1462\n",
      " 1480 1481 1482 1487 1489 1494 1497 1500 1514 1518 1529 1558 1570 1573\n",
      " 1586 1597 1599 1601 1617 1628 1629 1646 1648 1652 1654 1677 1681 1683\n",
      " 1691 1701 1702 1710 1726 1737 1767 1782 1812 1814 1851 1855 1864 1866\n",
      " 1875 1891 1894 1899 1905 1953 1961 1970 1991 2009 2022 2045 2048 2051\n",
      " 2053 2056 2087 2123 2126 2139 2156 2158 2163 2168 2169 2170 2179 2193\n",
      " 2204 2213 2246 2252 2279 2308 2312 2316 2330 2341 2350 2360 2362 2378\n",
      " 2381 2407 2434 2452 2453 2459 2470 2474 2476 2479 2481 2482 2483 2490\n",
      " 2508 2514 2531 2542 2555 2558 2572 2586 2598 2605 2614 2621 2624 2657\n",
      " 2682 2699 2704 2706 2708 2711 2721 2730 2744 2752 2756 2757 2761 2772\n",
      " 2781 2796 2810 2814 2815 2816 2819 2822 2833 2852 2854 2861 2862 2865\n",
      " 2902 2916 2921 2925 2933 2935 2936 2940 2945 2957 2970 2987 2990 2991\n",
      " 2998 3014 3016 3027 3028 3031 3034 3037 3041 3042 3057 3080 3083 3089\n",
      " 3097 3120 3124 3140 3147 3176 3192 3207 3210 3213 3216 3242 3250 3255\n",
      " 3273 3275 3276 3284 3292 3310 3330 3335 3338 3348 3358 3363 3364 3365\n",
      " 3382 3391 3394 3403 3431 3432 3436 3437 3439]\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 5s 897ms/step - loss: 0.1939 - mae: 0.3640 - rmsle_cust: 0.0182 - val_loss: 0.1486 - val_mae: 0.3119 - val_rmsle_cust: 0.0056\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 1s 193ms/step - loss: 0.1366 - mae: 0.2948 - rmsle_cust: 0.0103 - val_loss: 0.0965 - val_mae: 0.2397 - val_rmsle_cust: 0.0056\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 1s 195ms/step - loss: 0.0922 - mae: 0.2355 - rmsle_cust: 0.0103 - val_loss: 0.0612 - val_mae: 0.1963 - val_rmsle_cust: 0.0056\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 191ms/step - loss: 0.0648 - mae: 0.2042 - rmsle_cust: 0.0103 - val_loss: 0.0515 - val_mae: 0.1959 - val_rmsle_cust: 0.0056\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 195ms/step - loss: 0.0619 - mae: 0.2096 - rmsle_cust: 0.0103 - val_loss: 0.0531 - val_mae: 0.2008 - val_rmsle_cust: 0.0056\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 200ms/step - loss: 0.0640 - mae: 0.2126 - rmsle_cust: 0.0103 - val_loss: 0.0440 - val_mae: 0.1808 - val_rmsle_cust: 0.0056\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 1s 264ms/step - loss: 0.0514 - mae: 0.1885 - rmsle_cust: 0.0103 - val_loss: 0.0336 - val_mae: 0.1531 - val_rmsle_cust: 0.0056\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 212ms/step - loss: 0.0388 - mae: 0.1584 - rmsle_cust: 0.0103 - val_loss: 0.0305 - val_mae: 0.1387 - val_rmsle_cust: 0.0056\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 221ms/step - loss: 0.0331 - mae: 0.1405 - rmsle_cust: 0.0104 - val_loss: 0.0307 - val_mae: 0.1371 - val_rmsle_cust: 0.0059\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 219ms/step - loss: 0.0310 - mae: 0.1360 - rmsle_cust: 0.0105 - val_loss: 0.0280 - val_mae: 0.1322 - val_rmsle_cust: 0.0061\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 230ms/step - loss: 0.0268 - mae: 0.1272 - rmsle_cust: 0.0106 - val_loss: 0.0214 - val_mae: 0.1156 - val_rmsle_cust: 0.0061\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 222ms/step - loss: 0.0204 - mae: 0.1093 - rmsle_cust: 0.0102 - val_loss: 0.0153 - val_mae: 0.0945 - val_rmsle_cust: 0.0062\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 227ms/step - loss: 0.0160 - mae: 0.0933 - rmsle_cust: 0.0099 - val_loss: 0.0145 - val_mae: 0.0885 - val_rmsle_cust: 0.0060\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 210ms/step - loss: 0.0163 - mae: 0.0928 - rmsle_cust: 0.0097 - val_loss: 0.0159 - val_mae: 0.0961 - val_rmsle_cust: 0.0059\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 214ms/step - loss: 0.0180 - mae: 0.0988 - rmsle_cust: 0.0096 - val_loss: 0.0147 - val_mae: 0.0915 - val_rmsle_cust: 0.0059\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 215ms/step - loss: 0.0155 - mae: 0.0925 - rmsle_cust: 0.0097 - val_loss: 0.0134 - val_mae: 0.0867 - val_rmsle_cust: 0.0062\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 227ms/step - loss: 0.0128 - mae: 0.0850 - rmsle_cust: 0.0102 - val_loss: 0.0140 - val_mae: 0.0912 - val_rmsle_cust: 0.0062\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 210ms/step - loss: 0.0120 - mae: 0.0838 - rmsle_cust: 0.0095 - val_loss: 0.0139 - val_mae: 0.0911 - val_rmsle_cust: 0.0055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 238ms/step - loss: 0.0111 - mae: 0.0794 - rmsle_cust: 0.0081 - val_loss: 0.0122 - val_mae: 0.0831 - val_rmsle_cust: 0.0045\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 229ms/step - loss: 0.0093 - mae: 0.0702 - rmsle_cust: 0.0067 - val_loss: 0.0108 - val_mae: 0.0750 - val_rmsle_cust: 0.0043\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 216ms/step - loss: 0.0085 - mae: 0.0656 - rmsle_cust: 0.0062 - val_loss: 0.0105 - val_mae: 0.0739 - val_rmsle_cust: 0.0042\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 221ms/step - loss: 0.0085 - mae: 0.0670 - rmsle_cust: 0.0060 - val_loss: 0.0104 - val_mae: 0.0743 - val_rmsle_cust: 0.0042\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 208ms/step - loss: 0.0087 - mae: 0.0683 - rmsle_cust: 0.0059 - val_loss: 0.0101 - val_mae: 0.0722 - val_rmsle_cust: 0.0042\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 228ms/step - loss: 0.0079 - mae: 0.0646 - rmsle_cust: 0.0058 - val_loss: 0.0099 - val_mae: 0.0714 - val_rmsle_cust: 0.0042\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 1s 215ms/step - loss: 0.0071 - mae: 0.0603 - rmsle_cust: 0.0054 - val_loss: 0.0102 - val_mae: 0.0735 - val_rmsle_cust: 0.0044\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 212ms/step - loss: 0.0073 - mae: 0.0607 - rmsle_cust: 0.0055 - val_loss: 0.0103 - val_mae: 0.0741 - val_rmsle_cust: 0.0046\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 222ms/step - loss: 0.0069 - mae: 0.0589 - rmsle_cust: 0.0056 - val_loss: 0.0098 - val_mae: 0.0716 - val_rmsle_cust: 0.0047\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 210ms/step - loss: 0.0063 - mae: 0.0565 - rmsle_cust: 0.0055 - val_loss: 0.0092 - val_mae: 0.0693 - val_rmsle_cust: 0.0047\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 232ms/step - loss: 0.0061 - mae: 0.0556 - rmsle_cust: 0.0056 - val_loss: 0.0091 - val_mae: 0.0694 - val_rmsle_cust: 0.0046\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 211ms/step - loss: 0.0060 - mae: 0.0559 - rmsle_cust: 0.0053 - val_loss: 0.0090 - val_mae: 0.0689 - val_rmsle_cust: 0.0046\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 1s 240ms/step - loss: 0.0057 - mae: 0.0536 - rmsle_cust: 0.0051 - val_loss: 0.0089 - val_mae: 0.0674 - val_rmsle_cust: 0.0047\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 1s 222ms/step - loss: 0.0054 - mae: 0.0519 - rmsle_cust: 0.0050 - val_loss: 0.0090 - val_mae: 0.0676 - val_rmsle_cust: 0.0047\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 244ms/step - loss: 0.0051 - mae: 0.0506 - rmsle_cust: 0.0050 - val_loss: 0.0090 - val_mae: 0.0677 - val_rmsle_cust: 0.0048\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 1s 224ms/step - loss: 0.0049 - mae: 0.0492 - rmsle_cust: 0.0048 - val_loss: 0.0088 - val_mae: 0.0665 - val_rmsle_cust: 0.0048\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 1s 282ms/step - loss: 0.0049 - mae: 0.0491 - rmsle_cust: 0.0045 - val_loss: 0.0085 - val_mae: 0.0652 - val_rmsle_cust: 0.0048\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 1s 258ms/step - loss: 0.0048 - mae: 0.0485 - rmsle_cust: 0.0045 - val_loss: 0.0084 - val_mae: 0.0646 - val_rmsle_cust: 0.0048\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 1s 247ms/step - loss: 0.0048 - mae: 0.0486 - rmsle_cust: 0.0046 - val_loss: 0.0084 - val_mae: 0.0646 - val_rmsle_cust: 0.0048\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 1s 270ms/step - loss: 0.0047 - mae: 0.0481 - rmsle_cust: 0.0043 - val_loss: 0.0085 - val_mae: 0.0650 - val_rmsle_cust: 0.0049\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 1s 213ms/step - loss: 0.0046 - mae: 0.0474 - rmsle_cust: 0.0043 - val_loss: 0.0085 - val_mae: 0.0650 - val_rmsle_cust: 0.0048\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 1s 236ms/step - loss: 0.0045 - mae: 0.0470 - rmsle_cust: 0.0043 - val_loss: 0.0085 - val_mae: 0.0647 - val_rmsle_cust: 0.0048\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 1s 218ms/step - loss: 0.0043 - mae: 0.0457 - rmsle_cust: 0.0042 - val_loss: 0.0084 - val_mae: 0.0646 - val_rmsle_cust: 0.0047\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 1s 221ms/step - loss: 0.0042 - mae: 0.0457 - rmsle_cust: 0.0041 - val_loss: 0.0084 - val_mae: 0.0644 - val_rmsle_cust: 0.0046\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 1s 226ms/step - loss: 0.0041 - mae: 0.0449 - rmsle_cust: 0.0041 - val_loss: 0.0084 - val_mae: 0.0642 - val_rmsle_cust: 0.0045\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 236ms/step - loss: 0.0039 - mae: 0.0438 - rmsle_cust: 0.0039 - val_loss: 0.0084 - val_mae: 0.0642 - val_rmsle_cust: 0.0045\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 1s 220ms/step - loss: 0.0039 - mae: 0.0437 - rmsle_cust: 0.0038 - val_loss: 0.0084 - val_mae: 0.0640 - val_rmsle_cust: 0.0044\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 1s 240ms/step - loss: 0.0037 - mae: 0.0428 - rmsle_cust: 0.0038 - val_loss: 0.0083 - val_mae: 0.0637 - val_rmsle_cust: 0.0044\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 1s 210ms/step - loss: 0.0038 - mae: 0.0434 - rmsle_cust: 0.0037 - val_loss: 0.0083 - val_mae: 0.0635 - val_rmsle_cust: 0.0043\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 214ms/step - loss: 0.0037 - mae: 0.0432 - rmsle_cust: 0.0036 - val_loss: 0.0082 - val_mae: 0.0633 - val_rmsle_cust: 0.0043\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 215ms/step - loss: 0.0036 - mae: 0.0418 - rmsle_cust: 0.0035 - val_loss: 0.0082 - val_mae: 0.0633 - val_rmsle_cust: 0.0043\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 1s 243ms/step - loss: 0.0036 - mae: 0.0418 - rmsle_cust: 0.0035 - val_loss: 0.0083 - val_mae: 0.0636 - val_rmsle_cust: 0.0043\n",
      "11/11 [==============================] - 1s 9ms/step\n",
      "Train: [   0    1    3 ... 3443 3444 3445] Validation: [   2    9   22   26   27   29   33   46   74   78   88  131  135  142\n",
      "  147  156  159  163  175  185  211  215  234  239  251  274  278  279\n",
      "  286  295  298  308  310  311  315  323  352  371  384  388  395  404\n",
      "  408  409  426  430  474  488  491  496  500  502  507  528  531  538\n",
      "  567  570  593  594  597  604  618  622  629  641  647  649  659  663\n",
      "  682  722  740  742  754  759  786  789  810  815  819  824  827  831\n",
      "  833  847  860  861  862  863  868  888  890  914  922  938  950  951\n",
      "  952  966  969  974  991 1005 1014 1048 1057 1079 1085 1086 1090 1116\n",
      " 1124 1130 1134 1148 1166 1181 1203 1209 1211 1222 1229 1240 1243 1274\n",
      " 1279 1290 1304 1314 1318 1320 1331 1343 1358 1367 1370 1374 1378 1405\n",
      " 1416 1418 1428 1429 1461 1476 1505 1511 1512 1541 1553 1574 1576 1583\n",
      " 1585 1600 1618 1634 1655 1656 1662 1671 1682 1699 1705 1709 1711 1731\n",
      " 1733 1743 1753 1755 1764 1770 1779 1788 1811 1831 1841 1842 1843 1853\n",
      " 1861 1877 1886 1923 1932 1935 1940 1955 1972 1974 1978 1984 2004 2020\n",
      " 2023 2036 2047 2060 2063 2067 2075 2082 2083 2089 2100 2109 2110 2131\n",
      " 2138 2140 2145 2147 2151 2157 2167 2184 2196 2202 2237 2245 2263 2268\n",
      " 2281 2286 2304 2310 2317 2340 2358 2363 2364 2367 2398 2403 2415 2430\n",
      " 2437 2438 2449 2455 2461 2466 2480 2495 2517 2520 2529 2530 2543 2546\n",
      " 2563 2564 2573 2582 2583 2584 2585 2602 2608 2616 2619 2625 2642 2650\n",
      " 2662 2677 2685 2691 2692 2703 2709 2723 2728 2762 2766 2769 2784 2793\n",
      " 2826 2841 2855 2868 2887 2889 2896 2898 2903 2907 2926 2934 2941 2943\n",
      " 2967 2969 2974 2979 2992 2999 3010 3019 3065 3068 3073 3079 3090 3110\n",
      " 3121 3131 3142 3150 3152 3161 3174 3178 3206 3225 3227 3229 3231 3234\n",
      " 3236 3244 3245 3248 3263 3267 3268 3279 3302 3326 3327 3339 3340 3347\n",
      " 3349 3356 3372 3385 3401 3411 3428 3435]\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 6s 988ms/step - loss: 0.1821 - mae: 0.3551 - rmsle_cust: 0.0159 - val_loss: 0.1321 - val_mae: 0.2928 - val_rmsle_cust: 0.0095\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 2s 473ms/step - loss: 0.1122 - mae: 0.2646 - rmsle_cust: 0.0099 - val_loss: 0.0799 - val_mae: 0.2263 - val_rmsle_cust: 0.0095\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 1s 312ms/step - loss: 0.0692 - mae: 0.2077 - rmsle_cust: 0.0099 - val_loss: 0.0616 - val_mae: 0.2073 - val_rmsle_cust: 0.0095\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 297ms/step - loss: 0.0607 - mae: 0.2051 - rmsle_cust: 0.0099 - val_loss: 0.0628 - val_mae: 0.2085 - val_rmsle_cust: 0.0095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 304ms/step - loss: 0.0639 - mae: 0.2116 - rmsle_cust: 0.0099 - val_loss: 0.0515 - val_mae: 0.1860 - val_rmsle_cust: 0.0095\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 341ms/step - loss: 0.0509 - mae: 0.1864 - rmsle_cust: 0.0099 - val_loss: 0.0386 - val_mae: 0.1591 - val_rmsle_cust: 0.0095\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 2s 457ms/step - loss: 0.0361 - mae: 0.1524 - rmsle_cust: 0.0099 - val_loss: 0.0342 - val_mae: 0.1467 - val_rmsle_cust: 0.0096\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 340ms/step - loss: 0.0302 - mae: 0.1356 - rmsle_cust: 0.0098 - val_loss: 0.0336 - val_mae: 0.1478 - val_rmsle_cust: 0.0099\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 0.0283 - mae: 0.1315 - rmsle_cust: 0.0097 - val_loss: 0.0294 - val_mae: 0.1412 - val_rmsle_cust: 0.0102\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 320ms/step - loss: 0.0243 - mae: 0.1231 - rmsle_cust: 0.0099 - val_loss: 0.0218 - val_mae: 0.1188 - val_rmsle_cust: 0.0108\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 362ms/step - loss: 0.0187 - mae: 0.1061 - rmsle_cust: 0.0103 - val_loss: 0.0168 - val_mae: 0.0974 - val_rmsle_cust: 0.0108\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 321ms/step - loss: 0.0158 - mae: 0.0935 - rmsle_cust: 0.0103 - val_loss: 0.0172 - val_mae: 0.0961 - val_rmsle_cust: 0.0106\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 354ms/step - loss: 0.0176 - mae: 0.0974 - rmsle_cust: 0.0101 - val_loss: 0.0175 - val_mae: 0.0988 - val_rmsle_cust: 0.0104\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 292ms/step - loss: 0.0173 - mae: 0.0972 - rmsle_cust: 0.0098 - val_loss: 0.0148 - val_mae: 0.0909 - val_rmsle_cust: 0.0102\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 277ms/step - loss: 0.0139 - mae: 0.0868 - rmsle_cust: 0.0096 - val_loss: 0.0133 - val_mae: 0.0867 - val_rmsle_cust: 0.0099\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 279ms/step - loss: 0.0116 - mae: 0.0804 - rmsle_cust: 0.0091 - val_loss: 0.0135 - val_mae: 0.0910 - val_rmsle_cust: 0.0091\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 268ms/step - loss: 0.0111 - mae: 0.0794 - rmsle_cust: 0.0080 - val_loss: 0.0131 - val_mae: 0.0888 - val_rmsle_cust: 0.0081\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 286ms/step - loss: 0.0100 - mae: 0.0735 - rmsle_cust: 0.0067 - val_loss: 0.0116 - val_mae: 0.0812 - val_rmsle_cust: 0.0074\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0089 - mae: 0.0679 - rmsle_cust: 0.0060 - val_loss: 0.0107 - val_mae: 0.0758 - val_rmsle_cust: 0.0070\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 288ms/step - loss: 0.0084 - mae: 0.0663 - rmsle_cust: 0.0058 - val_loss: 0.0105 - val_mae: 0.0751 - val_rmsle_cust: 0.0069\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 281ms/step - loss: 0.0084 - mae: 0.0665 - rmsle_cust: 0.0059 - val_loss: 0.0102 - val_mae: 0.0743 - val_rmsle_cust: 0.0067\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 284ms/step - loss: 0.0078 - mae: 0.0645 - rmsle_cust: 0.0057 - val_loss: 0.0100 - val_mae: 0.0731 - val_rmsle_cust: 0.0065\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 290ms/step - loss: 0.0072 - mae: 0.0609 - rmsle_cust: 0.0056 - val_loss: 0.0102 - val_mae: 0.0742 - val_rmsle_cust: 0.0065\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 276ms/step - loss: 0.0067 - mae: 0.0584 - rmsle_cust: 0.0056 - val_loss: 0.0104 - val_mae: 0.0754 - val_rmsle_cust: 0.0069\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 1s 291ms/step - loss: 0.0064 - mae: 0.0578 - rmsle_cust: 0.0059 - val_loss: 0.0101 - val_mae: 0.0732 - val_rmsle_cust: 0.0071\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 279ms/step - loss: 0.0062 - mae: 0.0566 - rmsle_cust: 0.0060 - val_loss: 0.0095 - val_mae: 0.0703 - val_rmsle_cust: 0.0071\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 307ms/step - loss: 0.0059 - mae: 0.0543 - rmsle_cust: 0.0058 - val_loss: 0.0093 - val_mae: 0.0697 - val_rmsle_cust: 0.0070\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 344ms/step - loss: 0.0057 - mae: 0.0537 - rmsle_cust: 0.0056 - val_loss: 0.0091 - val_mae: 0.0689 - val_rmsle_cust: 0.0069\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 2s 597ms/step - loss: 0.0054 - mae: 0.0524 - rmsle_cust: 0.0053 - val_loss: 0.0091 - val_mae: 0.0680 - val_rmsle_cust: 0.0068\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 2s 360ms/step - loss: 0.0052 - mae: 0.0509 - rmsle_cust: 0.0052 - val_loss: 0.0092 - val_mae: 0.0685 - val_rmsle_cust: 0.0068\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 2s 562ms/step - loss: 0.0051 - mae: 0.0507 - rmsle_cust: 0.0048 - val_loss: 0.0094 - val_mae: 0.0693 - val_rmsle_cust: 0.0067\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 2s 507ms/step - loss: 0.0050 - mae: 0.0498 - rmsle_cust: 0.0048 - val_loss: 0.0092 - val_mae: 0.0685 - val_rmsle_cust: 0.0066\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 2s 602ms/step - loss: 0.0048 - mae: 0.0486 - rmsle_cust: 0.0046 - val_loss: 0.0090 - val_mae: 0.0669 - val_rmsle_cust: 0.0065\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 2s 483ms/step - loss: 0.0047 - mae: 0.0482 - rmsle_cust: 0.0044 - val_loss: 0.0088 - val_mae: 0.0660 - val_rmsle_cust: 0.0064\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.0044 - mae: 0.0469 - rmsle_cust: 0.0044 - val_loss: 0.0087 - val_mae: 0.0655 - val_rmsle_cust: 0.0064\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 2s 620ms/step - loss: 0.0044 - mae: 0.0469 - rmsle_cust: 0.0042 - val_loss: 0.0088 - val_mae: 0.0654 - val_rmsle_cust: 0.0063\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 2s 476ms/step - loss: 0.0043 - mae: 0.0462 - rmsle_cust: 0.0044 - val_loss: 0.0089 - val_mae: 0.0661 - val_rmsle_cust: 0.0064\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 2s 611ms/step - loss: 0.0042 - mae: 0.0456 - rmsle_cust: 0.0042 - val_loss: 0.0089 - val_mae: 0.0663 - val_rmsle_cust: 0.0064\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 2s 596ms/step - loss: 0.0041 - mae: 0.0453 - rmsle_cust: 0.0043 - val_loss: 0.0088 - val_mae: 0.0653 - val_rmsle_cust: 0.0063\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 2s 450ms/step - loss: 0.0040 - mae: 0.0440 - rmsle_cust: 0.0041 - val_loss: 0.0086 - val_mae: 0.0644 - val_rmsle_cust: 0.0062\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 2s 391ms/step - loss: 0.0038 - mae: 0.0433 - rmsle_cust: 0.0042 - val_loss: 0.0086 - val_mae: 0.0641 - val_rmsle_cust: 0.0061\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 2s 403ms/step - loss: 0.0038 - mae: 0.0430 - rmsle_cust: 0.0039 - val_loss: 0.0086 - val_mae: 0.0642 - val_rmsle_cust: 0.0062\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 2s 590ms/step - loss: 0.0036 - mae: 0.0421 - rmsle_cust: 0.0039 - val_loss: 0.0087 - val_mae: 0.0645 - val_rmsle_cust: 0.0062\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 2s 502ms/step - loss: 0.0037 - mae: 0.0431 - rmsle_cust: 0.0039 - val_loss: 0.0087 - val_mae: 0.0642 - val_rmsle_cust: 0.0062\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 2s 605ms/step - loss: 0.0036 - mae: 0.0427 - rmsle_cust: 0.0038 - val_loss: 0.0085 - val_mae: 0.0635 - val_rmsle_cust: 0.0062\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 2s 496ms/step - loss: 0.0036 - mae: 0.0424 - rmsle_cust: 0.0036 - val_loss: 0.0085 - val_mae: 0.0632 - val_rmsle_cust: 0.0061\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 1s 572ms/step - loss: 0.0033 - mae: 0.0408 - rmsle_cust: 0.0035 - val_loss: 0.0085 - val_mae: 0.0632 - val_rmsle_cust: 0.0062\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 2s 605ms/step - loss: 0.0033 - mae: 0.0404 - rmsle_cust: 0.0035 - val_loss: 0.0085 - val_mae: 0.0633 - val_rmsle_cust: 0.0062\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 365ms/step - loss: 0.0033 - mae: 0.0407 - rmsle_cust: 0.0037 - val_loss: 0.0086 - val_mae: 0.0635 - val_rmsle_cust: 0.0062\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 2s 366ms/step - loss: 0.0032 - mae: 0.0399 - rmsle_cust: 0.0036 - val_loss: 0.0086 - val_mae: 0.0633 - val_rmsle_cust: 0.0061\n",
      "11/11 [==============================] - 1s 15ms/step\n",
      "Train: [   0    1    2 ... 3443 3444 3445] Validation: [   5   31   43   55   57   61   66   69   84   85   97  106  117  121\n",
      "  127  141  164  170  178  193  237  238  244  265  276  277  284  300\n",
      "  307  320  322  326  328  339  354  355  358  360  374  381  387  392\n",
      "  433  437  449  455  458  463  466  472  479  482  483  504  511  530\n",
      "  548  550  554  568  603  646  673  686  694  699  723  731  741  744\n",
      "  746  752  755  765  776  780  782  798  826  830  841  842  867  875\n",
      "  892  912  923  930  942  962  978 1003 1007 1023 1024 1029 1055 1056\n",
      " 1092 1101 1119 1127 1158 1180 1184 1190 1198 1199 1242 1250 1256 1257\n",
      " 1259 1267 1280 1298 1302 1315 1319 1340 1341 1346 1356 1361 1371 1390\n",
      " 1437 1440 1444 1451 1458 1464 1479 1510 1516 1531 1543 1557 1567 1568\n",
      " 1591 1602 1633 1643 1665 1676 1704 1714 1718 1739 1746 1750 1769 1775\n",
      " 1786 1793 1797 1800 1809 1821 1839 1867 1868 1889 1901 1904 1910 1912\n",
      " 1948 1952 1954 1963 1969 1971 1975 1976 1980 1985 1989 1997 1998 2002\n",
      " 2003 2007 2014 2021 2041 2042 2044 2050 2057 2058 2061 2071 2076 2080\n",
      " 2081 2086 2090 2094 2116 2120 2122 2134 2137 2143 2153 2154 2187 2192\n",
      " 2199 2200 2203 2212 2220 2228 2232 2233 2235 2239 2255 2262 2266 2272\n",
      " 2280 2305 2319 2324 2339 2342 2355 2366 2370 2393 2397 2412 2428 2436\n",
      " 2439 2441 2460 2478 2485 2493 2501 2505 2507 2540 2541 2570 2591 2593\n",
      " 2594 2609 2611 2615 2620 2622 2626 2630 2670 2697 2700 2712 2713 2719\n",
      " 2725 2734 2739 2751 2765 2768 2773 2775 2777 2783 2792 2801 2803 2818\n",
      " 2847 2856 2858 2872 2878 2881 2882 2899 2904 2906 2912 2920 2947 2948\n",
      " 2975 2978 2989 2996 3009 3030 3040 3067 3071 3072 3085 3095 3096 3100\n",
      " 3128 3133 3141 3143 3156 3181 3200 3208 3222 3230 3232 3239 3253 3254\n",
      " 3258 3274 3282 3288 3300 3303 3307 3331 3334 3352 3361 3376 3379 3383\n",
      " 3387 3397 3406 3413 3419 3422 3427 3441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.1898 - mae: 0.3618 - rmsle_cust: 0.0170 - val_loss: 0.1233 - val_mae: 0.2775 - val_rmsle_cust: 0.0099\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 1s 300ms/step - loss: 0.1276 - mae: 0.2824 - rmsle_cust: 0.0099 - val_loss: 0.0797 - val_mae: 0.2204 - val_rmsle_cust: 0.0099\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 1s 458ms/step - loss: 0.0833 - mae: 0.2230 - rmsle_cust: 0.0099 - val_loss: 0.0594 - val_mae: 0.2004 - val_rmsle_cust: 0.0099\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 279ms/step - loss: 0.0628 - mae: 0.2055 - rmsle_cust: 0.0099 - val_loss: 0.0639 - val_mae: 0.2165 - val_rmsle_cust: 0.0099\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 236ms/step - loss: 0.0646 - mae: 0.2166 - rmsle_cust: 0.0099 - val_loss: 0.0652 - val_mae: 0.2172 - val_rmsle_cust: 0.0099\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.0632 - mae: 0.2120 - rmsle_cust: 0.0099 - val_loss: 0.0523 - val_mae: 0.1925 - val_rmsle_cust: 0.0099\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 2s 570ms/step - loss: 0.0497 - mae: 0.1851 - rmsle_cust: 0.0099 - val_loss: 0.0399 - val_mae: 0.1658 - val_rmsle_cust: 0.0099\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 2s 697ms/step - loss: 0.0393 - mae: 0.1620 - rmsle_cust: 0.0099 - val_loss: 0.0342 - val_mae: 0.1473 - val_rmsle_cust: 0.0099\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 2s 657ms/step - loss: 0.0346 - mae: 0.1465 - rmsle_cust: 0.0099 - val_loss: 0.0316 - val_mae: 0.1387 - val_rmsle_cust: 0.0099\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 2s 650ms/step - loss: 0.0317 - mae: 0.1385 - rmsle_cust: 0.0096 - val_loss: 0.0272 - val_mae: 0.1283 - val_rmsle_cust: 0.0098\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 2s 658ms/step - loss: 0.0266 - mae: 0.1272 - rmsle_cust: 0.0095 - val_loss: 0.0214 - val_mae: 0.1120 - val_rmsle_cust: 0.0096\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 2s 680ms/step - loss: 0.0199 - mae: 0.1081 - rmsle_cust: 0.0094 - val_loss: 0.0178 - val_mae: 0.0996 - val_rmsle_cust: 0.0095\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 2s 654ms/step - loss: 0.0152 - mae: 0.0911 - rmsle_cust: 0.0095 - val_loss: 0.0187 - val_mae: 0.0990 - val_rmsle_cust: 0.0093\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 2s 676ms/step - loss: 0.0155 - mae: 0.0907 - rmsle_cust: 0.0094 - val_loss: 0.0204 - val_mae: 0.1052 - val_rmsle_cust: 0.0093\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 2s 611ms/step - loss: 0.0167 - mae: 0.0956 - rmsle_cust: 0.0096 - val_loss: 0.0182 - val_mae: 0.0983 - val_rmsle_cust: 0.0094\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 2s 626ms/step - loss: 0.0148 - mae: 0.0896 - rmsle_cust: 0.0098 - val_loss: 0.0152 - val_mae: 0.0890 - val_rmsle_cust: 0.0096\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 2s 664ms/step - loss: 0.0126 - mae: 0.0838 - rmsle_cust: 0.0102 - val_loss: 0.0140 - val_mae: 0.0875 - val_rmsle_cust: 0.0094\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 2s 601ms/step - loss: 0.0116 - mae: 0.0813 - rmsle_cust: 0.0099 - val_loss: 0.0131 - val_mae: 0.0853 - val_rmsle_cust: 0.0084\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 2s 644ms/step - loss: 0.0102 - mae: 0.0765 - rmsle_cust: 0.0084 - val_loss: 0.0120 - val_mae: 0.0798 - val_rmsle_cust: 0.0072\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 2s 603ms/step - loss: 0.0088 - mae: 0.0686 - rmsle_cust: 0.0069 - val_loss: 0.0115 - val_mae: 0.0761 - val_rmsle_cust: 0.0070\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 2s 619ms/step - loss: 0.0079 - mae: 0.0637 - rmsle_cust: 0.0059 - val_loss: 0.0120 - val_mae: 0.0787 - val_rmsle_cust: 0.0074\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 2s 605ms/step - loss: 0.0081 - mae: 0.0651 - rmsle_cust: 0.0056 - val_loss: 0.0122 - val_mae: 0.0802 - val_rmsle_cust: 0.0076\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 2s 670ms/step - loss: 0.0081 - mae: 0.0659 - rmsle_cust: 0.0056 - val_loss: 0.0116 - val_mae: 0.0780 - val_rmsle_cust: 0.0076\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 2s 600ms/step - loss: 0.0074 - mae: 0.0621 - rmsle_cust: 0.0055 - val_loss: 0.0108 - val_mae: 0.0749 - val_rmsle_cust: 0.0074\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 2s 656ms/step - loss: 0.0067 - mae: 0.0587 - rmsle_cust: 0.0053 - val_loss: 0.0105 - val_mae: 0.0738 - val_rmsle_cust: 0.0074\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 2s 672ms/step - loss: 0.0067 - mae: 0.0583 - rmsle_cust: 0.0053 - val_loss: 0.0102 - val_mae: 0.0719 - val_rmsle_cust: 0.0074\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 2s 607ms/step - loss: 0.0062 - mae: 0.0567 - rmsle_cust: 0.0055 - val_loss: 0.0098 - val_mae: 0.0688 - val_rmsle_cust: 0.0074\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 2s 641ms/step - loss: 0.0059 - mae: 0.0552 - rmsle_cust: 0.0053 - val_loss: 0.0098 - val_mae: 0.0675 - val_rmsle_cust: 0.0074\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 2s 638ms/step - loss: 0.0058 - mae: 0.0544 - rmsle_cust: 0.0055 - val_loss: 0.0098 - val_mae: 0.0681 - val_rmsle_cust: 0.0073\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 2s 611ms/step - loss: 0.0057 - mae: 0.0542 - rmsle_cust: 0.0056 - val_loss: 0.0096 - val_mae: 0.0673 - val_rmsle_cust: 0.0072\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 2s 682ms/step - loss: 0.0053 - mae: 0.0521 - rmsle_cust: 0.0052 - val_loss: 0.0094 - val_mae: 0.0666 - val_rmsle_cust: 0.0072\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 2s 662ms/step - loss: 0.0053 - mae: 0.0512 - rmsle_cust: 0.0051 - val_loss: 0.0094 - val_mae: 0.0674 - val_rmsle_cust: 0.0071\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 2s 607ms/step - loss: 0.0049 - mae: 0.0504 - rmsle_cust: 0.0050 - val_loss: 0.0094 - val_mae: 0.0681 - val_rmsle_cust: 0.0071\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 2s 671ms/step - loss: 0.0047 - mae: 0.0484 - rmsle_cust: 0.0045 - val_loss: 0.0094 - val_mae: 0.0680 - val_rmsle_cust: 0.0071\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 2s 662ms/step - loss: 0.0047 - mae: 0.0486 - rmsle_cust: 0.0044 - val_loss: 0.0095 - val_mae: 0.0680 - val_rmsle_cust: 0.0071\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 2s 538ms/step - loss: 0.0046 - mae: 0.0473 - rmsle_cust: 0.0043 - val_loss: 0.0095 - val_mae: 0.0678 - val_rmsle_cust: 0.0070\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.0044 - mae: 0.0472 - rmsle_cust: 0.0042 - val_loss: 0.0093 - val_mae: 0.0671 - val_rmsle_cust: 0.0069\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 1s 340ms/step - loss: 0.0044 - mae: 0.0469 - rmsle_cust: 0.0042 - val_loss: 0.0091 - val_mae: 0.0664 - val_rmsle_cust: 0.0068\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 1s 393ms/step - loss: 0.0041 - mae: 0.0456 - rmsle_cust: 0.0041 - val_loss: 0.0090 - val_mae: 0.0658 - val_rmsle_cust: 0.0067\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.0041 - mae: 0.0451 - rmsle_cust: 0.0041 - val_loss: 0.0089 - val_mae: 0.0652 - val_rmsle_cust: 0.0067\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.0041 - mae: 0.0451 - rmsle_cust: 0.0041 - val_loss: 0.0089 - val_mae: 0.0651 - val_rmsle_cust: 0.0067\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 1s 392ms/step - loss: 0.0038 - mae: 0.0433 - rmsle_cust: 0.0039 - val_loss: 0.0089 - val_mae: 0.0650 - val_rmsle_cust: 0.0067\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 1s 440ms/step - loss: 0.0037 - mae: 0.0430 - rmsle_cust: 0.0037 - val_loss: 0.0088 - val_mae: 0.0649 - val_rmsle_cust: 0.0067\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 375ms/step - loss: 0.0037 - mae: 0.0429 - rmsle_cust: 0.0037 - val_loss: 0.0088 - val_mae: 0.0648 - val_rmsle_cust: 0.0066\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 1s 512ms/step - loss: 0.0035 - mae: 0.0418 - rmsle_cust: 0.0036 - val_loss: 0.0088 - val_mae: 0.0648 - val_rmsle_cust: 0.0066\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 1s 332ms/step - loss: 0.0034 - mae: 0.0414 - rmsle_cust: 0.0035 - val_loss: 0.0088 - val_mae: 0.0650 - val_rmsle_cust: 0.0066\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 484ms/step - loss: 0.0033 - mae: 0.0406 - rmsle_cust: 0.0033 - val_loss: 0.0088 - val_mae: 0.0650 - val_rmsle_cust: 0.0066\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 494ms/step - loss: 0.0033 - mae: 0.0412 - rmsle_cust: 0.0035 - val_loss: 0.0087 - val_mae: 0.0646 - val_rmsle_cust: 0.0066\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 375ms/step - loss: 0.0033 - mae: 0.0407 - rmsle_cust: 0.0035 - val_loss: 0.0086 - val_mae: 0.0641 - val_rmsle_cust: 0.0066\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 1s 469ms/step - loss: 0.0032 - mae: 0.0400 - rmsle_cust: 0.0034 - val_loss: 0.0086 - val_mae: 0.0636 - val_rmsle_cust: 0.0065\n",
      "11/11 [==============================] - 1s 22ms/step\n",
      "Train: [   0    1    2 ... 3443 3444 3445] Validation: [  14   18   36   42   52   70   71   92   95  105  112  113  114  116\n",
      "  118  122  153  162  168  174  179  184  197  213  214  225  232  240\n",
      "  245  246  247  249  263  271  272  296  312  317  333  334  337  342\n",
      "  343  344  356  362  373  377  378  379  402  410  411  412  417  454\n",
      "  467  506  519  522  539  552  565  580  601  611  621  651  652  657\n",
      "  658  665  667  676  677  684  691  704  716  721  732  734  738  762\n",
      "  770  775  794  795  803  814  818  822  835  838  844  848  850  856\n",
      "  873  886  897  903  907  908  915  939  944  947  956  963  967  971\n",
      "  980  989  998  999 1018 1022 1025 1033 1042 1062 1063 1076 1088 1106\n",
      " 1107 1113 1121 1133 1154 1168 1170 1174 1182 1192 1200 1207 1251 1253\n",
      " 1254 1261 1284 1309 1313 1332 1368 1376 1383 1389 1420 1423 1424 1434\n",
      " 1439 1445 1449 1460 1465 1469 1472 1473 1474 1491 1493 1499 1522 1528\n",
      " 1530 1547 1582 1594 1598 1615 1616 1622 1632 1638 1639 1666 1667 1693\n",
      " 1700 1706 1712 1715 1722 1740 1745 1757 1763 1771 1803 1806 1820 1835\n",
      " 1838 1840 1849 1871 1878 1883 1887 1898 1914 1936 1942 1981 1983 2012\n",
      " 2052 2055 2077 2091 2141 2144 2148 2172 2176 2180 2195 2205 2210 2216\n",
      " 2219 2221 2251 2260 2261 2265 2269 2278 2302 2309 2321 2335 2376 2382\n",
      " 2386 2394 2400 2405 2408 2413 2419 2442 2445 2450 2465 2477 2498 2502\n",
      " 2524 2536 2547 2548 2549 2559 2562 2565 2566 2581 2587 2588 2590 2592\n",
      " 2599 2636 2638 2648 2653 2658 2666 2693 2716 2791 2795 2804 2806 2829\n",
      " 2831 2835 2842 2843 2844 2853 2884 2892 2910 2914 2919 2924 2929 2930\n",
      " 2937 2951 2953 2961 2965 2973 2982 2985 3013 3018 3039 3046 3056 3069\n",
      " 3091 3137 3146 3151 3160 3172 3184 3188 3195 3217 3220 3224 3237 3243\n",
      " 3247 3251 3252 3278 3291 3293 3301 3311 3312 3322 3336 3342 3346 3354\n",
      " 3368 3374 3380 3399 3412 3414 3429 3438]\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 7s 1s/step - loss: 0.2059 - mae: 0.3781 - rmsle_cust: 0.0253 - val_loss: 0.1475 - val_mae: 0.3073 - val_rmsle_cust: 0.0104\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 2s 359ms/step - loss: 0.1418 - mae: 0.3019 - rmsle_cust: 0.0099 - val_loss: 0.0987 - val_mae: 0.2424 - val_rmsle_cust: 0.0104\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 2s 312ms/step - loss: 0.0942 - mae: 0.2401 - rmsle_cust: 0.0098 - val_loss: 0.0683 - val_mae: 0.2035 - val_rmsle_cust: 0.0104\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 2s 333ms/step - loss: 0.0666 - mae: 0.2074 - rmsle_cust: 0.0098 - val_loss: 0.0633 - val_mae: 0.2100 - val_rmsle_cust: 0.0104\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 2s 382ms/step - loss: 0.0627 - mae: 0.2108 - rmsle_cust: 0.0098 - val_loss: 0.0645 - val_mae: 0.2136 - val_rmsle_cust: 0.0104\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 2s 369ms/step - loss: 0.0634 - mae: 0.2126 - rmsle_cust: 0.0098 - val_loss: 0.0525 - val_mae: 0.1890 - val_rmsle_cust: 0.0104\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 2s 425ms/step - loss: 0.0502 - mae: 0.1863 - rmsle_cust: 0.0098 - val_loss: 0.0392 - val_mae: 0.1564 - val_rmsle_cust: 0.0104\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 2s 528ms/step - loss: 0.0364 - mae: 0.1524 - rmsle_cust: 0.0098 - val_loss: 0.0341 - val_mae: 0.1407 - val_rmsle_cust: 0.0104\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 2s 414ms/step - loss: 0.0309 - mae: 0.1364 - rmsle_cust: 0.0098 - val_loss: 0.0333 - val_mae: 0.1414 - val_rmsle_cust: 0.0105\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 346ms/step - loss: 0.0291 - mae: 0.1339 - rmsle_cust: 0.0097 - val_loss: 0.0295 - val_mae: 0.1347 - val_rmsle_cust: 0.0104\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 316ms/step - loss: 0.0251 - mae: 0.1251 - rmsle_cust: 0.0095 - val_loss: 0.0226 - val_mae: 0.1144 - val_rmsle_cust: 0.0099\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 317ms/step - loss: 0.0184 - mae: 0.1051 - rmsle_cust: 0.0092 - val_loss: 0.0178 - val_mae: 0.0950 - val_rmsle_cust: 0.0091\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 341ms/step - loss: 0.0154 - mae: 0.0921 - rmsle_cust: 0.0089 - val_loss: 0.0178 - val_mae: 0.0954 - val_rmsle_cust: 0.0084\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0167 - mae: 0.0965 - rmsle_cust: 0.0088 - val_loss: 0.0175 - val_mae: 0.0958 - val_rmsle_cust: 0.0079\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 321ms/step - loss: 0.0164 - mae: 0.0956 - rmsle_cust: 0.0090 - val_loss: 0.0151 - val_mae: 0.0861 - val_rmsle_cust: 0.0081\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.0132 - mae: 0.0854 - rmsle_cust: 0.0093 - val_loss: 0.0144 - val_mae: 0.0828 - val_rmsle_cust: 0.0080\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 2s 746ms/step - loss: 0.0117 - mae: 0.0816 - rmsle_cust: 0.0097 - val_loss: 0.0148 - val_mae: 0.0868 - val_rmsle_cust: 0.0075\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 2s 757ms/step - loss: 0.0110 - mae: 0.0799 - rmsle_cust: 0.0089 - val_loss: 0.0141 - val_mae: 0.0838 - val_rmsle_cust: 0.0065\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 2s 677ms/step - loss: 0.0099 - mae: 0.0746 - rmsle_cust: 0.0073 - val_loss: 0.0128 - val_mae: 0.0769 - val_rmsle_cust: 0.0056\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 2s 657ms/step - loss: 0.0088 - mae: 0.0688 - rmsle_cust: 0.0065 - val_loss: 0.0123 - val_mae: 0.0741 - val_rmsle_cust: 0.0056\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 2s 446ms/step - loss: 0.0084 - mae: 0.0658 - rmsle_cust: 0.0060 - val_loss: 0.0124 - val_mae: 0.0743 - val_rmsle_cust: 0.0057\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 2s 523ms/step - loss: 0.0082 - mae: 0.0660 - rmsle_cust: 0.0058 - val_loss: 0.0122 - val_mae: 0.0737 - val_rmsle_cust: 0.0055\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 2s 510ms/step - loss: 0.0078 - mae: 0.0643 - rmsle_cust: 0.0058 - val_loss: 0.0120 - val_mae: 0.0731 - val_rmsle_cust: 0.0051\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 2s 520ms/step - loss: 0.0070 - mae: 0.0606 - rmsle_cust: 0.0054 - val_loss: 0.0122 - val_mae: 0.0744 - val_rmsle_cust: 0.0048\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 2s 422ms/step - loss: 0.0069 - mae: 0.0595 - rmsle_cust: 0.0053 - val_loss: 0.0123 - val_mae: 0.0750 - val_rmsle_cust: 0.0047\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 458ms/step - loss: 0.0065 - mae: 0.0588 - rmsle_cust: 0.0054 - val_loss: 0.0119 - val_mae: 0.0730 - val_rmsle_cust: 0.0047\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 2s 613ms/step - loss: 0.0060 - mae: 0.0553 - rmsle_cust: 0.0054 - val_loss: 0.0115 - val_mae: 0.0705 - val_rmsle_cust: 0.0047\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 2s 346ms/step - loss: 0.0057 - mae: 0.0548 - rmsle_cust: 0.0055 - val_loss: 0.0113 - val_mae: 0.0703 - val_rmsle_cust: 0.0048\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 312ms/step - loss: 0.0057 - mae: 0.0549 - rmsle_cust: 0.0053 - val_loss: 0.0112 - val_mae: 0.0699 - val_rmsle_cust: 0.0048\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 364ms/step - loss: 0.0055 - mae: 0.0536 - rmsle_cust: 0.0054 - val_loss: 0.0112 - val_mae: 0.0695 - val_rmsle_cust: 0.0049\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 2s 435ms/step - loss: 0.0052 - mae: 0.0511 - rmsle_cust: 0.0050 - val_loss: 0.0113 - val_mae: 0.0709 - val_rmsle_cust: 0.0049\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 2s 517ms/step - loss: 0.0049 - mae: 0.0502 - rmsle_cust: 0.0049 - val_loss: 0.0113 - val_mae: 0.0712 - val_rmsle_cust: 0.0049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50\n",
      "2/2 [==============================] - 2s 404ms/step - loss: 0.0050 - mae: 0.0507 - rmsle_cust: 0.0046 - val_loss: 0.0111 - val_mae: 0.0702 - val_rmsle_cust: 0.0050\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 2s 453ms/step - loss: 0.0047 - mae: 0.0489 - rmsle_cust: 0.0043 - val_loss: 0.0108 - val_mae: 0.0688 - val_rmsle_cust: 0.0050\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 2s 373ms/step - loss: 0.0046 - mae: 0.0483 - rmsle_cust: 0.0044 - val_loss: 0.0107 - val_mae: 0.0681 - val_rmsle_cust: 0.0049\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 2s 437ms/step - loss: 0.0044 - mae: 0.0475 - rmsle_cust: 0.0044 - val_loss: 0.0107 - val_mae: 0.0679 - val_rmsle_cust: 0.0048\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 2s 612ms/step - loss: 0.0043 - mae: 0.0467 - rmsle_cust: 0.0040 - val_loss: 0.0107 - val_mae: 0.0685 - val_rmsle_cust: 0.0048\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 2s 418ms/step - loss: 0.0042 - mae: 0.0462 - rmsle_cust: 0.0040 - val_loss: 0.0108 - val_mae: 0.0691 - val_rmsle_cust: 0.0047\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 2s 497ms/step - loss: 0.0040 - mae: 0.0451 - rmsle_cust: 0.0041 - val_loss: 0.0108 - val_mae: 0.0684 - val_rmsle_cust: 0.0047\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 2s 644ms/step - loss: 0.0040 - mae: 0.0449 - rmsle_cust: 0.0040 - val_loss: 0.0107 - val_mae: 0.0671 - val_rmsle_cust: 0.0046\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 1s 445ms/step - loss: 0.0038 - mae: 0.0445 - rmsle_cust: 0.0039 - val_loss: 0.0106 - val_mae: 0.0667 - val_rmsle_cust: 0.0046\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 2s 685ms/step - loss: 0.0038 - mae: 0.0441 - rmsle_cust: 0.0038 - val_loss: 0.0105 - val_mae: 0.0666 - val_rmsle_cust: 0.0047\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 2s 378ms/step - loss: 0.0036 - mae: 0.0436 - rmsle_cust: 0.0037 - val_loss: 0.0106 - val_mae: 0.0668 - val_rmsle_cust: 0.0047\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.0035 - mae: 0.0424 - rmsle_cust: 0.0036 - val_loss: 0.0106 - val_mae: 0.0673 - val_rmsle_cust: 0.0047\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 2s 461ms/step - loss: 0.0037 - mae: 0.0436 - rmsle_cust: 0.0036 - val_loss: 0.0105 - val_mae: 0.0668 - val_rmsle_cust: 0.0048\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 1s 365ms/step - loss: 0.0034 - mae: 0.0416 - rmsle_cust: 0.0035 - val_loss: 0.0103 - val_mae: 0.0662 - val_rmsle_cust: 0.0048\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.0033 - mae: 0.0416 - rmsle_cust: 0.0034 - val_loss: 0.0103 - val_mae: 0.0660 - val_rmsle_cust: 0.0048\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 2s 547ms/step - loss: 0.0033 - mae: 0.0407 - rmsle_cust: 0.0034 - val_loss: 0.0104 - val_mae: 0.0664 - val_rmsle_cust: 0.0048\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 300ms/step - loss: 0.0032 - mae: 0.0405 - rmsle_cust: 0.0035 - val_loss: 0.0105 - val_mae: 0.0670 - val_rmsle_cust: 0.0049\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 1s 332ms/step - loss: 0.0033 - mae: 0.0410 - rmsle_cust: 0.0034 - val_loss: 0.0104 - val_mae: 0.0666 - val_rmsle_cust: 0.0049\n",
      "11/11 [==============================] - 1s 12ms/step\n",
      "Train: [   0    1    2 ... 3443 3444 3445] Validation: [  15   20   25   63   77   86   96  100  109  129  143  144  145  150\n",
      "  152  155  183  222  235  243  252  253  266  269  290  313  316  319\n",
      "  324  327  357  376  391  405  413  423  431  450  451  461  465  469\n",
      "  475  489  497  505  515  524  532  542  545  562  564  566  591  606\n",
      "  609  617  624  626  633  637  648  656  668  687  690  695  696  706\n",
      "  714  715  717  727  728  753  806  811  832  837  843  865  869  871\n",
      "  878  879  882  883  884  889  901  905  910  928  949  960  964 1019\n",
      " 1031 1046 1049 1052 1061 1067 1072 1075 1095 1096 1099 1110 1111 1137\n",
      " 1150 1155 1160 1175 1187 1191 1202 1214 1235 1258 1275 1277 1278 1288\n",
      " 1295 1300 1303 1306 1311 1312 1321 1324 1327 1337 1345 1350 1360 1366\n",
      " 1372 1393 1411 1425 1431 1455 1478 1484 1485 1486 1492 1495 1521 1523\n",
      " 1527 1538 1560 1571 1572 1580 1581 1590 1612 1619 1620 1627 1630 1635\n",
      " 1640 1679 1686 1689 1692 1694 1717 1738 1741 1752 1756 1762 1774 1795\n",
      " 1798 1799 1801 1824 1830 1834 1844 1845 1869 1874 1876 1879 1880 1888\n",
      " 1890 1920 1927 1939 1949 1968 1979 1982 1996 2025 2033 2069 2072 2092\n",
      " 2103 2125 2127 2171 2178 2186 2189 2197 2214 2218 2222 2224 2230 2244\n",
      " 2247 2254 2257 2258 2267 2277 2283 2288 2311 2326 2327 2365 2417 2432\n",
      " 2446 2464 2468 2486 2488 2492 2499 2504 2506 2516 2518 2523 2525 2533\n",
      " 2537 2556 2561 2574 2617 2628 2631 2634 2643 2663 2669 2674 2675 2678\n",
      " 2690 2705 2714 2729 2737 2754 2760 2763 2770 2780 2787 2788 2797 2799\n",
      " 2808 2832 2836 2840 2877 2880 2895 2897 2944 2949 2955 2958 2959 2962\n",
      " 2981 3002 3012 3015 3021 3022 3023 3029 3047 3049 3050 3082 3087 3098\n",
      " 3104 3109 3115 3125 3136 3144 3159 3168 3170 3209 3212 3228 3238 3265\n",
      " 3266 3270 3272 3277 3287 3314 3316 3321 3351 3353 3373 3377 3384 3389\n",
      " 3392 3396 3410 3415 3417 3418 3423 3425]\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 5s 947ms/step - loss: 0.2044 - mae: 0.3764 - rmsle_cust: 0.0242 - val_loss: 0.1359 - val_mae: 0.2934 - val_rmsle_cust: 0.0117\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 1s 259ms/step - loss: 0.1296 - mae: 0.2848 - rmsle_cust: 0.0097 - val_loss: 0.0867 - val_mae: 0.2322 - val_rmsle_cust: 0.0117\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 1s 319ms/step - loss: 0.0818 - mae: 0.2217 - rmsle_cust: 0.0097 - val_loss: 0.0680 - val_mae: 0.2163 - val_rmsle_cust: 0.0117\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 237ms/step - loss: 0.0642 - mae: 0.2096 - rmsle_cust: 0.0097 - val_loss: 0.0738 - val_mae: 0.2313 - val_rmsle_cust: 0.0117\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 294ms/step - loss: 0.0677 - mae: 0.2214 - rmsle_cust: 0.0097 - val_loss: 0.0689 - val_mae: 0.2212 - val_rmsle_cust: 0.0117\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 280ms/step - loss: 0.0606 - mae: 0.2071 - rmsle_cust: 0.0097 - val_loss: 0.0526 - val_mae: 0.1898 - val_rmsle_cust: 0.0117\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 1s 286ms/step - loss: 0.0440 - mae: 0.1719 - rmsle_cust: 0.0097 - val_loss: 0.0413 - val_mae: 0.1606 - val_rmsle_cust: 0.0117\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 289ms/step - loss: 0.0340 - mae: 0.1434 - rmsle_cust: 0.0097 - val_loss: 0.0378 - val_mae: 0.1520 - val_rmsle_cust: 0.0116\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 276ms/step - loss: 0.0303 - mae: 0.1345 - rmsle_cust: 0.0097 - val_loss: 0.0352 - val_mae: 0.1479 - val_rmsle_cust: 0.0116\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 270ms/step - loss: 0.0273 - mae: 0.1296 - rmsle_cust: 0.0100 - val_loss: 0.0294 - val_mae: 0.1347 - val_rmsle_cust: 0.0121\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 291ms/step - loss: 0.0214 - mae: 0.1146 - rmsle_cust: 0.0102 - val_loss: 0.0239 - val_mae: 0.1159 - val_rmsle_cust: 0.0122\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 261ms/step - loss: 0.0157 - mae: 0.0946 - rmsle_cust: 0.0102 - val_loss: 0.0231 - val_mae: 0.1121 - val_rmsle_cust: 0.0120\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 253ms/step - loss: 0.0156 - mae: 0.0918 - rmsle_cust: 0.0099 - val_loss: 0.0245 - val_mae: 0.1169 - val_rmsle_cust: 0.0118\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 0.0166 - mae: 0.0963 - rmsle_cust: 0.0093 - val_loss: 0.0223 - val_mae: 0.1117 - val_rmsle_cust: 0.0115\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 273ms/step - loss: 0.0146 - mae: 0.0896 - rmsle_cust: 0.0091 - val_loss: 0.0189 - val_mae: 0.1022 - val_rmsle_cust: 0.0110\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 258ms/step - loss: 0.0114 - mae: 0.0800 - rmsle_cust: 0.0090 - val_loss: 0.0178 - val_mae: 0.0994 - val_rmsle_cust: 0.0102\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 379ms/step - loss: 0.0106 - mae: 0.0782 - rmsle_cust: 0.0088 - val_loss: 0.0174 - val_mae: 0.0991 - val_rmsle_cust: 0.0093\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 277ms/step - loss: 0.0100 - mae: 0.0756 - rmsle_cust: 0.0078 - val_loss: 0.0163 - val_mae: 0.0949 - val_rmsle_cust: 0.0087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 271ms/step - loss: 0.0087 - mae: 0.0690 - rmsle_cust: 0.0065 - val_loss: 0.0155 - val_mae: 0.0908 - val_rmsle_cust: 0.0086\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 249ms/step - loss: 0.0079 - mae: 0.0654 - rmsle_cust: 0.0060 - val_loss: 0.0156 - val_mae: 0.0911 - val_rmsle_cust: 0.0088\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 257ms/step - loss: 0.0081 - mae: 0.0661 - rmsle_cust: 0.0058 - val_loss: 0.0159 - val_mae: 0.0925 - val_rmsle_cust: 0.0088\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 275ms/step - loss: 0.0080 - mae: 0.0660 - rmsle_cust: 0.0057 - val_loss: 0.0154 - val_mae: 0.0907 - val_rmsle_cust: 0.0087\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 276ms/step - loss: 0.0074 - mae: 0.0629 - rmsle_cust: 0.0052 - val_loss: 0.0147 - val_mae: 0.0881 - val_rmsle_cust: 0.0084\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 247ms/step - loss: 0.0068 - mae: 0.0594 - rmsle_cust: 0.0053 - val_loss: 0.0145 - val_mae: 0.0869 - val_rmsle_cust: 0.0083\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 1s 250ms/step - loss: 0.0063 - mae: 0.0566 - rmsle_cust: 0.0052 - val_loss: 0.0143 - val_mae: 0.0863 - val_rmsle_cust: 0.0084\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 326ms/step - loss: 0.0064 - mae: 0.0571 - rmsle_cust: 0.0054 - val_loss: 0.0140 - val_mae: 0.0852 - val_rmsle_cust: 0.0084\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 296ms/step - loss: 0.0058 - mae: 0.0546 - rmsle_cust: 0.0055 - val_loss: 0.0139 - val_mae: 0.0848 - val_rmsle_cust: 0.0085\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 280ms/step - loss: 0.0058 - mae: 0.0545 - rmsle_cust: 0.0053 - val_loss: 0.0139 - val_mae: 0.0853 - val_rmsle_cust: 0.0085\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 246ms/step - loss: 0.0057 - mae: 0.0543 - rmsle_cust: 0.0053 - val_loss: 0.0137 - val_mae: 0.0851 - val_rmsle_cust: 0.0085\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 287ms/step - loss: 0.0054 - mae: 0.0524 - rmsle_cust: 0.0051 - val_loss: 0.0135 - val_mae: 0.0842 - val_rmsle_cust: 0.0085\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 1s 281ms/step - loss: 0.0050 - mae: 0.0502 - rmsle_cust: 0.0048 - val_loss: 0.0134 - val_mae: 0.0839 - val_rmsle_cust: 0.0086\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 1s 280ms/step - loss: 0.0048 - mae: 0.0497 - rmsle_cust: 0.0047 - val_loss: 0.0134 - val_mae: 0.0838 - val_rmsle_cust: 0.0086\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 0.0048 - mae: 0.0488 - rmsle_cust: 0.0046 - val_loss: 0.0133 - val_mae: 0.0835 - val_rmsle_cust: 0.0086\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 1s 278ms/step - loss: 0.0047 - mae: 0.0481 - rmsle_cust: 0.0044 - val_loss: 0.0133 - val_mae: 0.0831 - val_rmsle_cust: 0.0086\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 1s 261ms/step - loss: 0.0046 - mae: 0.0478 - rmsle_cust: 0.0044 - val_loss: 0.0132 - val_mae: 0.0826 - val_rmsle_cust: 0.0086\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 1s 264ms/step - loss: 0.0044 - mae: 0.0474 - rmsle_cust: 0.0043 - val_loss: 0.0132 - val_mae: 0.0821 - val_rmsle_cust: 0.0086\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 1s 266ms/step - loss: 0.0044 - mae: 0.0468 - rmsle_cust: 0.0044 - val_loss: 0.0131 - val_mae: 0.0815 - val_rmsle_cust: 0.0085\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 1s 255ms/step - loss: 0.0043 - mae: 0.0460 - rmsle_cust: 0.0041 - val_loss: 0.0130 - val_mae: 0.0811 - val_rmsle_cust: 0.0084\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 1s 263ms/step - loss: 0.0041 - mae: 0.0454 - rmsle_cust: 0.0042 - val_loss: 0.0129 - val_mae: 0.0810 - val_rmsle_cust: 0.0083\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 1s 245ms/step - loss: 0.0040 - mae: 0.0446 - rmsle_cust: 0.0040 - val_loss: 0.0129 - val_mae: 0.0809 - val_rmsle_cust: 0.0083\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 1s 257ms/step - loss: 0.0038 - mae: 0.0439 - rmsle_cust: 0.0040 - val_loss: 0.0128 - val_mae: 0.0808 - val_rmsle_cust: 0.0082\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 1s 243ms/step - loss: 0.0038 - mae: 0.0433 - rmsle_cust: 0.0040 - val_loss: 0.0128 - val_mae: 0.0808 - val_rmsle_cust: 0.0082\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 1s 256ms/step - loss: 0.0037 - mae: 0.0434 - rmsle_cust: 0.0039 - val_loss: 0.0127 - val_mae: 0.0806 - val_rmsle_cust: 0.0082\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 0.0037 - mae: 0.0431 - rmsle_cust: 0.0037 - val_loss: 0.0127 - val_mae: 0.0803 - val_rmsle_cust: 0.0082\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 1s 252ms/step - loss: 0.0037 - mae: 0.0429 - rmsle_cust: 0.0038 - val_loss: 0.0127 - val_mae: 0.0801 - val_rmsle_cust: 0.0082\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 1s 255ms/step - loss: 0.0036 - mae: 0.0419 - rmsle_cust: 0.0037 - val_loss: 0.0126 - val_mae: 0.0799 - val_rmsle_cust: 0.0082\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 1s 258ms/step - loss: 0.0036 - mae: 0.0423 - rmsle_cust: 0.0037 - val_loss: 0.0126 - val_mae: 0.0795 - val_rmsle_cust: 0.0082\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 272ms/step - loss: 0.0034 - mae: 0.0413 - rmsle_cust: 0.0037 - val_loss: 0.0126 - val_mae: 0.0793 - val_rmsle_cust: 0.0082\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 294ms/step - loss: 0.0034 - mae: 0.0409 - rmsle_cust: 0.0035 - val_loss: 0.0126 - val_mae: 0.0791 - val_rmsle_cust: 0.0082\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 1s 303ms/step - loss: 0.0032 - mae: 0.0403 - rmsle_cust: 0.0035 - val_loss: 0.0125 - val_mae: 0.0790 - val_rmsle_cust: 0.0083\n",
      "11/11 [==============================] - 1s 11ms/step\n",
      " RMSLE error for GRU Based Model on Test Data for K-Fold Validation: 0.24197557239766782\n",
      " RMSE error for GRU Based Model on Test Data for K-Fold Validation: 11.3019072419932\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, random_state=1, shuffle=True) \n",
    "kgru_output = []\n",
    "for train_index, test_index in kf.split(data_cleaned):\n",
    "#     print(\"Train:\", train_index, \"Validation:\",test_index)\n",
    "    KX_train, KX_test = data_cleaned.iloc[train_index], data_cleaned.iloc[test_index]\n",
    "    KXX_train = get_keras_data(KX_train)\n",
    "    KXX_test = get_keras_data(KX_test)\n",
    "    gru_model = get_gru_model(KXX_train)\n",
    "    gru_model.fit(KXX_train, KX_train.target, epochs=epochs, batch_size=BATCH_SIZE\n",
    "          , validation_data=(KXX_test, KX_test.target)\n",
    "          , verbose=1, callbacks=[history])\n",
    "    val_preds = gru_model.predict(KXX_test)\n",
    "    val_preds = target_scaler.inverse_transform(val_preds)\n",
    "    val_preds = np.exp(val_preds)+1\n",
    "    y_true = np.array(KX_test.mrp.values)\n",
    "    y_pred = val_preds[:,0]\n",
    "    v_rmsle = rmsle(y_true, y_pred)\n",
    "    v_rmse = mean_squared_error(y_true, y_pred , squared=False)\n",
    "    output.append([v_rmse, v_rmsle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RMSLE error for GRU Based Model on Test Data for K-Fold Validation: 0.24197557239766782\n",
      " RMSE error for GRU Based Model on Test Data for K-Fold Validation: 11.3019072419932\n"
     ]
    }
   ],
   "source": [
    "print(\" RMSLE error for GRU Based Model on Test Data for K-Fold Validation: \"+str(sum([i[1] for i in output])/len(output)))\n",
    "print(\" RMSE error for GRU Based Model on Test Data for K-Fold Validation: \"+str(sum([i[0] for i in output])/len(output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_33\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " item_desc (InputLayer)         [(None, 75)]         0           []                               \n",
      "                                                                                                  \n",
      " item_name (InputLayer)         [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " brand_name (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " product_category (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_133 (Embedding)      (None, 75, 50)       195500      ['item_desc[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_132 (Embedding)      (None, 10, 50)       195500      ['item_name[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_134 (Embedding)      (None, 1, 10)        160         ['brand_name[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_135 (Embedding)      (None, 1, 10)        160         ['product_category[0][0]']       \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 16)           4288        ['embedding_133[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 8)            1888        ['embedding_132[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_66 (Flatten)           (None, 10)           0           ['embedding_134[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_67 (Flatten)           (None, 10)           0           ['embedding_135[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_33 (Concatenate)   (None, 44)           0           ['lstm[0][0]',                   \n",
      "                                                                  'lstm_1[0][0]',                 \n",
      "                                                                  'flatten_66[0][0]',             \n",
      "                                                                  'flatten_67[0][0]']             \n",
      "                                                                                                  \n",
      " dense_99 (Dense)               (None, 128)          5760        ['concatenate_33[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_66 (Dropout)           (None, 128)          0           ['dense_99[0][0]']               \n",
      "                                                                                                  \n",
      " dense_100 (Dense)              (None, 64)           8256        ['dropout_66[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_67 (Dropout)           (None, 64)           0           ['dense_100[0][0]']              \n",
      "                                                                                                  \n",
      " dense_101 (Dense)              (None, 1)            65          ['dropout_67[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 411,577\n",
      "Trainable params: 411,577\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_lstm_model(data):\n",
    "    # Params\n",
    "    dr_r = 0.1\n",
    "    \n",
    "    # Inputs\n",
    "    item_name = Input(shape=[data[\"item_name\"].shape[1]], name=\"item_name\")\n",
    "    item_desc = Input(shape=[data[\"item_desc\"].shape[1]], name=\"item_desc\")\n",
    "    brand_name = Input(shape=[1], name=\"brand_name\")\n",
    "    product_category = Input(shape=[1], name=\"product_category\")\n",
    "    \n",
    "    # Embeddings Layers\n",
    "    emb_item_name = Embedding(MAX_TEXT, 50)(item_name)\n",
    "    emb_item_desc = Embedding(MAX_TEXT, 50)(item_desc)\n",
    "    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n",
    "    emb_product_category = Embedding(MAX_CAT, 10)(product_category)\n",
    "    \n",
    "    # RNN Layer\n",
    "    rnn_layer1 = LSTM(16) (emb_item_desc)\n",
    "    rnn_layer2 = LSTM(8) (emb_item_name)\n",
    "    \n",
    "    # Main Layer\n",
    "    main_l = concatenate([\n",
    "        rnn_layer1,\n",
    "        rnn_layer2,\n",
    "        Flatten() (emb_brand_name)\n",
    "        , Flatten() (emb_product_category)\n",
    "    ])\n",
    "    main_l = Dropout(dr_r) (Dense(128) (main_l))\n",
    "    main_l = Dropout(dr_r) (Dense(64) (main_l))\n",
    "    \n",
    "    # Output Layer\n",
    "    output = Dense(1, activation=\"linear\") (main_l)\n",
    "    \n",
    "    # Init Model\n",
    "    model = Model([item_name, item_desc, brand_name, product_category], output)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", rmsle_cust])\n",
    "    \n",
    "    return model\n",
    "\n",
    "    \n",
    "lstm_model = get_lstm_model(X_train)\n",
    "history = LossHistory()\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2/2 [==============================] - 6s 1s/step - loss: 0.1842 - mae: 0.3558 - rmsle_cust: 0.0155 - val_loss: 0.1133 - val_mae: 0.2637 - val_rmsle_cust: 0.0077\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 1s 512ms/step - loss: 0.1216 - mae: 0.2751 - rmsle_cust: 0.0101 - val_loss: 0.0711 - val_mae: 0.2052 - val_rmsle_cust: 0.0077\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 2s 613ms/step - loss: 0.0775 - mae: 0.2168 - rmsle_cust: 0.0101 - val_loss: 0.0621 - val_mae: 0.2076 - val_rmsle_cust: 0.0077\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.0675 - mae: 0.2145 - rmsle_cust: 0.0101 - val_loss: 0.0674 - val_mae: 0.2225 - val_rmsle_cust: 0.0077\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 383ms/step - loss: 0.0685 - mae: 0.2208 - rmsle_cust: 0.0101 - val_loss: 0.0538 - val_mae: 0.1968 - val_rmsle_cust: 0.0077\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 465ms/step - loss: 0.0543 - mae: 0.1938 - rmsle_cust: 0.0101 - val_loss: 0.0390 - val_mae: 0.1609 - val_rmsle_cust: 0.0077\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 2s 623ms/step - loss: 0.0415 - mae: 0.1630 - rmsle_cust: 0.0101 - val_loss: 0.0327 - val_mae: 0.1405 - val_rmsle_cust: 0.0077\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.0361 - mae: 0.1462 - rmsle_cust: 0.0101 - val_loss: 0.0303 - val_mae: 0.1331 - val_rmsle_cust: 0.0077\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 392ms/step - loss: 0.0336 - mae: 0.1400 - rmsle_cust: 0.0099 - val_loss: 0.0264 - val_mae: 0.1246 - val_rmsle_cust: 0.0075\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 554ms/step - loss: 0.0282 - mae: 0.1291 - rmsle_cust: 0.0092 - val_loss: 0.0210 - val_mae: 0.1104 - val_rmsle_cust: 0.0073\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.0223 - mae: 0.1136 - rmsle_cust: 0.0088 - val_loss: 0.0173 - val_mae: 0.0991 - val_rmsle_cust: 0.0071\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 589ms/step - loss: 0.0184 - mae: 0.1001 - rmsle_cust: 0.0086 - val_loss: 0.0172 - val_mae: 0.0976 - val_rmsle_cust: 0.0071\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 487ms/step - loss: 0.0185 - mae: 0.0997 - rmsle_cust: 0.0085 - val_loss: 0.0172 - val_mae: 0.0980 - val_rmsle_cust: 0.0074\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 505ms/step - loss: 0.0177 - mae: 0.0973 - rmsle_cust: 0.0084 - val_loss: 0.0156 - val_mae: 0.0936 - val_rmsle_cust: 0.0082\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 379ms/step - loss: 0.0159 - mae: 0.0929 - rmsle_cust: 0.0094 - val_loss: 0.0146 - val_mae: 0.0919 - val_rmsle_cust: 0.0096\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.0140 - mae: 0.0896 - rmsle_cust: 0.0101 - val_loss: 0.0143 - val_mae: 0.0926 - val_rmsle_cust: 0.0095\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 475ms/step - loss: 0.0129 - mae: 0.0875 - rmsle_cust: 0.0098 - val_loss: 0.0130 - val_mae: 0.0871 - val_rmsle_cust: 0.0077\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.0117 - mae: 0.0821 - rmsle_cust: 0.0081 - val_loss: 0.0113 - val_mae: 0.0792 - val_rmsle_cust: 0.0062\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 376ms/step - loss: 0.0095 - mae: 0.0712 - rmsle_cust: 0.0068 - val_loss: 0.0105 - val_mae: 0.0749 - val_rmsle_cust: 0.0057\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.0089 - mae: 0.0674 - rmsle_cust: 0.0064 - val_loss: 0.0106 - val_mae: 0.0753 - val_rmsle_cust: 0.0058\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.0088 - mae: 0.0672 - rmsle_cust: 0.0063 - val_loss: 0.0103 - val_mae: 0.0743 - val_rmsle_cust: 0.0058\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.0078 - mae: 0.0634 - rmsle_cust: 0.0061 - val_loss: 0.0099 - val_mae: 0.0732 - val_rmsle_cust: 0.0058\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.0072 - mae: 0.0612 - rmsle_cust: 0.0059 - val_loss: 0.0098 - val_mae: 0.0729 - val_rmsle_cust: 0.0056\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 457ms/step - loss: 0.0067 - mae: 0.0596 - rmsle_cust: 0.0057 - val_loss: 0.0097 - val_mae: 0.0725 - val_rmsle_cust: 0.0055\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 1s 382ms/step - loss: 0.0064 - mae: 0.0584 - rmsle_cust: 0.0058 - val_loss: 0.0093 - val_mae: 0.0711 - val_rmsle_cust: 0.0054\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 563ms/step - loss: 0.0060 - mae: 0.0569 - rmsle_cust: 0.0056 - val_loss: 0.0089 - val_mae: 0.0698 - val_rmsle_cust: 0.0053\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 596ms/step - loss: 0.0059 - mae: 0.0560 - rmsle_cust: 0.0058 - val_loss: 0.0087 - val_mae: 0.0688 - val_rmsle_cust: 0.0052\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 513ms/step - loss: 0.0057 - mae: 0.0550 - rmsle_cust: 0.0057 - val_loss: 0.0085 - val_mae: 0.0676 - val_rmsle_cust: 0.0052\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 482ms/step - loss: 0.0052 - mae: 0.0534 - rmsle_cust: 0.0056 - val_loss: 0.0083 - val_mae: 0.0663 - val_rmsle_cust: 0.0051\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 674ms/step - loss: 0.0049 - mae: 0.0511 - rmsle_cust: 0.0058 - val_loss: 0.0081 - val_mae: 0.0652 - val_rmsle_cust: 0.0050\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.0049 - mae: 0.0511 - rmsle_cust: 0.0055 - val_loss: 0.0079 - val_mae: 0.0640 - val_rmsle_cust: 0.0049\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 1s 546ms/step - loss: 0.0046 - mae: 0.0490 - rmsle_cust: 0.0048 - val_loss: 0.0077 - val_mae: 0.0631 - val_rmsle_cust: 0.0049\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 392ms/step - loss: 0.0045 - mae: 0.0487 - rmsle_cust: 0.0046 - val_loss: 0.0076 - val_mae: 0.0627 - val_rmsle_cust: 0.0048\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 1s 480ms/step - loss: 0.0043 - mae: 0.0470 - rmsle_cust: 0.0043 - val_loss: 0.0076 - val_mae: 0.0625 - val_rmsle_cust: 0.0048\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 1s 487ms/step - loss: 0.0041 - mae: 0.0466 - rmsle_cust: 0.0040 - val_loss: 0.0076 - val_mae: 0.0622 - val_rmsle_cust: 0.0047\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.0039 - mae: 0.0449 - rmsle_cust: 0.0041 - val_loss: 0.0075 - val_mae: 0.0617 - val_rmsle_cust: 0.0047\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.0039 - mae: 0.0450 - rmsle_cust: 0.0039 - val_loss: 0.0074 - val_mae: 0.0611 - val_rmsle_cust: 0.0047\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 1s 458ms/step - loss: 0.0038 - mae: 0.0444 - rmsle_cust: 0.0039 - val_loss: 0.0073 - val_mae: 0.0607 - val_rmsle_cust: 0.0047\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 1s 376ms/step - loss: 0.0037 - mae: 0.0437 - rmsle_cust: 0.0039 - val_loss: 0.0072 - val_mae: 0.0603 - val_rmsle_cust: 0.0046\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 2s 583ms/step - loss: 0.0035 - mae: 0.0430 - rmsle_cust: 0.0036 - val_loss: 0.0072 - val_mae: 0.0599 - val_rmsle_cust: 0.0046\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 2s 657ms/step - loss: 0.0036 - mae: 0.0435 - rmsle_cust: 0.0036 - val_loss: 0.0072 - val_mae: 0.0598 - val_rmsle_cust: 0.0046\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 1s 499ms/step - loss: 0.0034 - mae: 0.0422 - rmsle_cust: 0.0037 - val_loss: 0.0071 - val_mae: 0.0594 - val_rmsle_cust: 0.0046\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 1s 371ms/step - loss: 0.0033 - mae: 0.0416 - rmsle_cust: 0.0036 - val_loss: 0.0071 - val_mae: 0.0591 - val_rmsle_cust: 0.0046\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 474ms/step - loss: 0.0031 - mae: 0.0404 - rmsle_cust: 0.0033 - val_loss: 0.0071 - val_mae: 0.0589 - val_rmsle_cust: 0.0045\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 1s 584ms/step - loss: 0.0032 - mae: 0.0407 - rmsle_cust: 0.0033 - val_loss: 0.0070 - val_mae: 0.0590 - val_rmsle_cust: 0.0045\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 2s 465ms/step - loss: 0.0030 - mae: 0.0404 - rmsle_cust: 0.0032 - val_loss: 0.0070 - val_mae: 0.0590 - val_rmsle_cust: 0.0045\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 462ms/step - loss: 0.0030 - mae: 0.0402 - rmsle_cust: 0.0033 - val_loss: 0.0069 - val_mae: 0.0589 - val_rmsle_cust: 0.0045\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 392ms/step - loss: 0.0030 - mae: 0.0404 - rmsle_cust: 0.0032 - val_loss: 0.0069 - val_mae: 0.0586 - val_rmsle_cust: 0.0045\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 389ms/step - loss: 0.0029 - mae: 0.0394 - rmsle_cust: 0.0030 - val_loss: 0.0069 - val_mae: 0.0584 - val_rmsle_cust: 0.0045\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.0028 - mae: 0.0389 - rmsle_cust: 0.0034 - val_loss: 0.0068 - val_mae: 0.0581 - val_rmsle_cust: 0.0045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23ac5b0fa30>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = get_lstm_model(X_train)\n",
    "lstm_model.fit(X_train, dtrain.target, epochs=epochs, batch_size=BATCH_SIZE\n",
    "          , validation_data=(X_test, dtest.target)\n",
    "          , verbose=1, callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_range=np.log(history.losses)\n",
    "sns.distplot(history.losses, hist=False)\n",
    "plt.title('Training Loss')\n",
    "plt.grid()\n",
    "plt.legend('top')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 10ms/step\n",
      " RMSLE error for LSTM Based Model on Test Data: 0.2106766146729653\n",
      " RMSE error for LSTM Based Model on Test Data: 7.451365961524918\n"
     ]
    }
   ],
   "source": [
    "val_preds = lstm_model.predict(X_test)\n",
    "val_preds = target_scaler.inverse_transform(val_preds)\n",
    "val_preds = np.exp(val_preds)+1\n",
    "\n",
    "# mean_absolute_error, mean_squared_log_error.\n",
    "y_true = np.array(dtest.mrp.values)\n",
    "y_pred = val_preds[:,0]\n",
    "v_rmsle = rmsle(y_true, y_pred)\n",
    "v_rmse = mean_squared_error(y_true, y_pred , squared=False)\n",
    "print(\" RMSLE error for LSTM Based Model on Test Data: \"+str(v_rmsle))\n",
    "print(\" RMSE error for LSTM Based Model on Test Data: \"+str(v_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 76ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>product_category_wide</th>\n",
       "      <th>mrp</th>\n",
       "      <th>clean_description</th>\n",
       "      <th>seq_description</th>\n",
       "      <th>seq_product_name</th>\n",
       "      <th>target</th>\n",
       "      <th>predicted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3619</th>\n",
       "      <td>logo to go low rise thong 631581</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>22.00</td>\n",
       "      <td>soft  sheer lace wraps around waist lightweigh...</td>\n",
       "      <td>[18, 72, 1, 1261, 521, 161, 264, 20, 8, 83, 90]</td>\n",
       "      <td>[35, 87, 65, 21, 16, 8, 1980]</td>\n",
       "      <td>-0.468620</td>\n",
       "      <td>24.776022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3444</th>\n",
       "      <td>lace plunge bra</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>35.00</td>\n",
       "      <td>elegant black plunge bra prettified gorgeous l...</td>\n",
       "      <td>[488, 133, 80, 2, 2973, 238, 1, 711, 109, 324,...</td>\n",
       "      <td>[1, 80, 2]</td>\n",
       "      <td>-0.268490</td>\n",
       "      <td>27.632160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517</th>\n",
       "      <td>classic mesh triangle bralette</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>54.00</td>\n",
       "      <td>classic silhouettes airy stretch mesh</td>\n",
       "      <td>[100, 682, 616, 34, 36]</td>\n",
       "      <td>[100, 36, 68, 19]</td>\n",
       "      <td>-0.079174</td>\n",
       "      <td>50.406063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>happy unlined bandeau bra</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>26.95</td>\n",
       "      <td>hello  happy bras  happiness feeling like real...</td>\n",
       "      <td>[240, 52, 92, 148, 269, 150, 12, 235, 360, 52,...</td>\n",
       "      <td>[52, 169, 312, 2]</td>\n",
       "      <td>-0.381549</td>\n",
       "      <td>26.959198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5635</th>\n",
       "      <td>b.provocative contrast-lace bra 951222</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>40.00</td>\n",
       "      <td>flattering sheer nude panels decorated intrica...</td>\n",
       "      <td>[215, 72, 393, 781, 999, 571, 1250, 325, 3311,...</td>\n",
       "      <td>[67, 686, 216, 1, 2, 3892]</td>\n",
       "      <td>-0.210396</td>\n",
       "      <td>38.890709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                product_name  brand_name  product_category_wide    mrp                                  clean_description                                    seq_description               seq_product_name    target  predicted_price\n",
       "3619        logo to go low rise thong 631581           6                      7  22.00  soft  sheer lace wraps around waist lightweigh...    [18, 72, 1, 1261, 521, 161, 264, 20, 8, 83, 90]  [35, 87, 65, 21, 16, 8, 1980] -0.468620        24.776022\n",
       "3444                         lace plunge bra          12                      4  35.00  elegant black plunge bra prettified gorgeous l...  [488, 133, 80, 2, 2973, 238, 1, 711, 109, 324,...                     [1, 80, 2] -0.268490        27.632160\n",
       "2517          classic mesh triangle bralette           6                      3  54.00             classic silhouettes airy stretch mesh                             [100, 682, 616, 34, 36]              [100, 36, 68, 19] -0.079174        50.406063\n",
       "3076               happy unlined bandeau bra           0                      4  26.95  hello  happy bras  happiness feeling like real...  [240, 52, 92, 148, 269, 150, 12, 235, 360, 52,...              [52, 169, 312, 2] -0.381549        26.959198\n",
       "5635  b.provocative contrast-lace bra 951222           1                      4  40.00  flattering sheer nude panels decorated intrica...  [215, 72, 393, 781, 999, 571, 1250, 325, 3311,...     [67, 686, 216, 1, 2, 3892] -0.210396        38.890709"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the price.\n",
    "preds = lstm_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "preds = target_scaler.inverse_transform(preds)\n",
    "preds = np.exp(preds)-1\n",
    "dtest[\"predicted_price\"] = preds\n",
    "dtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.1994 - mae: 0.3724 - rmsle_cust: 0.0201 - val_loss: 0.1331 - val_mae: 0.2952 - val_rmsle_cust: 0.0100\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.1344 - mae: 0.2932 - rmsle_cust: 0.0099 - val_loss: 0.0851 - val_mae: 0.2255 - val_rmsle_cust: 0.0100\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 0.0880 - mae: 0.2295 - rmsle_cust: 0.0099 - val_loss: 0.0591 - val_mae: 0.1889 - val_rmsle_cust: 0.0100\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 550ms/step - loss: 0.0629 - mae: 0.2027 - rmsle_cust: 0.0099 - val_loss: 0.0580 - val_mae: 0.1976 - val_rmsle_cust: 0.0100\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 348ms/step - loss: 0.0608 - mae: 0.2077 - rmsle_cust: 0.0099 - val_loss: 0.0609 - val_mae: 0.2053 - val_rmsle_cust: 0.0100\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 353ms/step - loss: 0.0607 - mae: 0.2065 - rmsle_cust: 0.0099 - val_loss: 0.0503 - val_mae: 0.1831 - val_rmsle_cust: 0.0100\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 1s 359ms/step - loss: 0.0486 - mae: 0.1822 - rmsle_cust: 0.0099 - val_loss: 0.0370 - val_mae: 0.1510 - val_rmsle_cust: 0.0100\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 394ms/step - loss: 0.0353 - mae: 0.1505 - rmsle_cust: 0.0099 - val_loss: 0.0307 - val_mae: 0.1331 - val_rmsle_cust: 0.0098\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 349ms/step - loss: 0.0298 - mae: 0.1330 - rmsle_cust: 0.0098 - val_loss: 0.0296 - val_mae: 0.1322 - val_rmsle_cust: 0.0096\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 333ms/step - loss: 0.0282 - mae: 0.1296 - rmsle_cust: 0.0095 - val_loss: 0.0275 - val_mae: 0.1289 - val_rmsle_cust: 0.0095\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.0252 - mae: 0.1238 - rmsle_cust: 0.0094 - val_loss: 0.0233 - val_mae: 0.1167 - val_rmsle_cust: 0.0095\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 440ms/step - loss: 0.0207 - mae: 0.1105 - rmsle_cust: 0.0098 - val_loss: 0.0201 - val_mae: 0.1021 - val_rmsle_cust: 0.0095\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.0176 - mae: 0.0967 - rmsle_cust: 0.0096 - val_loss: 0.0204 - val_mae: 0.0986 - val_rmsle_cust: 0.0094\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 393ms/step - loss: 0.0179 - mae: 0.0967 - rmsle_cust: 0.0094 - val_loss: 0.0209 - val_mae: 0.1026 - val_rmsle_cust: 0.0093\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 2s 399ms/step - loss: 0.0181 - mae: 0.0979 - rmsle_cust: 0.0092 - val_loss: 0.0185 - val_mae: 0.0953 - val_rmsle_cust: 0.0096\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 519ms/step - loss: 0.0161 - mae: 0.0923 - rmsle_cust: 0.0095 - val_loss: 0.0157 - val_mae: 0.0861 - val_rmsle_cust: 0.0098\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 389ms/step - loss: 0.0131 - mae: 0.0842 - rmsle_cust: 0.0094 - val_loss: 0.0146 - val_mae: 0.0850 - val_rmsle_cust: 0.0097\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 473ms/step - loss: 0.0123 - mae: 0.0830 - rmsle_cust: 0.0091 - val_loss: 0.0137 - val_mae: 0.0827 - val_rmsle_cust: 0.0090\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 2s 804ms/step - loss: 0.0114 - mae: 0.0799 - rmsle_cust: 0.0078 - val_loss: 0.0123 - val_mae: 0.0757 - val_rmsle_cust: 0.0080\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 2s 483ms/step - loss: 0.0098 - mae: 0.0712 - rmsle_cust: 0.0068 - val_loss: 0.0112 - val_mae: 0.0700 - val_rmsle_cust: 0.0076\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 505ms/step - loss: 0.0086 - mae: 0.0655 - rmsle_cust: 0.0061 - val_loss: 0.0110 - val_mae: 0.0699 - val_rmsle_cust: 0.0077\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.0086 - mae: 0.0660 - rmsle_cust: 0.0059 - val_loss: 0.0108 - val_mae: 0.0706 - val_rmsle_cust: 0.0079\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 515ms/step - loss: 0.0079 - mae: 0.0643 - rmsle_cust: 0.0058 - val_loss: 0.0102 - val_mae: 0.0690 - val_rmsle_cust: 0.0081\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 2s 572ms/step - loss: 0.0073 - mae: 0.0620 - rmsle_cust: 0.0058 - val_loss: 0.0097 - val_mae: 0.0676 - val_rmsle_cust: 0.0082\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 2s 527ms/step - loss: 0.0068 - mae: 0.0592 - rmsle_cust: 0.0054 - val_loss: 0.0096 - val_mae: 0.0670 - val_rmsle_cust: 0.0085\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 2s 523ms/step - loss: 0.0062 - mae: 0.0574 - rmsle_cust: 0.0056 - val_loss: 0.0094 - val_mae: 0.0662 - val_rmsle_cust: 0.0086\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 0.0060 - mae: 0.0567 - rmsle_cust: 0.0057 - val_loss: 0.0091 - val_mae: 0.0652 - val_rmsle_cust: 0.0086\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 2s 405ms/step - loss: 0.0056 - mae: 0.0546 - rmsle_cust: 0.0057 - val_loss: 0.0090 - val_mae: 0.0648 - val_rmsle_cust: 0.0086\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 2s 535ms/step - loss: 0.0054 - mae: 0.0536 - rmsle_cust: 0.0053 - val_loss: 0.0089 - val_mae: 0.0643 - val_rmsle_cust: 0.0085\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 541ms/step - loss: 0.0051 - mae: 0.0524 - rmsle_cust: 0.0053 - val_loss: 0.0087 - val_mae: 0.0637 - val_rmsle_cust: 0.0084\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 2s 736ms/step - loss: 0.0048 - mae: 0.0506 - rmsle_cust: 0.0050 - val_loss: 0.0086 - val_mae: 0.0634 - val_rmsle_cust: 0.0083\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 2s 635ms/step - loss: 0.0047 - mae: 0.0498 - rmsle_cust: 0.0050 - val_loss: 0.0086 - val_mae: 0.0637 - val_rmsle_cust: 0.0082\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 2s 857ms/step - loss: 0.0044 - mae: 0.0482 - rmsle_cust: 0.0049 - val_loss: 0.0085 - val_mae: 0.0634 - val_rmsle_cust: 0.0081\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 2s 710ms/step - loss: 0.0044 - mae: 0.0479 - rmsle_cust: 0.0047 - val_loss: 0.0084 - val_mae: 0.0628 - val_rmsle_cust: 0.0080\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 2s 685ms/step - loss: 0.0041 - mae: 0.0466 - rmsle_cust: 0.0043 - val_loss: 0.0084 - val_mae: 0.0622 - val_rmsle_cust: 0.0078\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 2s 877ms/step - loss: 0.0041 - mae: 0.0461 - rmsle_cust: 0.0044 - val_loss: 0.0083 - val_mae: 0.0618 - val_rmsle_cust: 0.0077\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 2s 622ms/step - loss: 0.0038 - mae: 0.0447 - rmsle_cust: 0.0042 - val_loss: 0.0083 - val_mae: 0.0615 - val_rmsle_cust: 0.0076\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 2s 898ms/step - loss: 0.0038 - mae: 0.0448 - rmsle_cust: 0.0041 - val_loss: 0.0083 - val_mae: 0.0612 - val_rmsle_cust: 0.0075\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 2s 652ms/step - loss: 0.0037 - mae: 0.0438 - rmsle_cust: 0.0039 - val_loss: 0.0083 - val_mae: 0.0609 - val_rmsle_cust: 0.0075\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 2s 701ms/step - loss: 0.0035 - mae: 0.0432 - rmsle_cust: 0.0038 - val_loss: 0.0082 - val_mae: 0.0606 - val_rmsle_cust: 0.0074\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 2s 486ms/step - loss: 0.0034 - mae: 0.0420 - rmsle_cust: 0.0036 - val_loss: 0.0082 - val_mae: 0.0603 - val_rmsle_cust: 0.0073\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 1s 468ms/step - loss: 0.0033 - mae: 0.0413 - rmsle_cust: 0.0034 - val_loss: 0.0081 - val_mae: 0.0603 - val_rmsle_cust: 0.0073\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 1s 443ms/step - loss: 0.0032 - mae: 0.0408 - rmsle_cust: 0.0037 - val_loss: 0.0081 - val_mae: 0.0603 - val_rmsle_cust: 0.0072\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 541ms/step - loss: 0.0031 - mae: 0.0403 - rmsle_cust: 0.0036 - val_loss: 0.0081 - val_mae: 0.0602 - val_rmsle_cust: 0.0072\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.0029 - mae: 0.0395 - rmsle_cust: 0.0034 - val_loss: 0.0081 - val_mae: 0.0599 - val_rmsle_cust: 0.0071\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 1s 386ms/step - loss: 0.0030 - mae: 0.0395 - rmsle_cust: 0.0033 - val_loss: 0.0081 - val_mae: 0.0599 - val_rmsle_cust: 0.0071\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 362ms/step - loss: 0.0029 - mae: 0.0394 - rmsle_cust: 0.0032 - val_loss: 0.0081 - val_mae: 0.0597 - val_rmsle_cust: 0.0070\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 370ms/step - loss: 0.0029 - mae: 0.0390 - rmsle_cust: 0.0032 - val_loss: 0.0080 - val_mae: 0.0594 - val_rmsle_cust: 0.0070\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 371ms/step - loss: 0.0028 - mae: 0.0384 - rmsle_cust: 0.0031 - val_loss: 0.0080 - val_mae: 0.0592 - val_rmsle_cust: 0.0070\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.0026 - mae: 0.0369 - rmsle_cust: 0.0031 - val_loss: 0.0081 - val_mae: 0.0593 - val_rmsle_cust: 0.0071\n",
      "11/11 [==============================] - 1s 9ms/step\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.1791 - mae: 0.3499 - rmsle_cust: 0.0140 - val_loss: 0.1193 - val_mae: 0.2731 - val_rmsle_cust: 0.0095\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 1s 329ms/step - loss: 0.1184 - mae: 0.2730 - rmsle_cust: 0.0099 - val_loss: 0.0783 - val_mae: 0.2161 - val_rmsle_cust: 0.0095\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 1s 339ms/step - loss: 0.0788 - mae: 0.2183 - rmsle_cust: 0.0099 - val_loss: 0.0616 - val_mae: 0.2010 - val_rmsle_cust: 0.0095\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.0632 - mae: 0.2039 - rmsle_cust: 0.0099 - val_loss: 0.0644 - val_mae: 0.2133 - val_rmsle_cust: 0.0095\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 373ms/step - loss: 0.0643 - mae: 0.2131 - rmsle_cust: 0.0099 - val_loss: 0.0600 - val_mae: 0.2061 - val_rmsle_cust: 0.0095\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 355ms/step - loss: 0.0578 - mae: 0.2004 - rmsle_cust: 0.0099 - val_loss: 0.0459 - val_mae: 0.1752 - val_rmsle_cust: 0.0095\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 1s 367ms/step - loss: 0.0445 - mae: 0.1713 - rmsle_cust: 0.0099 - val_loss: 0.0353 - val_mae: 0.1496 - val_rmsle_cust: 0.0095\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 354ms/step - loss: 0.0345 - mae: 0.1456 - rmsle_cust: 0.0099 - val_loss: 0.0321 - val_mae: 0.1397 - val_rmsle_cust: 0.0095\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 343ms/step - loss: 0.0309 - mae: 0.1362 - rmsle_cust: 0.0099 - val_loss: 0.0301 - val_mae: 0.1357 - val_rmsle_cust: 0.0097\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 361ms/step - loss: 0.0282 - mae: 0.1309 - rmsle_cust: 0.0096 - val_loss: 0.0251 - val_mae: 0.1238 - val_rmsle_cust: 0.0100\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 334ms/step - loss: 0.0226 - mae: 0.1157 - rmsle_cust: 0.0096 - val_loss: 0.0198 - val_mae: 0.1056 - val_rmsle_cust: 0.0103\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 354ms/step - loss: 0.0179 - mae: 0.0998 - rmsle_cust: 0.0094 - val_loss: 0.0184 - val_mae: 0.0993 - val_rmsle_cust: 0.0105\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 333ms/step - loss: 0.0170 - mae: 0.0960 - rmsle_cust: 0.0092 - val_loss: 0.0192 - val_mae: 0.1025 - val_rmsle_cust: 0.0106\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 350ms/step - loss: 0.0181 - mae: 0.0997 - rmsle_cust: 0.0094 - val_loss: 0.0177 - val_mae: 0.0965 - val_rmsle_cust: 0.0108\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 368ms/step - loss: 0.0163 - mae: 0.0940 - rmsle_cust: 0.0098 - val_loss: 0.0157 - val_mae: 0.0892 - val_rmsle_cust: 0.0108\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 344ms/step - loss: 0.0138 - mae: 0.0865 - rmsle_cust: 0.0102 - val_loss: 0.0152 - val_mae: 0.0919 - val_rmsle_cust: 0.0104\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 333ms/step - loss: 0.0124 - mae: 0.0842 - rmsle_cust: 0.0097 - val_loss: 0.0146 - val_mae: 0.0927 - val_rmsle_cust: 0.0095\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 331ms/step - loss: 0.0116 - mae: 0.0815 - rmsle_cust: 0.0087 - val_loss: 0.0129 - val_mae: 0.0852 - val_rmsle_cust: 0.0085\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 366ms/step - loss: 0.0097 - mae: 0.0720 - rmsle_cust: 0.0072 - val_loss: 0.0116 - val_mae: 0.0769 - val_rmsle_cust: 0.0078\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 535ms/step - loss: 0.0087 - mae: 0.0663 - rmsle_cust: 0.0066 - val_loss: 0.0113 - val_mae: 0.0753 - val_rmsle_cust: 0.0076\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 373ms/step - loss: 0.0084 - mae: 0.0655 - rmsle_cust: 0.0061 - val_loss: 0.0110 - val_mae: 0.0757 - val_rmsle_cust: 0.0075\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.0079 - mae: 0.0643 - rmsle_cust: 0.0061 - val_loss: 0.0105 - val_mae: 0.0742 - val_rmsle_cust: 0.0072\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 361ms/step - loss: 0.0071 - mae: 0.0611 - rmsle_cust: 0.0059 - val_loss: 0.0102 - val_mae: 0.0741 - val_rmsle_cust: 0.0068\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 392ms/step - loss: 0.0067 - mae: 0.0589 - rmsle_cust: 0.0057 - val_loss: 0.0101 - val_mae: 0.0740 - val_rmsle_cust: 0.0068\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 1s 331ms/step - loss: 0.0065 - mae: 0.0585 - rmsle_cust: 0.0060 - val_loss: 0.0096 - val_mae: 0.0721 - val_rmsle_cust: 0.0069\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 355ms/step - loss: 0.0060 - mae: 0.0568 - rmsle_cust: 0.0060 - val_loss: 0.0091 - val_mae: 0.0696 - val_rmsle_cust: 0.0069\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 353ms/step - loss: 0.0056 - mae: 0.0546 - rmsle_cust: 0.0060 - val_loss: 0.0089 - val_mae: 0.0684 - val_rmsle_cust: 0.0070\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 363ms/step - loss: 0.0053 - mae: 0.0529 - rmsle_cust: 0.0058 - val_loss: 0.0088 - val_mae: 0.0669 - val_rmsle_cust: 0.0071\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 346ms/step - loss: 0.0051 - mae: 0.0521 - rmsle_cust: 0.0056 - val_loss: 0.0087 - val_mae: 0.0657 - val_rmsle_cust: 0.0072\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 350ms/step - loss: 0.0049 - mae: 0.0509 - rmsle_cust: 0.0055 - val_loss: 0.0088 - val_mae: 0.0663 - val_rmsle_cust: 0.0073\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 1s 350ms/step - loss: 0.0046 - mae: 0.0496 - rmsle_cust: 0.0052 - val_loss: 0.0087 - val_mae: 0.0656 - val_rmsle_cust: 0.0072\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 1s 370ms/step - loss: 0.0044 - mae: 0.0482 - rmsle_cust: 0.0048 - val_loss: 0.0086 - val_mae: 0.0641 - val_rmsle_cust: 0.0071\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 338ms/step - loss: 0.0042 - mae: 0.0468 - rmsle_cust: 0.0047 - val_loss: 0.0085 - val_mae: 0.0637 - val_rmsle_cust: 0.0070\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 1s 375ms/step - loss: 0.0040 - mae: 0.0459 - rmsle_cust: 0.0043 - val_loss: 0.0086 - val_mae: 0.0640 - val_rmsle_cust: 0.0070\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 1s 333ms/step - loss: 0.0038 - mae: 0.0449 - rmsle_cust: 0.0042 - val_loss: 0.0086 - val_mae: 0.0642 - val_rmsle_cust: 0.0071\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 1s 348ms/step - loss: 0.0037 - mae: 0.0442 - rmsle_cust: 0.0041 - val_loss: 0.0087 - val_mae: 0.0644 - val_rmsle_cust: 0.0071\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 1s 367ms/step - loss: 0.0036 - mae: 0.0437 - rmsle_cust: 0.0039 - val_loss: 0.0087 - val_mae: 0.0641 - val_rmsle_cust: 0.0071\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.0034 - mae: 0.0418 - rmsle_cust: 0.0038 - val_loss: 0.0086 - val_mae: 0.0633 - val_rmsle_cust: 0.0070\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 1s 366ms/step - loss: 0.0033 - mae: 0.0417 - rmsle_cust: 0.0037 - val_loss: 0.0086 - val_mae: 0.0628 - val_rmsle_cust: 0.0070\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 1s 393ms/step - loss: 0.0032 - mae: 0.0409 - rmsle_cust: 0.0038 - val_loss: 0.0086 - val_mae: 0.0626 - val_rmsle_cust: 0.0069\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 1s 380ms/step - loss: 0.0032 - mae: 0.0414 - rmsle_cust: 0.0038 - val_loss: 0.0086 - val_mae: 0.0626 - val_rmsle_cust: 0.0068\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 1s 353ms/step - loss: 0.0030 - mae: 0.0396 - rmsle_cust: 0.0035 - val_loss: 0.0086 - val_mae: 0.0627 - val_rmsle_cust: 0.0067\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 411ms/step - loss: 0.0030 - mae: 0.0400 - rmsle_cust: 0.0036 - val_loss: 0.0086 - val_mae: 0.0627 - val_rmsle_cust: 0.0066\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 338ms/step - loss: 0.0027 - mae: 0.0375 - rmsle_cust: 0.0035 - val_loss: 0.0086 - val_mae: 0.0624 - val_rmsle_cust: 0.0066\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 1s 362ms/step - loss: 0.0029 - mae: 0.0385 - rmsle_cust: 0.0033 - val_loss: 0.0086 - val_mae: 0.0624 - val_rmsle_cust: 0.0065\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 1s 361ms/step - loss: 0.0027 - mae: 0.0377 - rmsle_cust: 0.0033 - val_loss: 0.0086 - val_mae: 0.0625 - val_rmsle_cust: 0.0065\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 1s 366ms/step - loss: 0.0026 - mae: 0.0368 - rmsle_cust: 0.0032 - val_loss: 0.0087 - val_mae: 0.0630 - val_rmsle_cust: 0.0065\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 343ms/step - loss: 0.0026 - mae: 0.0367 - rmsle_cust: 0.0032 - val_loss: 0.0087 - val_mae: 0.0630 - val_rmsle_cust: 0.0065\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 347ms/step - loss: 0.0025 - mae: 0.0364 - rmsle_cust: 0.0031 - val_loss: 0.0087 - val_mae: 0.0624 - val_rmsle_cust: 0.0064\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 1s 337ms/step - loss: 0.0025 - mae: 0.0366 - rmsle_cust: 0.0031 - val_loss: 0.0087 - val_mae: 0.0621 - val_rmsle_cust: 0.0063\n",
      "11/11 [==============================] - 1s 10ms/step\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 6s 1s/step - loss: 0.1952 - mae: 0.3664 - rmsle_cust: 0.0169 - val_loss: 0.1453 - val_mae: 0.3073 - val_rmsle_cust: 0.0114\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 1s 393ms/step - loss: 0.1386 - mae: 0.2981 - rmsle_cust: 0.0097 - val_loss: 0.1002 - val_mae: 0.2475 - val_rmsle_cust: 0.0114\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.0944 - mae: 0.2384 - rmsle_cust: 0.0097 - val_loss: 0.0708 - val_mae: 0.2142 - val_rmsle_cust: 0.0114\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 357ms/step - loss: 0.0665 - mae: 0.2048 - rmsle_cust: 0.0097 - val_loss: 0.0648 - val_mae: 0.2133 - val_rmsle_cust: 0.0114\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 360ms/step - loss: 0.0620 - mae: 0.2092 - rmsle_cust: 0.0097 - val_loss: 0.0674 - val_mae: 0.2189 - val_rmsle_cust: 0.0114\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 371ms/step - loss: 0.0630 - mae: 0.2115 - rmsle_cust: 0.0097 - val_loss: 0.0566 - val_mae: 0.1980 - val_rmsle_cust: 0.0114\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 1s 354ms/step - loss: 0.0515 - mae: 0.1884 - rmsle_cust: 0.0097 - val_loss: 0.0425 - val_mae: 0.1709 - val_rmsle_cust: 0.0114\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 379ms/step - loss: 0.0378 - mae: 0.1580 - rmsle_cust: 0.0097 - val_loss: 0.0356 - val_mae: 0.1541 - val_rmsle_cust: 0.0114\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 367ms/step - loss: 0.0321 - mae: 0.1429 - rmsle_cust: 0.0095 - val_loss: 0.0335 - val_mae: 0.1475 - val_rmsle_cust: 0.0103\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.0304 - mae: 0.1389 - rmsle_cust: 0.0094 - val_loss: 0.0287 - val_mae: 0.1356 - val_rmsle_cust: 0.0107\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 364ms/step - loss: 0.0254 - mae: 0.1271 - rmsle_cust: 0.0100 - val_loss: 0.0216 - val_mae: 0.1143 - val_rmsle_cust: 0.0106\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 390ms/step - loss: 0.0182 - mae: 0.1044 - rmsle_cust: 0.0099 - val_loss: 0.0186 - val_mae: 0.0997 - val_rmsle_cust: 0.0100\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 347ms/step - loss: 0.0158 - mae: 0.0934 - rmsle_cust: 0.0089 - val_loss: 0.0200 - val_mae: 0.1016 - val_rmsle_cust: 0.0095\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 374ms/step - loss: 0.0168 - mae: 0.0955 - rmsle_cust: 0.0086 - val_loss: 0.0186 - val_mae: 0.0969 - val_rmsle_cust: 0.0092\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.0153 - mae: 0.0908 - rmsle_cust: 0.0083 - val_loss: 0.0154 - val_mae: 0.0870 - val_rmsle_cust: 0.0095\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 387ms/step - loss: 0.0127 - mae: 0.0843 - rmsle_cust: 0.0089 - val_loss: 0.0146 - val_mae: 0.0881 - val_rmsle_cust: 0.0095\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.0116 - mae: 0.0833 - rmsle_cust: 0.0088 - val_loss: 0.0143 - val_mae: 0.0891 - val_rmsle_cust: 0.0090\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 498ms/step - loss: 0.0109 - mae: 0.0808 - rmsle_cust: 0.0077 - val_loss: 0.0130 - val_mae: 0.0827 - val_rmsle_cust: 0.0085\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 442ms/step - loss: 0.0093 - mae: 0.0720 - rmsle_cust: 0.0067 - val_loss: 0.0122 - val_mae: 0.0769 - val_rmsle_cust: 0.0086\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 369ms/step - loss: 0.0080 - mae: 0.0645 - rmsle_cust: 0.0060 - val_loss: 0.0123 - val_mae: 0.0766 - val_rmsle_cust: 0.0087\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 449ms/step - loss: 0.0077 - mae: 0.0638 - rmsle_cust: 0.0060 - val_loss: 0.0122 - val_mae: 0.0768 - val_rmsle_cust: 0.0085\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 386ms/step - loss: 0.0074 - mae: 0.0629 - rmsle_cust: 0.0059 - val_loss: 0.0116 - val_mae: 0.0758 - val_rmsle_cust: 0.0083\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.0065 - mae: 0.0594 - rmsle_cust: 0.0053 - val_loss: 0.0115 - val_mae: 0.0759 - val_rmsle_cust: 0.0082\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 385ms/step - loss: 0.0061 - mae: 0.0573 - rmsle_cust: 0.0052 - val_loss: 0.0115 - val_mae: 0.0762 - val_rmsle_cust: 0.0083\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.0060 - mae: 0.0569 - rmsle_cust: 0.0052 - val_loss: 0.0110 - val_mae: 0.0744 - val_rmsle_cust: 0.0083\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 382ms/step - loss: 0.0056 - mae: 0.0551 - rmsle_cust: 0.0053 - val_loss: 0.0106 - val_mae: 0.0725 - val_rmsle_cust: 0.0082\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 378ms/step - loss: 0.0051 - mae: 0.0525 - rmsle_cust: 0.0049 - val_loss: 0.0105 - val_mae: 0.0714 - val_rmsle_cust: 0.0081\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.0048 - mae: 0.0505 - rmsle_cust: 0.0049 - val_loss: 0.0103 - val_mae: 0.0705 - val_rmsle_cust: 0.0081\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 386ms/step - loss: 0.0046 - mae: 0.0495 - rmsle_cust: 0.0046 - val_loss: 0.0101 - val_mae: 0.0704 - val_rmsle_cust: 0.0080\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 369ms/step - loss: 0.0045 - mae: 0.0488 - rmsle_cust: 0.0048 - val_loss: 0.0101 - val_mae: 0.0704 - val_rmsle_cust: 0.0080\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.0043 - mae: 0.0480 - rmsle_cust: 0.0045 - val_loss: 0.0100 - val_mae: 0.0695 - val_rmsle_cust: 0.0080\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 1s 366ms/step - loss: 0.0040 - mae: 0.0461 - rmsle_cust: 0.0045 - val_loss: 0.0099 - val_mae: 0.0680 - val_rmsle_cust: 0.0080\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 371ms/step - loss: 0.0038 - mae: 0.0445 - rmsle_cust: 0.0044 - val_loss: 0.0099 - val_mae: 0.0677 - val_rmsle_cust: 0.0079\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 1s 378ms/step - loss: 0.0037 - mae: 0.0442 - rmsle_cust: 0.0043 - val_loss: 0.0099 - val_mae: 0.0678 - val_rmsle_cust: 0.0078\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 1s 383ms/step - loss: 0.0035 - mae: 0.0432 - rmsle_cust: 0.0040 - val_loss: 0.0098 - val_mae: 0.0672 - val_rmsle_cust: 0.0078\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 1s 376ms/step - loss: 0.0034 - mae: 0.0427 - rmsle_cust: 0.0040 - val_loss: 0.0098 - val_mae: 0.0667 - val_rmsle_cust: 0.0077\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.0035 - mae: 0.0427 - rmsle_cust: 0.0039 - val_loss: 0.0097 - val_mae: 0.0663 - val_rmsle_cust: 0.0077\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 1s 376ms/step - loss: 0.0032 - mae: 0.0412 - rmsle_cust: 0.0038 - val_loss: 0.0097 - val_mae: 0.0659 - val_rmsle_cust: 0.0076\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 447ms/step - loss: 0.0032 - mae: 0.0410 - rmsle_cust: 0.0037 - val_loss: 0.0097 - val_mae: 0.0657 - val_rmsle_cust: 0.0076\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 1s 371ms/step - loss: 0.0031 - mae: 0.0405 - rmsle_cust: 0.0035 - val_loss: 0.0097 - val_mae: 0.0655 - val_rmsle_cust: 0.0076\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 1s 535ms/step - loss: 0.0029 - mae: 0.0394 - rmsle_cust: 0.0034 - val_loss: 0.0097 - val_mae: 0.0654 - val_rmsle_cust: 0.0075\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 1s 385ms/step - loss: 0.0029 - mae: 0.0394 - rmsle_cust: 0.0034 - val_loss: 0.0097 - val_mae: 0.0653 - val_rmsle_cust: 0.0075\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 1s 365ms/step - loss: 0.0028 - mae: 0.0389 - rmsle_cust: 0.0033 - val_loss: 0.0097 - val_mae: 0.0652 - val_rmsle_cust: 0.0074\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 359ms/step - loss: 0.0027 - mae: 0.0375 - rmsle_cust: 0.0034 - val_loss: 0.0097 - val_mae: 0.0652 - val_rmsle_cust: 0.0074\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 1s 371ms/step - loss: 0.0027 - mae: 0.0374 - rmsle_cust: 0.0032 - val_loss: 0.0097 - val_mae: 0.0651 - val_rmsle_cust: 0.0074\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 1s 349ms/step - loss: 0.0027 - mae: 0.0379 - rmsle_cust: 0.0031 - val_loss: 0.0097 - val_mae: 0.0647 - val_rmsle_cust: 0.0074\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.0025 - mae: 0.0371 - rmsle_cust: 0.0031 - val_loss: 0.0096 - val_mae: 0.0644 - val_rmsle_cust: 0.0074\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 391ms/step - loss: 0.0025 - mae: 0.0362 - rmsle_cust: 0.0029 - val_loss: 0.0097 - val_mae: 0.0642 - val_rmsle_cust: 0.0074\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 560ms/step - loss: 0.0025 - mae: 0.0360 - rmsle_cust: 0.0031 - val_loss: 0.0097 - val_mae: 0.0642 - val_rmsle_cust: 0.0074\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 1s 473ms/step - loss: 0.0024 - mae: 0.0350 - rmsle_cust: 0.0030 - val_loss: 0.0097 - val_mae: 0.0642 - val_rmsle_cust: 0.0074\n",
      "11/11 [==============================] - 1s 8ms/step\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.1952 - mae: 0.3699 - rmsle_cust: 0.0187 - val_loss: 0.1332 - val_mae: 0.2944 - val_rmsle_cust: 0.0105\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.1360 - mae: 0.2978 - rmsle_cust: 0.0098 - val_loss: 0.0908 - val_mae: 0.2332 - val_rmsle_cust: 0.0105\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 1s 391ms/step - loss: 0.0930 - mae: 0.2374 - rmsle_cust: 0.0098 - val_loss: 0.0650 - val_mae: 0.1969 - val_rmsle_cust: 0.0105\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 370ms/step - loss: 0.0670 - mae: 0.2010 - rmsle_cust: 0.0098 - val_loss: 0.0597 - val_mae: 0.2000 - val_rmsle_cust: 0.0105\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 390ms/step - loss: 0.0609 - mae: 0.2022 - rmsle_cust: 0.0098 - val_loss: 0.0632 - val_mae: 0.2099 - val_rmsle_cust: 0.0105\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.0624 - mae: 0.2094 - rmsle_cust: 0.0098 - val_loss: 0.0571 - val_mae: 0.1971 - val_rmsle_cust: 0.0105\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.0551 - mae: 0.1951 - rmsle_cust: 0.0098 - val_loss: 0.0446 - val_mae: 0.1701 - val_rmsle_cust: 0.0105\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.0436 - mae: 0.1686 - rmsle_cust: 0.0098 - val_loss: 0.0359 - val_mae: 0.1489 - val_rmsle_cust: 0.0105\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 380ms/step - loss: 0.0360 - mae: 0.1470 - rmsle_cust: 0.0097 - val_loss: 0.0332 - val_mae: 0.1413 - val_rmsle_cust: 0.0106\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 477ms/step - loss: 0.0336 - mae: 0.1401 - rmsle_cust: 0.0097 - val_loss: 0.0313 - val_mae: 0.1379 - val_rmsle_cust: 0.0108\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.0311 - mae: 0.1359 - rmsle_cust: 0.0096 - val_loss: 0.0272 - val_mae: 0.1283 - val_rmsle_cust: 0.0109\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.0261 - mae: 0.1243 - rmsle_cust: 0.0097 - val_loss: 0.0224 - val_mae: 0.1127 - val_rmsle_cust: 0.0107\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.0216 - mae: 0.1106 - rmsle_cust: 0.0096 - val_loss: 0.0206 - val_mae: 0.1059 - val_rmsle_cust: 0.0104\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 440ms/step - loss: 0.0196 - mae: 0.1012 - rmsle_cust: 0.0096 - val_loss: 0.0212 - val_mae: 0.1053 - val_rmsle_cust: 0.0100\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 393ms/step - loss: 0.0204 - mae: 0.1027 - rmsle_cust: 0.0098 - val_loss: 0.0203 - val_mae: 0.1032 - val_rmsle_cust: 0.0098\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 506ms/step - loss: 0.0190 - mae: 0.0992 - rmsle_cust: 0.0102 - val_loss: 0.0183 - val_mae: 0.0973 - val_rmsle_cust: 0.0096\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.0165 - mae: 0.0926 - rmsle_cust: 0.0110 - val_loss: 0.0171 - val_mae: 0.0957 - val_rmsle_cust: 0.0094\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 389ms/step - loss: 0.0152 - mae: 0.0907 - rmsle_cust: 0.0111 - val_loss: 0.0162 - val_mae: 0.0946 - val_rmsle_cust: 0.0084\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 477ms/step - loss: 0.0132 - mae: 0.0856 - rmsle_cust: 0.0098 - val_loss: 0.0146 - val_mae: 0.0890 - val_rmsle_cust: 0.0075\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 469ms/step - loss: 0.0113 - mae: 0.0773 - rmsle_cust: 0.0081 - val_loss: 0.0129 - val_mae: 0.0815 - val_rmsle_cust: 0.0071\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.0097 - mae: 0.0691 - rmsle_cust: 0.0070 - val_loss: 0.0122 - val_mae: 0.0793 - val_rmsle_cust: 0.0072\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 493ms/step - loss: 0.0091 - mae: 0.0670 - rmsle_cust: 0.0063 - val_loss: 0.0118 - val_mae: 0.0792 - val_rmsle_cust: 0.0073\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.0085 - mae: 0.0663 - rmsle_cust: 0.0060 - val_loss: 0.0112 - val_mae: 0.0767 - val_rmsle_cust: 0.0072\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 384ms/step - loss: 0.0077 - mae: 0.0632 - rmsle_cust: 0.0058 - val_loss: 0.0107 - val_mae: 0.0743 - val_rmsle_cust: 0.0071\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 1s 484ms/step - loss: 0.0072 - mae: 0.0610 - rmsle_cust: 0.0059 - val_loss: 0.0105 - val_mae: 0.0733 - val_rmsle_cust: 0.0070\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 390ms/step - loss: 0.0070 - mae: 0.0605 - rmsle_cust: 0.0061 - val_loss: 0.0102 - val_mae: 0.0716 - val_rmsle_cust: 0.0070\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.0066 - mae: 0.0587 - rmsle_cust: 0.0060 - val_loss: 0.0098 - val_mae: 0.0695 - val_rmsle_cust: 0.0071\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.0060 - mae: 0.0567 - rmsle_cust: 0.0058 - val_loss: 0.0095 - val_mae: 0.0686 - val_rmsle_cust: 0.0070\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.0059 - mae: 0.0562 - rmsle_cust: 0.0056 - val_loss: 0.0091 - val_mae: 0.0670 - val_rmsle_cust: 0.0070\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 380ms/step - loss: 0.0053 - mae: 0.0527 - rmsle_cust: 0.0052 - val_loss: 0.0089 - val_mae: 0.0654 - val_rmsle_cust: 0.0069\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 1s 478ms/step - loss: 0.0048 - mae: 0.0503 - rmsle_cust: 0.0052 - val_loss: 0.0088 - val_mae: 0.0652 - val_rmsle_cust: 0.0069\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.0047 - mae: 0.0497 - rmsle_cust: 0.0049 - val_loss: 0.0086 - val_mae: 0.0645 - val_rmsle_cust: 0.0068\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 538ms/step - loss: 0.0046 - mae: 0.0488 - rmsle_cust: 0.0049 - val_loss: 0.0084 - val_mae: 0.0634 - val_rmsle_cust: 0.0067\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 1s 382ms/step - loss: 0.0044 - mae: 0.0476 - rmsle_cust: 0.0045 - val_loss: 0.0082 - val_mae: 0.0626 - val_rmsle_cust: 0.0066\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 478ms/step - loss: 0.0041 - mae: 0.0462 - rmsle_cust: 0.0042 - val_loss: 0.0081 - val_mae: 0.0619 - val_rmsle_cust: 0.0065\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 1s 445ms/step - loss: 0.0039 - mae: 0.0452 - rmsle_cust: 0.0040 - val_loss: 0.0080 - val_mae: 0.0611 - val_rmsle_cust: 0.0063\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 1s 380ms/step - loss: 0.0038 - mae: 0.0446 - rmsle_cust: 0.0039 - val_loss: 0.0079 - val_mae: 0.0604 - val_rmsle_cust: 0.0061\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.0036 - mae: 0.0434 - rmsle_cust: 0.0040 - val_loss: 0.0078 - val_mae: 0.0600 - val_rmsle_cust: 0.0059\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.0035 - mae: 0.0425 - rmsle_cust: 0.0038 - val_loss: 0.0077 - val_mae: 0.0597 - val_rmsle_cust: 0.0058\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.0034 - mae: 0.0427 - rmsle_cust: 0.0037 - val_loss: 0.0076 - val_mae: 0.0592 - val_rmsle_cust: 0.0058\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 1s 369ms/step - loss: 0.0032 - mae: 0.0408 - rmsle_cust: 0.0034 - val_loss: 0.0075 - val_mae: 0.0589 - val_rmsle_cust: 0.0058\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.0032 - mae: 0.0416 - rmsle_cust: 0.0035 - val_loss: 0.0075 - val_mae: 0.0588 - val_rmsle_cust: 0.0058\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 1s 502ms/step - loss: 0.0030 - mae: 0.0392 - rmsle_cust: 0.0033 - val_loss: 0.0075 - val_mae: 0.0584 - val_rmsle_cust: 0.0058\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 372ms/step - loss: 0.0030 - mae: 0.0397 - rmsle_cust: 0.0034 - val_loss: 0.0074 - val_mae: 0.0580 - val_rmsle_cust: 0.0058\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.0029 - mae: 0.0386 - rmsle_cust: 0.0033 - val_loss: 0.0073 - val_mae: 0.0576 - val_rmsle_cust: 0.0058\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 1s 363ms/step - loss: 0.0027 - mae: 0.0378 - rmsle_cust: 0.0031 - val_loss: 0.0073 - val_mae: 0.0571 - val_rmsle_cust: 0.0057\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 1s 362ms/step - loss: 0.0028 - mae: 0.0381 - rmsle_cust: 0.0033 - val_loss: 0.0072 - val_mae: 0.0567 - val_rmsle_cust: 0.0057\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.0026 - mae: 0.0368 - rmsle_cust: 0.0032 - val_loss: 0.0071 - val_mae: 0.0562 - val_rmsle_cust: 0.0057\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 386ms/step - loss: 0.0028 - mae: 0.0387 - rmsle_cust: 0.0032 - val_loss: 0.0070 - val_mae: 0.0559 - val_rmsle_cust: 0.0056\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 1s 389ms/step - loss: 0.0025 - mae: 0.0365 - rmsle_cust: 0.0030 - val_loss: 0.0070 - val_mae: 0.0560 - val_rmsle_cust: 0.0056\n",
      "11/11 [==============================] - 1s 9ms/step\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.1938 - mae: 0.3656 - rmsle_cust: 0.0185 - val_loss: 0.1297 - val_mae: 0.2861 - val_rmsle_cust: 0.0101\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 1s 350ms/step - loss: 0.1243 - mae: 0.2805 - rmsle_cust: 0.0098 - val_loss: 0.0803 - val_mae: 0.2178 - val_rmsle_cust: 0.0101\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.0782 - mae: 0.2201 - rmsle_cust: 0.0098 - val_loss: 0.0583 - val_mae: 0.1982 - val_rmsle_cust: 0.0101\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 385ms/step - loss: 0.0596 - mae: 0.2046 - rmsle_cust: 0.0098 - val_loss: 0.0630 - val_mae: 0.2077 - val_rmsle_cust: 0.0101\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 358ms/step - loss: 0.0639 - mae: 0.2135 - rmsle_cust: 0.0098 - val_loss: 0.0593 - val_mae: 0.1973 - val_rmsle_cust: 0.0101\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 500ms/step - loss: 0.0572 - mae: 0.1991 - rmsle_cust: 0.0098 - val_loss: 0.0450 - val_mae: 0.1700 - val_rmsle_cust: 0.0101\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 1s 385ms/step - loss: 0.0426 - mae: 0.1712 - rmsle_cust: 0.0098 - val_loss: 0.0360 - val_mae: 0.1477 - val_rmsle_cust: 0.0101\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 463ms/step - loss: 0.0333 - mae: 0.1465 - rmsle_cust: 0.0097 - val_loss: 0.0337 - val_mae: 0.1358 - val_rmsle_cust: 0.0099\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 532ms/step - loss: 0.0307 - mae: 0.1368 - rmsle_cust: 0.0087 - val_loss: 0.0322 - val_mae: 0.1332 - val_rmsle_cust: 0.0100\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.0289 - mae: 0.1326 - rmsle_cust: 0.0077 - val_loss: 0.0282 - val_mae: 0.1249 - val_rmsle_cust: 0.0101\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 506ms/step - loss: 0.0239 - mae: 0.1201 - rmsle_cust: 0.0077 - val_loss: 0.0236 - val_mae: 0.1100 - val_rmsle_cust: 0.0104\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.0189 - mae: 0.1034 - rmsle_cust: 0.0081 - val_loss: 0.0221 - val_mae: 0.1018 - val_rmsle_cust: 0.0108\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.0168 - mae: 0.0941 - rmsle_cust: 0.0087 - val_loss: 0.0238 - val_mae: 0.1030 - val_rmsle_cust: 0.0112\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 468ms/step - loss: 0.0177 - mae: 0.0957 - rmsle_cust: 0.0091 - val_loss: 0.0237 - val_mae: 0.1045 - val_rmsle_cust: 0.0117\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 384ms/step - loss: 0.0173 - mae: 0.0954 - rmsle_cust: 0.0098 - val_loss: 0.0208 - val_mae: 0.0998 - val_rmsle_cust: 0.0122\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 362ms/step - loss: 0.0152 - mae: 0.0902 - rmsle_cust: 0.0104 - val_loss: 0.0184 - val_mae: 0.0977 - val_rmsle_cust: 0.0123\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 447ms/step - loss: 0.0131 - mae: 0.0868 - rmsle_cust: 0.0108 - val_loss: 0.0172 - val_mae: 0.0972 - val_rmsle_cust: 0.0115\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 394ms/step - loss: 0.0122 - mae: 0.0848 - rmsle_cust: 0.0096 - val_loss: 0.0156 - val_mae: 0.0908 - val_rmsle_cust: 0.0101\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 459ms/step - loss: 0.0104 - mae: 0.0764 - rmsle_cust: 0.0077 - val_loss: 0.0139 - val_mae: 0.0819 - val_rmsle_cust: 0.0091\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 383ms/step - loss: 0.0088 - mae: 0.0675 - rmsle_cust: 0.0063 - val_loss: 0.0132 - val_mae: 0.0769 - val_rmsle_cust: 0.0086\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 356ms/step - loss: 0.0082 - mae: 0.0649 - rmsle_cust: 0.0063 - val_loss: 0.0130 - val_mae: 0.0769 - val_rmsle_cust: 0.0085\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 359ms/step - loss: 0.0080 - mae: 0.0652 - rmsle_cust: 0.0061 - val_loss: 0.0123 - val_mae: 0.0754 - val_rmsle_cust: 0.0082\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 362ms/step - loss: 0.0074 - mae: 0.0635 - rmsle_cust: 0.0061 - val_loss: 0.0113 - val_mae: 0.0732 - val_rmsle_cust: 0.0079\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.0068 - mae: 0.0603 - rmsle_cust: 0.0061 - val_loss: 0.0108 - val_mae: 0.0732 - val_rmsle_cust: 0.0076\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 1s 365ms/step - loss: 0.0066 - mae: 0.0590 - rmsle_cust: 0.0058 - val_loss: 0.0103 - val_mae: 0.0723 - val_rmsle_cust: 0.0074\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.0065 - mae: 0.0593 - rmsle_cust: 0.0059 - val_loss: 0.0097 - val_mae: 0.0692 - val_rmsle_cust: 0.0072\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 384ms/step - loss: 0.0058 - mae: 0.0561 - rmsle_cust: 0.0058 - val_loss: 0.0094 - val_mae: 0.0681 - val_rmsle_cust: 0.0071\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 354ms/step - loss: 0.0054 - mae: 0.0545 - rmsle_cust: 0.0057 - val_loss: 0.0093 - val_mae: 0.0679 - val_rmsle_cust: 0.0071\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.0054 - mae: 0.0539 - rmsle_cust: 0.0055 - val_loss: 0.0089 - val_mae: 0.0665 - val_rmsle_cust: 0.0071\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 372ms/step - loss: 0.0049 - mae: 0.0512 - rmsle_cust: 0.0055 - val_loss: 0.0086 - val_mae: 0.0654 - val_rmsle_cust: 0.0072\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 359ms/step - loss: 0.0048 - mae: 0.0511 - rmsle_cust: 0.0052 - val_loss: 0.0085 - val_mae: 0.0650 - val_rmsle_cust: 0.0073\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 1s 364ms/step - loss: 0.0045 - mae: 0.0495 - rmsle_cust: 0.0051 - val_loss: 0.0084 - val_mae: 0.0641 - val_rmsle_cust: 0.0072\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 351ms/step - loss: 0.0041 - mae: 0.0465 - rmsle_cust: 0.0046 - val_loss: 0.0083 - val_mae: 0.0632 - val_rmsle_cust: 0.0072\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 1s 357ms/step - loss: 0.0042 - mae: 0.0471 - rmsle_cust: 0.0043 - val_loss: 0.0083 - val_mae: 0.0630 - val_rmsle_cust: 0.0071\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.0040 - mae: 0.0458 - rmsle_cust: 0.0041 - val_loss: 0.0083 - val_mae: 0.0627 - val_rmsle_cust: 0.0071\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.0039 - mae: 0.0454 - rmsle_cust: 0.0041 - val_loss: 0.0082 - val_mae: 0.0623 - val_rmsle_cust: 0.0070\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 1s 382ms/step - loss: 0.0036 - mae: 0.0435 - rmsle_cust: 0.0039 - val_loss: 0.0081 - val_mae: 0.0621 - val_rmsle_cust: 0.0069\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 1s 470ms/step - loss: 0.0037 - mae: 0.0442 - rmsle_cust: 0.0038 - val_loss: 0.0080 - val_mae: 0.0618 - val_rmsle_cust: 0.0068\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 1s 608ms/step - loss: 0.0036 - mae: 0.0434 - rmsle_cust: 0.0038 - val_loss: 0.0078 - val_mae: 0.0612 - val_rmsle_cust: 0.0066\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 1s 477ms/step - loss: 0.0036 - mae: 0.0435 - rmsle_cust: 0.0038 - val_loss: 0.0077 - val_mae: 0.0607 - val_rmsle_cust: 0.0065\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.0032 - mae: 0.0411 - rmsle_cust: 0.0035 - val_loss: 0.0076 - val_mae: 0.0603 - val_rmsle_cust: 0.0064\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 1s 515ms/step - loss: 0.0032 - mae: 0.0410 - rmsle_cust: 0.0036 - val_loss: 0.0075 - val_mae: 0.0600 - val_rmsle_cust: 0.0063\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 2s 581ms/step - loss: 0.0029 - mae: 0.0396 - rmsle_cust: 0.0033 - val_loss: 0.0074 - val_mae: 0.0596 - val_rmsle_cust: 0.0063\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 490ms/step - loss: 0.0030 - mae: 0.0396 - rmsle_cust: 0.0032 - val_loss: 0.0073 - val_mae: 0.0593 - val_rmsle_cust: 0.0063\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 2s 543ms/step - loss: 0.0029 - mae: 0.0394 - rmsle_cust: 0.0032 - val_loss: 0.0073 - val_mae: 0.0592 - val_rmsle_cust: 0.0062\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 1s 371ms/step - loss: 0.0029 - mae: 0.0395 - rmsle_cust: 0.0033 - val_loss: 0.0073 - val_mae: 0.0589 - val_rmsle_cust: 0.0062\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 1s 390ms/step - loss: 0.0028 - mae: 0.0386 - rmsle_cust: 0.0032 - val_loss: 0.0072 - val_mae: 0.0586 - val_rmsle_cust: 0.0063\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 343ms/step - loss: 0.0027 - mae: 0.0385 - rmsle_cust: 0.0032 - val_loss: 0.0072 - val_mae: 0.0583 - val_rmsle_cust: 0.0063\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 359ms/step - loss: 0.0028 - mae: 0.0387 - rmsle_cust: 0.0030 - val_loss: 0.0071 - val_mae: 0.0581 - val_rmsle_cust: 0.0063\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 1s 374ms/step - loss: 0.0026 - mae: 0.0370 - rmsle_cust: 0.0029 - val_loss: 0.0071 - val_mae: 0.0578 - val_rmsle_cust: 0.0062\n",
      "11/11 [==============================] - 1s 9ms/step\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.1715 - mae: 0.3394 - rmsle_cust: 0.0110 - val_loss: 0.1225 - val_mae: 0.2785 - val_rmsle_cust: 0.0056\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 2s 579ms/step - loss: 0.1129 - mae: 0.2642 - rmsle_cust: 0.0103 - val_loss: 0.0756 - val_mae: 0.2105 - val_rmsle_cust: 0.0056\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 2s 588ms/step - loss: 0.0743 - mae: 0.2112 - rmsle_cust: 0.0103 - val_loss: 0.0517 - val_mae: 0.1856 - val_rmsle_cust: 0.0056\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 2s 583ms/step - loss: 0.0575 - mae: 0.1945 - rmsle_cust: 0.0103 - val_loss: 0.0492 - val_mae: 0.1930 - val_rmsle_cust: 0.0056\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 360ms/step - loss: 0.0577 - mae: 0.2023 - rmsle_cust: 0.0103 - val_loss: 0.0467 - val_mae: 0.1872 - val_rmsle_cust: 0.0056\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 540ms/step - loss: 0.0536 - mae: 0.1938 - rmsle_cust: 0.0103 - val_loss: 0.0361 - val_mae: 0.1613 - val_rmsle_cust: 0.0056\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 1s 364ms/step - loss: 0.0409 - mae: 0.1654 - rmsle_cust: 0.0103 - val_loss: 0.0281 - val_mae: 0.1368 - val_rmsle_cust: 0.0056\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 368ms/step - loss: 0.0315 - mae: 0.1383 - rmsle_cust: 0.0100 - val_loss: 0.0268 - val_mae: 0.1292 - val_rmsle_cust: 0.0055\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.0285 - mae: 0.1283 - rmsle_cust: 0.0087 - val_loss: 0.0266 - val_mae: 0.1300 - val_rmsle_cust: 0.0053\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.0272 - mae: 0.1278 - rmsle_cust: 0.0084 - val_loss: 0.0224 - val_mae: 0.1194 - val_rmsle_cust: 0.0054\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 2s 569ms/step - loss: 0.0229 - mae: 0.1175 - rmsle_cust: 0.0094 - val_loss: 0.0165 - val_mae: 0.0997 - val_rmsle_cust: 0.0053\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 2s 358ms/step - loss: 0.0183 - mae: 0.1012 - rmsle_cust: 0.0093 - val_loss: 0.0146 - val_mae: 0.0895 - val_rmsle_cust: 0.0050\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 585ms/step - loss: 0.0172 - mae: 0.0951 - rmsle_cust: 0.0088 - val_loss: 0.0153 - val_mae: 0.0923 - val_rmsle_cust: 0.0048\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 2s 575ms/step - loss: 0.0179 - mae: 0.0969 - rmsle_cust: 0.0085 - val_loss: 0.0137 - val_mae: 0.0870 - val_rmsle_cust: 0.0046\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 467ms/step - loss: 0.0155 - mae: 0.0896 - rmsle_cust: 0.0083 - val_loss: 0.0120 - val_mae: 0.0818 - val_rmsle_cust: 0.0046\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 449ms/step - loss: 0.0126 - mae: 0.0822 - rmsle_cust: 0.0085 - val_loss: 0.0124 - val_mae: 0.0877 - val_rmsle_cust: 0.0042\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.0121 - mae: 0.0835 - rmsle_cust: 0.0082 - val_loss: 0.0128 - val_mae: 0.0896 - val_rmsle_cust: 0.0039\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 559ms/step - loss: 0.0111 - mae: 0.0798 - rmsle_cust: 0.0073 - val_loss: 0.0114 - val_mae: 0.0828 - val_rmsle_cust: 0.0038\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 371ms/step - loss: 0.0099 - mae: 0.0732 - rmsle_cust: 0.0067 - val_loss: 0.0099 - val_mae: 0.0734 - val_rmsle_cust: 0.0038\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.0086 - mae: 0.0661 - rmsle_cust: 0.0063 - val_loss: 0.0094 - val_mae: 0.0694 - val_rmsle_cust: 0.0038\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.0085 - mae: 0.0666 - rmsle_cust: 0.0063 - val_loss: 0.0091 - val_mae: 0.0690 - val_rmsle_cust: 0.0039\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 365ms/step - loss: 0.0081 - mae: 0.0655 - rmsle_cust: 0.0063 - val_loss: 0.0088 - val_mae: 0.0681 - val_rmsle_cust: 0.0040\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.0072 - mae: 0.0614 - rmsle_cust: 0.0059 - val_loss: 0.0089 - val_mae: 0.0701 - val_rmsle_cust: 0.0041\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 364ms/step - loss: 0.0067 - mae: 0.0594 - rmsle_cust: 0.0058 - val_loss: 0.0091 - val_mae: 0.0717 - val_rmsle_cust: 0.0042\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 1s 358ms/step - loss: 0.0065 - mae: 0.0594 - rmsle_cust: 0.0060 - val_loss: 0.0085 - val_mae: 0.0685 - val_rmsle_cust: 0.0044\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 372ms/step - loss: 0.0060 - mae: 0.0560 - rmsle_cust: 0.0059 - val_loss: 0.0077 - val_mae: 0.0649 - val_rmsle_cust: 0.0043\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 418ms/step - loss: 0.0057 - mae: 0.0545 - rmsle_cust: 0.0059 - val_loss: 0.0074 - val_mae: 0.0636 - val_rmsle_cust: 0.0043\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 382ms/step - loss: 0.0055 - mae: 0.0539 - rmsle_cust: 0.0056 - val_loss: 0.0072 - val_mae: 0.0625 - val_rmsle_cust: 0.0042\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 596ms/step - loss: 0.0052 - mae: 0.0520 - rmsle_cust: 0.0054 - val_loss: 0.0071 - val_mae: 0.0617 - val_rmsle_cust: 0.0042\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 2s 493ms/step - loss: 0.0048 - mae: 0.0499 - rmsle_cust: 0.0051 - val_loss: 0.0072 - val_mae: 0.0615 - val_rmsle_cust: 0.0042\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 1s 657ms/step - loss: 0.0047 - mae: 0.0492 - rmsle_cust: 0.0051 - val_loss: 0.0071 - val_mae: 0.0607 - val_rmsle_cust: 0.0041\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.0045 - mae: 0.0486 - rmsle_cust: 0.0049 - val_loss: 0.0069 - val_mae: 0.0595 - val_rmsle_cust: 0.0041\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 360ms/step - loss: 0.0042 - mae: 0.0468 - rmsle_cust: 0.0046 - val_loss: 0.0067 - val_mae: 0.0586 - val_rmsle_cust: 0.0041\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 1s 344ms/step - loss: 0.0041 - mae: 0.0466 - rmsle_cust: 0.0044 - val_loss: 0.0067 - val_mae: 0.0580 - val_rmsle_cust: 0.0040\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 1s 514ms/step - loss: 0.0039 - mae: 0.0453 - rmsle_cust: 0.0042 - val_loss: 0.0067 - val_mae: 0.0581 - val_rmsle_cust: 0.0041\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 1s 369ms/step - loss: 0.0037 - mae: 0.0448 - rmsle_cust: 0.0042 - val_loss: 0.0069 - val_mae: 0.0587 - val_rmsle_cust: 0.0042\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 1s 364ms/step - loss: 0.0038 - mae: 0.0449 - rmsle_cust: 0.0041 - val_loss: 0.0068 - val_mae: 0.0585 - val_rmsle_cust: 0.0042\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 1s 408ms/step - loss: 0.0036 - mae: 0.0435 - rmsle_cust: 0.0039 - val_loss: 0.0067 - val_mae: 0.0574 - val_rmsle_cust: 0.0042\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 1s 336ms/step - loss: 0.0035 - mae: 0.0430 - rmsle_cust: 0.0037 - val_loss: 0.0066 - val_mae: 0.0571 - val_rmsle_cust: 0.0041\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 1s 388ms/step - loss: 0.0033 - mae: 0.0424 - rmsle_cust: 0.0035 - val_loss: 0.0066 - val_mae: 0.0571 - val_rmsle_cust: 0.0041\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 1s 350ms/step - loss: 0.0033 - mae: 0.0422 - rmsle_cust: 0.0036 - val_loss: 0.0067 - val_mae: 0.0573 - val_rmsle_cust: 0.0041\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 1s 369ms/step - loss: 0.0032 - mae: 0.0411 - rmsle_cust: 0.0036 - val_loss: 0.0067 - val_mae: 0.0576 - val_rmsle_cust: 0.0041\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 1s 515ms/step - loss: 0.0031 - mae: 0.0402 - rmsle_cust: 0.0035 - val_loss: 0.0066 - val_mae: 0.0571 - val_rmsle_cust: 0.0041\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 386ms/step - loss: 0.0030 - mae: 0.0394 - rmsle_cust: 0.0032 - val_loss: 0.0065 - val_mae: 0.0566 - val_rmsle_cust: 0.0041\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 1s 363ms/step - loss: 0.0029 - mae: 0.0393 - rmsle_cust: 0.0032 - val_loss: 0.0065 - val_mae: 0.0561 - val_rmsle_cust: 0.0041\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.0028 - mae: 0.0389 - rmsle_cust: 0.0032 - val_loss: 0.0065 - val_mae: 0.0559 - val_rmsle_cust: 0.0041\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 2s 549ms/step - loss: 0.0027 - mae: 0.0383 - rmsle_cust: 0.0031 - val_loss: 0.0065 - val_mae: 0.0560 - val_rmsle_cust: 0.0041\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 461ms/step - loss: 0.0027 - mae: 0.0377 - rmsle_cust: 0.0032 - val_loss: 0.0064 - val_mae: 0.0556 - val_rmsle_cust: 0.0041\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 563ms/step - loss: 0.0027 - mae: 0.0377 - rmsle_cust: 0.0030 - val_loss: 0.0064 - val_mae: 0.0554 - val_rmsle_cust: 0.0041\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 1s 390ms/step - loss: 0.0026 - mae: 0.0371 - rmsle_cust: 0.0029 - val_loss: 0.0064 - val_mae: 0.0553 - val_rmsle_cust: 0.0041\n",
      "11/11 [==============================] - 1s 8ms/step\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.1934 - mae: 0.3647 - rmsle_cust: 0.0177 - val_loss: 0.1445 - val_mae: 0.3058 - val_rmsle_cust: 0.0095\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 1s 359ms/step - loss: 0.1240 - mae: 0.2787 - rmsle_cust: 0.0099 - val_loss: 0.0868 - val_mae: 0.2328 - val_rmsle_cust: 0.0095\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 1s 361ms/step - loss: 0.0755 - mae: 0.2134 - rmsle_cust: 0.0099 - val_loss: 0.0619 - val_mae: 0.2054 - val_rmsle_cust: 0.0095\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 367ms/step - loss: 0.0619 - mae: 0.2058 - rmsle_cust: 0.0099 - val_loss: 0.0641 - val_mae: 0.2139 - val_rmsle_cust: 0.0095\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 514ms/step - loss: 0.0664 - mae: 0.2180 - rmsle_cust: 0.0099 - val_loss: 0.0521 - val_mae: 0.1902 - val_rmsle_cust: 0.0095\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.0531 - mae: 0.1918 - rmsle_cust: 0.0099 - val_loss: 0.0387 - val_mae: 0.1595 - val_rmsle_cust: 0.0095\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 1s 466ms/step - loss: 0.0383 - mae: 0.1572 - rmsle_cust: 0.0099 - val_loss: 0.0346 - val_mae: 0.1476 - val_rmsle_cust: 0.0095\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 408ms/step - loss: 0.0333 - mae: 0.1421 - rmsle_cust: 0.0098 - val_loss: 0.0340 - val_mae: 0.1474 - val_rmsle_cust: 0.0088\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 458ms/step - loss: 0.0320 - mae: 0.1393 - rmsle_cust: 0.0092 - val_loss: 0.0298 - val_mae: 0.1394 - val_rmsle_cust: 0.0090\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 2s 558ms/step - loss: 0.0279 - mae: 0.1299 - rmsle_cust: 0.0086 - val_loss: 0.0220 - val_mae: 0.1180 - val_rmsle_cust: 0.0093\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 368ms/step - loss: 0.0214 - mae: 0.1120 - rmsle_cust: 0.0085 - val_loss: 0.0166 - val_mae: 0.0993 - val_rmsle_cust: 0.0092\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.0177 - mae: 0.0979 - rmsle_cust: 0.0084 - val_loss: 0.0166 - val_mae: 0.0940 - val_rmsle_cust: 0.0090\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 472ms/step - loss: 0.0186 - mae: 0.0999 - rmsle_cust: 0.0083 - val_loss: 0.0170 - val_mae: 0.0959 - val_rmsle_cust: 0.0091\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.0183 - mae: 0.1001 - rmsle_cust: 0.0087 - val_loss: 0.0147 - val_mae: 0.0898 - val_rmsle_cust: 0.0099\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 385ms/step - loss: 0.0160 - mae: 0.0935 - rmsle_cust: 0.0097 - val_loss: 0.0134 - val_mae: 0.0886 - val_rmsle_cust: 0.0107\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 501ms/step - loss: 0.0141 - mae: 0.0899 - rmsle_cust: 0.0105 - val_loss: 0.0133 - val_mae: 0.0912 - val_rmsle_cust: 0.0102\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 345ms/step - loss: 0.0133 - mae: 0.0887 - rmsle_cust: 0.0100 - val_loss: 0.0120 - val_mae: 0.0854 - val_rmsle_cust: 0.0086\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.0116 - mae: 0.0804 - rmsle_cust: 0.0080 - val_loss: 0.0103 - val_mae: 0.0758 - val_rmsle_cust: 0.0073\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 392ms/step - loss: 0.0098 - mae: 0.0712 - rmsle_cust: 0.0067 - val_loss: 0.0098 - val_mae: 0.0709 - val_rmsle_cust: 0.0069\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 343ms/step - loss: 0.0092 - mae: 0.0682 - rmsle_cust: 0.0066 - val_loss: 0.0097 - val_mae: 0.0707 - val_rmsle_cust: 0.0067\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 334ms/step - loss: 0.0090 - mae: 0.0685 - rmsle_cust: 0.0067 - val_loss: 0.0092 - val_mae: 0.0693 - val_rmsle_cust: 0.0066\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.0081 - mae: 0.0648 - rmsle_cust: 0.0064 - val_loss: 0.0088 - val_mae: 0.0687 - val_rmsle_cust: 0.0063\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 357ms/step - loss: 0.0074 - mae: 0.0624 - rmsle_cust: 0.0061 - val_loss: 0.0088 - val_mae: 0.0696 - val_rmsle_cust: 0.0065\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 375ms/step - loss: 0.0072 - mae: 0.0619 - rmsle_cust: 0.0058 - val_loss: 0.0086 - val_mae: 0.0691 - val_rmsle_cust: 0.0067\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 1s 357ms/step - loss: 0.0064 - mae: 0.0588 - rmsle_cust: 0.0058 - val_loss: 0.0080 - val_mae: 0.0665 - val_rmsle_cust: 0.0069\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 364ms/step - loss: 0.0060 - mae: 0.0572 - rmsle_cust: 0.0059 - val_loss: 0.0076 - val_mae: 0.0647 - val_rmsle_cust: 0.0069\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 351ms/step - loss: 0.0057 - mae: 0.0555 - rmsle_cust: 0.0058 - val_loss: 0.0075 - val_mae: 0.0646 - val_rmsle_cust: 0.0070\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 373ms/step - loss: 0.0055 - mae: 0.0548 - rmsle_cust: 0.0060 - val_loss: 0.0073 - val_mae: 0.0635 - val_rmsle_cust: 0.0070\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 362ms/step - loss: 0.0051 - mae: 0.0533 - rmsle_cust: 0.0059 - val_loss: 0.0073 - val_mae: 0.0636 - val_rmsle_cust: 0.0070\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 447ms/step - loss: 0.0048 - mae: 0.0508 - rmsle_cust: 0.0057 - val_loss: 0.0072 - val_mae: 0.0633 - val_rmsle_cust: 0.0067\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 1s 370ms/step - loss: 0.0047 - mae: 0.0506 - rmsle_cust: 0.0054 - val_loss: 0.0070 - val_mae: 0.0616 - val_rmsle_cust: 0.0062\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 1s 376ms/step - loss: 0.0043 - mae: 0.0482 - rmsle_cust: 0.0048 - val_loss: 0.0068 - val_mae: 0.0601 - val_rmsle_cust: 0.0059\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 343ms/step - loss: 0.0043 - mae: 0.0483 - rmsle_cust: 0.0047 - val_loss: 0.0067 - val_mae: 0.0593 - val_rmsle_cust: 0.0057\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 1s 353ms/step - loss: 0.0041 - mae: 0.0464 - rmsle_cust: 0.0043 - val_loss: 0.0067 - val_mae: 0.0594 - val_rmsle_cust: 0.0058\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 1s 372ms/step - loss: 0.0038 - mae: 0.0446 - rmsle_cust: 0.0043 - val_loss: 0.0067 - val_mae: 0.0593 - val_rmsle_cust: 0.0058\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 1s 462ms/step - loss: 0.0039 - mae: 0.0453 - rmsle_cust: 0.0041 - val_loss: 0.0066 - val_mae: 0.0589 - val_rmsle_cust: 0.0058\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 1s 509ms/step - loss: 0.0035 - mae: 0.0432 - rmsle_cust: 0.0041 - val_loss: 0.0065 - val_mae: 0.0580 - val_rmsle_cust: 0.0058\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 2s 440ms/step - loss: 0.0035 - mae: 0.0432 - rmsle_cust: 0.0038 - val_loss: 0.0064 - val_mae: 0.0574 - val_rmsle_cust: 0.0058\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 2s 714ms/step - loss: 0.0034 - mae: 0.0424 - rmsle_cust: 0.0038 - val_loss: 0.0064 - val_mae: 0.0572 - val_rmsle_cust: 0.0058\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 1s 636ms/step - loss: 0.0033 - mae: 0.0423 - rmsle_cust: 0.0038 - val_loss: 0.0064 - val_mae: 0.0574 - val_rmsle_cust: 0.0058\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 2s 518ms/step - loss: 0.0030 - mae: 0.0409 - rmsle_cust: 0.0037 - val_loss: 0.0064 - val_mae: 0.0574 - val_rmsle_cust: 0.0058\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 2s 613ms/step - loss: 0.0030 - mae: 0.0400 - rmsle_cust: 0.0036 - val_loss: 0.0064 - val_mae: 0.0569 - val_rmsle_cust: 0.0058\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 1s 542ms/step - loss: 0.0029 - mae: 0.0396 - rmsle_cust: 0.0034 - val_loss: 0.0064 - val_mae: 0.0566 - val_rmsle_cust: 0.0059\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 2s 647ms/step - loss: 0.0028 - mae: 0.0386 - rmsle_cust: 0.0033 - val_loss: 0.0064 - val_mae: 0.0565 - val_rmsle_cust: 0.0059\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 2s 534ms/step - loss: 0.0029 - mae: 0.0393 - rmsle_cust: 0.0033 - val_loss: 0.0064 - val_mae: 0.0564 - val_rmsle_cust: 0.0060\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 2s 571ms/step - loss: 0.0028 - mae: 0.0380 - rmsle_cust: 0.0033 - val_loss: 0.0064 - val_mae: 0.0564 - val_rmsle_cust: 0.0060\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 1s 372ms/step - loss: 0.0027 - mae: 0.0378 - rmsle_cust: 0.0031 - val_loss: 0.0064 - val_mae: 0.0565 - val_rmsle_cust: 0.0060\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 373ms/step - loss: 0.0025 - mae: 0.0365 - rmsle_cust: 0.0030 - val_loss: 0.0064 - val_mae: 0.0563 - val_rmsle_cust: 0.0060\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 516ms/step - loss: 0.0026 - mae: 0.0365 - rmsle_cust: 0.0032 - val_loss: 0.0064 - val_mae: 0.0565 - val_rmsle_cust: 0.0060\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 2s 671ms/step - loss: 0.0025 - mae: 0.0359 - rmsle_cust: 0.0030 - val_loss: 0.0065 - val_mae: 0.0567 - val_rmsle_cust: 0.0060\n",
      "11/11 [==============================] - 1s 9ms/step\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 6s 2s/step - loss: 0.2043 - mae: 0.3787 - rmsle_cust: 0.0234 - val_loss: 0.1348 - val_mae: 0.2938 - val_rmsle_cust: 0.0099\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 2s 774ms/step - loss: 0.1389 - mae: 0.2996 - rmsle_cust: 0.0099 - val_loss: 0.0899 - val_mae: 0.2359 - val_rmsle_cust: 0.0099\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 2s 858ms/step - loss: 0.0928 - mae: 0.2365 - rmsle_cust: 0.0099 - val_loss: 0.0643 - val_mae: 0.2035 - val_rmsle_cust: 0.0099\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 2s 755ms/step - loss: 0.0671 - mae: 0.2044 - rmsle_cust: 0.0099 - val_loss: 0.0636 - val_mae: 0.2076 - val_rmsle_cust: 0.0099\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 3s 959ms/step - loss: 0.0650 - mae: 0.2121 - rmsle_cust: 0.0099 - val_loss: 0.0675 - val_mae: 0.2177 - val_rmsle_cust: 0.0099\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 2s 742ms/step - loss: 0.0663 - mae: 0.2151 - rmsle_cust: 0.0099 - val_loss: 0.0568 - val_mae: 0.1972 - val_rmsle_cust: 0.0099\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 2s 699ms/step - loss: 0.0561 - mae: 0.1938 - rmsle_cust: 0.0099 - val_loss: 0.0431 - val_mae: 0.1653 - val_rmsle_cust: 0.0099\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 2s 778ms/step - loss: 0.0438 - mae: 0.1647 - rmsle_cust: 0.0099 - val_loss: 0.0360 - val_mae: 0.1466 - val_rmsle_cust: 0.0099\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 2s 710ms/step - loss: 0.0384 - mae: 0.1499 - rmsle_cust: 0.0099 - val_loss: 0.0337 - val_mae: 0.1438 - val_rmsle_cust: 0.0099\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 2s 833ms/step - loss: 0.0365 - mae: 0.1482 - rmsle_cust: 0.0098 - val_loss: 0.0307 - val_mae: 0.1384 - val_rmsle_cust: 0.0099\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 2s 735ms/step - loss: 0.0335 - mae: 0.1430 - rmsle_cust: 0.0098 - val_loss: 0.0254 - val_mae: 0.1251 - val_rmsle_cust: 0.0101\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 2s 896ms/step - loss: 0.0272 - mae: 0.1270 - rmsle_cust: 0.0098 - val_loss: 0.0208 - val_mae: 0.1089 - val_rmsle_cust: 0.0103\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 2s 811ms/step - loss: 0.0215 - mae: 0.1086 - rmsle_cust: 0.0099 - val_loss: 0.0209 - val_mae: 0.1085 - val_rmsle_cust: 0.0103\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 2s 705ms/step - loss: 0.0206 - mae: 0.1062 - rmsle_cust: 0.0095 - val_loss: 0.0229 - val_mae: 0.1174 - val_rmsle_cust: 0.0101\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 2s 810ms/step - loss: 0.0216 - mae: 0.1099 - rmsle_cust: 0.0093 - val_loss: 0.0210 - val_mae: 0.1116 - val_rmsle_cust: 0.0100\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 2s 502ms/step - loss: 0.0193 - mae: 0.1041 - rmsle_cust: 0.0098 - val_loss: 0.0176 - val_mae: 0.0982 - val_rmsle_cust: 0.0101\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 2s 785ms/step - loss: 0.0162 - mae: 0.0945 - rmsle_cust: 0.0108 - val_loss: 0.0161 - val_mae: 0.0949 - val_rmsle_cust: 0.0099\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 2s 861ms/step - loss: 0.0148 - mae: 0.0912 - rmsle_cust: 0.0106 - val_loss: 0.0149 - val_mae: 0.0922 - val_rmsle_cust: 0.0088\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 801ms/step - loss: 0.0134 - mae: 0.0869 - rmsle_cust: 0.0089 - val_loss: 0.0133 - val_mae: 0.0854 - val_rmsle_cust: 0.0078\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 2s 674ms/step - loss: 0.0117 - mae: 0.0783 - rmsle_cust: 0.0075 - val_loss: 0.0127 - val_mae: 0.0818 - val_rmsle_cust: 0.0076\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 2s 904ms/step - loss: 0.0103 - mae: 0.0710 - rmsle_cust: 0.0068 - val_loss: 0.0128 - val_mae: 0.0826 - val_rmsle_cust: 0.0076\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 2s 860ms/step - loss: 0.0097 - mae: 0.0693 - rmsle_cust: 0.0066 - val_loss: 0.0124 - val_mae: 0.0813 - val_rmsle_cust: 0.0077\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 2s 810ms/step - loss: 0.0094 - mae: 0.0686 - rmsle_cust: 0.0065 - val_loss: 0.0111 - val_mae: 0.0761 - val_rmsle_cust: 0.0076\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 2s 723ms/step - loss: 0.0085 - mae: 0.0651 - rmsle_cust: 0.0064 - val_loss: 0.0101 - val_mae: 0.0719 - val_rmsle_cust: 0.0073\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 2s 759ms/step - loss: 0.0076 - mae: 0.0623 - rmsle_cust: 0.0061 - val_loss: 0.0096 - val_mae: 0.0698 - val_rmsle_cust: 0.0071\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 2s 687ms/step - loss: 0.0074 - mae: 0.0624 - rmsle_cust: 0.0065 - val_loss: 0.0091 - val_mae: 0.0673 - val_rmsle_cust: 0.0073\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 2s 814ms/step - loss: 0.0069 - mae: 0.0604 - rmsle_cust: 0.0065 - val_loss: 0.0088 - val_mae: 0.0658 - val_rmsle_cust: 0.0074\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 2s 949ms/step - loss: 0.0064 - mae: 0.0587 - rmsle_cust: 0.0063 - val_loss: 0.0089 - val_mae: 0.0675 - val_rmsle_cust: 0.0075\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 2s 729ms/step - loss: 0.0062 - mae: 0.0584 - rmsle_cust: 0.0060 - val_loss: 0.0086 - val_mae: 0.0660 - val_rmsle_cust: 0.0074\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 2s 778ms/step - loss: 0.0059 - mae: 0.0563 - rmsle_cust: 0.0061 - val_loss: 0.0082 - val_mae: 0.0635 - val_rmsle_cust: 0.0074\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 2s 727ms/step - loss: 0.0054 - mae: 0.0535 - rmsle_cust: 0.0058 - val_loss: 0.0080 - val_mae: 0.0630 - val_rmsle_cust: 0.0074\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 2s 674ms/step - loss: 0.0052 - mae: 0.0526 - rmsle_cust: 0.0055 - val_loss: 0.0080 - val_mae: 0.0632 - val_rmsle_cust: 0.0074\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 2s 564ms/step - loss: 0.0049 - mae: 0.0510 - rmsle_cust: 0.0053 - val_loss: 0.0079 - val_mae: 0.0635 - val_rmsle_cust: 0.0075\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 2s 761ms/step - loss: 0.0048 - mae: 0.0498 - rmsle_cust: 0.0049 - val_loss: 0.0080 - val_mae: 0.0639 - val_rmsle_cust: 0.0076\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 2s 727ms/step - loss: 0.0046 - mae: 0.0493 - rmsle_cust: 0.0048 - val_loss: 0.0078 - val_mae: 0.0634 - val_rmsle_cust: 0.0075\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 2s 691ms/step - loss: 0.0045 - mae: 0.0483 - rmsle_cust: 0.0046 - val_loss: 0.0075 - val_mae: 0.0619 - val_rmsle_cust: 0.0074\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 2s 714ms/step - loss: 0.0043 - mae: 0.0473 - rmsle_cust: 0.0045 - val_loss: 0.0073 - val_mae: 0.0607 - val_rmsle_cust: 0.0073\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 2s 696ms/step - loss: 0.0042 - mae: 0.0467 - rmsle_cust: 0.0045 - val_loss: 0.0072 - val_mae: 0.0600 - val_rmsle_cust: 0.0072\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 2s 609ms/step - loss: 0.0039 - mae: 0.0454 - rmsle_cust: 0.0044 - val_loss: 0.0071 - val_mae: 0.0600 - val_rmsle_cust: 0.0072\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 2s 767ms/step - loss: 0.0039 - mae: 0.0452 - rmsle_cust: 0.0043 - val_loss: 0.0071 - val_mae: 0.0602 - val_rmsle_cust: 0.0071\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 2s 679ms/step - loss: 0.0036 - mae: 0.0440 - rmsle_cust: 0.0039 - val_loss: 0.0071 - val_mae: 0.0598 - val_rmsle_cust: 0.0071\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 2s 716ms/step - loss: 0.0036 - mae: 0.0432 - rmsle_cust: 0.0040 - val_loss: 0.0070 - val_mae: 0.0591 - val_rmsle_cust: 0.0071\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 2s 826ms/step - loss: 0.0033 - mae: 0.0422 - rmsle_cust: 0.0037 - val_loss: 0.0070 - val_mae: 0.0588 - val_rmsle_cust: 0.0071\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 2s 826ms/step - loss: 0.0035 - mae: 0.0427 - rmsle_cust: 0.0035 - val_loss: 0.0070 - val_mae: 0.0588 - val_rmsle_cust: 0.0071\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 2s 757ms/step - loss: 0.0033 - mae: 0.0416 - rmsle_cust: 0.0036 - val_loss: 0.0070 - val_mae: 0.0590 - val_rmsle_cust: 0.0071\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 2s 824ms/step - loss: 0.0031 - mae: 0.0409 - rmsle_cust: 0.0036 - val_loss: 0.0070 - val_mae: 0.0590 - val_rmsle_cust: 0.0071\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 2s 647ms/step - loss: 0.0029 - mae: 0.0393 - rmsle_cust: 0.0034 - val_loss: 0.0069 - val_mae: 0.0585 - val_rmsle_cust: 0.0071\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 2s 762ms/step - loss: 0.0029 - mae: 0.0398 - rmsle_cust: 0.0035 - val_loss: 0.0068 - val_mae: 0.0579 - val_rmsle_cust: 0.0071\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 2s 817ms/step - loss: 0.0030 - mae: 0.0396 - rmsle_cust: 0.0034 - val_loss: 0.0067 - val_mae: 0.0576 - val_rmsle_cust: 0.0070\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 2s 896ms/step - loss: 0.0029 - mae: 0.0389 - rmsle_cust: 0.0034 - val_loss: 0.0067 - val_mae: 0.0573 - val_rmsle_cust: 0.0070\n",
      "11/11 [==============================] - 1s 12ms/step\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 6s 2s/step - loss: 0.2046 - mae: 0.3804 - rmsle_cust: 0.0272 - val_loss: 0.1310 - val_mae: 0.2871 - val_rmsle_cust: 0.0104\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 2s 727ms/step - loss: 0.1225 - mae: 0.2785 - rmsle_cust: 0.0098 - val_loss: 0.0783 - val_mae: 0.2149 - val_rmsle_cust: 0.0104\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 2s 823ms/step - loss: 0.0733 - mae: 0.2117 - rmsle_cust: 0.0098 - val_loss: 0.0682 - val_mae: 0.2157 - val_rmsle_cust: 0.0104\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 2s 919ms/step - loss: 0.0673 - mae: 0.2166 - rmsle_cust: 0.0098 - val_loss: 0.0741 - val_mae: 0.2294 - val_rmsle_cust: 0.0104\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 2s 663ms/step - loss: 0.0700 - mae: 0.2242 - rmsle_cust: 0.0098 - val_loss: 0.0591 - val_mae: 0.2026 - val_rmsle_cust: 0.0104\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 2s 737ms/step - loss: 0.0546 - mae: 0.1954 - rmsle_cust: 0.0098 - val_loss: 0.0439 - val_mae: 0.1696 - val_rmsle_cust: 0.0104\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 2s 774ms/step - loss: 0.0395 - mae: 0.1620 - rmsle_cust: 0.0098 - val_loss: 0.0384 - val_mae: 0.1507 - val_rmsle_cust: 0.0104\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 2s 774ms/step - loss: 0.0341 - mae: 0.1459 - rmsle_cust: 0.0098 - val_loss: 0.0366 - val_mae: 0.1441 - val_rmsle_cust: 0.0102\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 2s 727ms/step - loss: 0.0323 - mae: 0.1409 - rmsle_cust: 0.0097 - val_loss: 0.0319 - val_mae: 0.1342 - val_rmsle_cust: 0.0100\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 2s 645ms/step - loss: 0.0268 - mae: 0.1281 - rmsle_cust: 0.0093 - val_loss: 0.0244 - val_mae: 0.1138 - val_rmsle_cust: 0.0096\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 2s 817ms/step - loss: 0.0200 - mae: 0.1080 - rmsle_cust: 0.0089 - val_loss: 0.0193 - val_mae: 0.0972 - val_rmsle_cust: 0.0092\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 2s 869ms/step - loss: 0.0160 - mae: 0.0944 - rmsle_cust: 0.0087 - val_loss: 0.0191 - val_mae: 0.0976 - val_rmsle_cust: 0.0089\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 2s 765ms/step - loss: 0.0168 - mae: 0.0969 - rmsle_cust: 0.0086 - val_loss: 0.0192 - val_mae: 0.0983 - val_rmsle_cust: 0.0088\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 2s 785ms/step - loss: 0.0170 - mae: 0.0977 - rmsle_cust: 0.0089 - val_loss: 0.0171 - val_mae: 0.0930 - val_rmsle_cust: 0.0089\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 731ms/step - loss: 0.0145 - mae: 0.0899 - rmsle_cust: 0.0094 - val_loss: 0.0162 - val_mae: 0.0912 - val_rmsle_cust: 0.0091\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 2s 686ms/step - loss: 0.0129 - mae: 0.0872 - rmsle_cust: 0.0102 - val_loss: 0.0164 - val_mae: 0.0937 - val_rmsle_cust: 0.0086\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 2s 465ms/step - loss: 0.0124 - mae: 0.0867 - rmsle_cust: 0.0098 - val_loss: 0.0154 - val_mae: 0.0895 - val_rmsle_cust: 0.0076\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 2s 696ms/step - loss: 0.0107 - mae: 0.0792 - rmsle_cust: 0.0084 - val_loss: 0.0135 - val_mae: 0.0799 - val_rmsle_cust: 0.0070\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 2s 867ms/step - loss: 0.0089 - mae: 0.0695 - rmsle_cust: 0.0068 - val_loss: 0.0125 - val_mae: 0.0740 - val_rmsle_cust: 0.0070\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 2s 762ms/step - loss: 0.0079 - mae: 0.0639 - rmsle_cust: 0.0060 - val_loss: 0.0125 - val_mae: 0.0746 - val_rmsle_cust: 0.0070\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 2s 759ms/step - loss: 0.0080 - mae: 0.0646 - rmsle_cust: 0.0059 - val_loss: 0.0124 - val_mae: 0.0749 - val_rmsle_cust: 0.0069\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 2s 702ms/step - loss: 0.0074 - mae: 0.0629 - rmsle_cust: 0.0057 - val_loss: 0.0121 - val_mae: 0.0735 - val_rmsle_cust: 0.0064\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 2s 666ms/step - loss: 0.0068 - mae: 0.0600 - rmsle_cust: 0.0055 - val_loss: 0.0121 - val_mae: 0.0736 - val_rmsle_cust: 0.0058\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 2s 670ms/step - loss: 0.0065 - mae: 0.0587 - rmsle_cust: 0.0053 - val_loss: 0.0121 - val_mae: 0.0736 - val_rmsle_cust: 0.0053\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 2s 545ms/step - loss: 0.0061 - mae: 0.0571 - rmsle_cust: 0.0053 - val_loss: 0.0117 - val_mae: 0.0714 - val_rmsle_cust: 0.0049\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 2s 780ms/step - loss: 0.0057 - mae: 0.0555 - rmsle_cust: 0.0053 - val_loss: 0.0110 - val_mae: 0.0678 - val_rmsle_cust: 0.0048\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 2s 725ms/step - loss: 0.0054 - mae: 0.0533 - rmsle_cust: 0.0054 - val_loss: 0.0106 - val_mae: 0.0659 - val_rmsle_cust: 0.0047\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 2s 655ms/step - loss: 0.0051 - mae: 0.0527 - rmsle_cust: 0.0055 - val_loss: 0.0104 - val_mae: 0.0651 - val_rmsle_cust: 0.0046\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 2s 758ms/step - loss: 0.0050 - mae: 0.0513 - rmsle_cust: 0.0054 - val_loss: 0.0102 - val_mae: 0.0643 - val_rmsle_cust: 0.0044\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 2s 677ms/step - loss: 0.0047 - mae: 0.0505 - rmsle_cust: 0.0056 - val_loss: 0.0103 - val_mae: 0.0649 - val_rmsle_cust: 0.0043\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 2s 439ms/step - loss: 0.0045 - mae: 0.0495 - rmsle_cust: 0.0056 - val_loss: 0.0103 - val_mae: 0.0657 - val_rmsle_cust: 0.0043\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 2s 632ms/step - loss: 0.0044 - mae: 0.0483 - rmsle_cust: 0.0052 - val_loss: 0.0101 - val_mae: 0.0649 - val_rmsle_cust: 0.0044\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 505ms/step - loss: 0.0042 - mae: 0.0471 - rmsle_cust: 0.0047 - val_loss: 0.0098 - val_mae: 0.0637 - val_rmsle_cust: 0.0046\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 2s 859ms/step - loss: 0.0042 - mae: 0.0470 - rmsle_cust: 0.0044 - val_loss: 0.0097 - val_mae: 0.0632 - val_rmsle_cust: 0.0048\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 3s 990ms/step - loss: 0.0040 - mae: 0.0464 - rmsle_cust: 0.0043 - val_loss: 0.0096 - val_mae: 0.0630 - val_rmsle_cust: 0.0048\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0038 - mae: 0.0452 - rmsle_cust: 0.0041 - val_loss: 0.0096 - val_mae: 0.0626 - val_rmsle_cust: 0.0047\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 3s 873ms/step - loss: 0.0036 - mae: 0.0440 - rmsle_cust: 0.0041 - val_loss: 0.0096 - val_mae: 0.0628 - val_rmsle_cust: 0.0045\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 3s 829ms/step - loss: 0.0036 - mae: 0.0435 - rmsle_cust: 0.0041 - val_loss: 0.0096 - val_mae: 0.0629 - val_rmsle_cust: 0.0043\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 3s 908ms/step - loss: 0.0035 - mae: 0.0432 - rmsle_cust: 0.0040 - val_loss: 0.0095 - val_mae: 0.0619 - val_rmsle_cust: 0.0043\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 2s 768ms/step - loss: 0.0034 - mae: 0.0425 - rmsle_cust: 0.0041 - val_loss: 0.0093 - val_mae: 0.0609 - val_rmsle_cust: 0.0043\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 2s 743ms/step - loss: 0.0032 - mae: 0.0414 - rmsle_cust: 0.0038 - val_loss: 0.0092 - val_mae: 0.0608 - val_rmsle_cust: 0.0043\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 2s 915ms/step - loss: 0.0033 - mae: 0.0420 - rmsle_cust: 0.0039 - val_loss: 0.0092 - val_mae: 0.0606 - val_rmsle_cust: 0.0043\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 2s 787ms/step - loss: 0.0030 - mae: 0.0398 - rmsle_cust: 0.0036 - val_loss: 0.0092 - val_mae: 0.0610 - val_rmsle_cust: 0.0043\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 2s 622ms/step - loss: 0.0030 - mae: 0.0399 - rmsle_cust: 0.0036 - val_loss: 0.0092 - val_mae: 0.0616 - val_rmsle_cust: 0.0043\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 2s 782ms/step - loss: 0.0030 - mae: 0.0400 - rmsle_cust: 0.0035 - val_loss: 0.0091 - val_mae: 0.0611 - val_rmsle_cust: 0.0044\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 2s 786ms/step - loss: 0.0029 - mae: 0.0392 - rmsle_cust: 0.0034 - val_loss: 0.0090 - val_mae: 0.0608 - val_rmsle_cust: 0.0045\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 2s 586ms/step - loss: 0.0028 - mae: 0.0387 - rmsle_cust: 0.0033 - val_loss: 0.0089 - val_mae: 0.0607 - val_rmsle_cust: 0.0046\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 2s 725ms/step - loss: 0.0029 - mae: 0.0388 - rmsle_cust: 0.0032 - val_loss: 0.0090 - val_mae: 0.0608 - val_rmsle_cust: 0.0046\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 2s 799ms/step - loss: 0.0029 - mae: 0.0388 - rmsle_cust: 0.0034 - val_loss: 0.0090 - val_mae: 0.0612 - val_rmsle_cust: 0.0046\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 2s 747ms/step - loss: 0.0027 - mae: 0.0374 - rmsle_cust: 0.0030 - val_loss: 0.0090 - val_mae: 0.0610 - val_rmsle_cust: 0.0045\n",
      "11/11 [==============================] - 1s 15ms/step\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1833 - mae: 0.3541 - rmsle_cust: 0.0137 - val_loss: 0.1242 - val_mae: 0.2850 - val_rmsle_cust: 0.0117\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 2s 730ms/step - loss: 0.1193 - mae: 0.2732 - rmsle_cust: 0.0097 - val_loss: 0.0808 - val_mae: 0.2264 - val_rmsle_cust: 0.0117\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 2s 719ms/step - loss: 0.0764 - mae: 0.2158 - rmsle_cust: 0.0097 - val_loss: 0.0636 - val_mae: 0.2057 - val_rmsle_cust: 0.0117\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 2s 913ms/step - loss: 0.0595 - mae: 0.1999 - rmsle_cust: 0.0097 - val_loss: 0.0672 - val_mae: 0.2167 - val_rmsle_cust: 0.0117\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 2s 798ms/step - loss: 0.0620 - mae: 0.2095 - rmsle_cust: 0.0097 - val_loss: 0.0606 - val_mae: 0.2045 - val_rmsle_cust: 0.0117\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 2s 730ms/step - loss: 0.0547 - mae: 0.1948 - rmsle_cust: 0.0097 - val_loss: 0.0468 - val_mae: 0.1769 - val_rmsle_cust: 0.0117\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 2s 705ms/step - loss: 0.0416 - mae: 0.1666 - rmsle_cust: 0.0097 - val_loss: 0.0381 - val_mae: 0.1552 - val_rmsle_cust: 0.0117\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 2s 834ms/step - loss: 0.0343 - mae: 0.1465 - rmsle_cust: 0.0097 - val_loss: 0.0354 - val_mae: 0.1473 - val_rmsle_cust: 0.0116\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0319 - mae: 0.1390 - rmsle_cust: 0.0095 - val_loss: 0.0327 - val_mae: 0.1421 - val_rmsle_cust: 0.0109\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0287 - mae: 0.1325 - rmsle_cust: 0.0090 - val_loss: 0.0274 - val_mae: 0.1281 - val_rmsle_cust: 0.0099\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 783ms/step - loss: 0.0235 - mae: 0.1184 - rmsle_cust: 0.0088 - val_loss: 0.0233 - val_mae: 0.1110 - val_rmsle_cust: 0.0093\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 2s 693ms/step - loss: 0.0191 - mae: 0.1018 - rmsle_cust: 0.0087 - val_loss: 0.0228 - val_mae: 0.1062 - val_rmsle_cust: 0.0090\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 2s 764ms/step - loss: 0.0183 - mae: 0.0968 - rmsle_cust: 0.0087 - val_loss: 0.0230 - val_mae: 0.1054 - val_rmsle_cust: 0.0090\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 2s 724ms/step - loss: 0.0179 - mae: 0.0954 - rmsle_cust: 0.0088 - val_loss: 0.0211 - val_mae: 0.1007 - val_rmsle_cust: 0.0096\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 2s 815ms/step - loss: 0.0158 - mae: 0.0908 - rmsle_cust: 0.0099 - val_loss: 0.0195 - val_mae: 0.0999 - val_rmsle_cust: 0.0116\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 2s 811ms/step - loss: 0.0145 - mae: 0.0903 - rmsle_cust: 0.0111 - val_loss: 0.0187 - val_mae: 0.1010 - val_rmsle_cust: 0.0118\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 2s 636ms/step - loss: 0.0136 - mae: 0.0894 - rmsle_cust: 0.0108 - val_loss: 0.0170 - val_mae: 0.0949 - val_rmsle_cust: 0.0098\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 2s 615ms/step - loss: 0.0118 - mae: 0.0811 - rmsle_cust: 0.0089 - val_loss: 0.0156 - val_mae: 0.0875 - val_rmsle_cust: 0.0080\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 2s 810ms/step - loss: 0.0101 - mae: 0.0722 - rmsle_cust: 0.0072 - val_loss: 0.0152 - val_mae: 0.0853 - val_rmsle_cust: 0.0077\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 2s 594ms/step - loss: 0.0094 - mae: 0.0685 - rmsle_cust: 0.0064 - val_loss: 0.0147 - val_mae: 0.0842 - val_rmsle_cust: 0.0078\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 2s 772ms/step - loss: 0.0089 - mae: 0.0670 - rmsle_cust: 0.0062 - val_loss: 0.0138 - val_mae: 0.0823 - val_rmsle_cust: 0.0077\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 2s 747ms/step - loss: 0.0081 - mae: 0.0640 - rmsle_cust: 0.0061 - val_loss: 0.0132 - val_mae: 0.0818 - val_rmsle_cust: 0.0077\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 2s 622ms/step - loss: 0.0075 - mae: 0.0626 - rmsle_cust: 0.0057 - val_loss: 0.0127 - val_mae: 0.0811 - val_rmsle_cust: 0.0078\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 2s 750ms/step - loss: 0.0070 - mae: 0.0610 - rmsle_cust: 0.0056 - val_loss: 0.0122 - val_mae: 0.0794 - val_rmsle_cust: 0.0079\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 2s 846ms/step - loss: 0.0064 - mae: 0.0580 - rmsle_cust: 0.0057 - val_loss: 0.0120 - val_mae: 0.0789 - val_rmsle_cust: 0.0080\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 2s 696ms/step - loss: 0.0062 - mae: 0.0576 - rmsle_cust: 0.0057 - val_loss: 0.0119 - val_mae: 0.0788 - val_rmsle_cust: 0.0080\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.0060 - mae: 0.0564 - rmsle_cust: 0.0054 - val_loss: 0.0115 - val_mae: 0.0770 - val_rmsle_cust: 0.0081\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 446ms/step - loss: 0.0057 - mae: 0.0554 - rmsle_cust: 0.0056 - val_loss: 0.0112 - val_mae: 0.0760 - val_rmsle_cust: 0.0081\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 544ms/step - loss: 0.0053 - mae: 0.0531 - rmsle_cust: 0.0056 - val_loss: 0.0110 - val_mae: 0.0753 - val_rmsle_cust: 0.0080\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 2s 743ms/step - loss: 0.0051 - mae: 0.0518 - rmsle_cust: 0.0055 - val_loss: 0.0108 - val_mae: 0.0736 - val_rmsle_cust: 0.0079\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 2s 702ms/step - loss: 0.0047 - mae: 0.0493 - rmsle_cust: 0.0049 - val_loss: 0.0108 - val_mae: 0.0728 - val_rmsle_cust: 0.0078\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 2s 696ms/step - loss: 0.0046 - mae: 0.0483 - rmsle_cust: 0.0048 - val_loss: 0.0108 - val_mae: 0.0725 - val_rmsle_cust: 0.0077\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 2s 639ms/step - loss: 0.0044 - mae: 0.0473 - rmsle_cust: 0.0045 - val_loss: 0.0106 - val_mae: 0.0723 - val_rmsle_cust: 0.0077\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 2s 800ms/step - loss: 0.0041 - mae: 0.0459 - rmsle_cust: 0.0042 - val_loss: 0.0106 - val_mae: 0.0727 - val_rmsle_cust: 0.0077\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 2s 653ms/step - loss: 0.0041 - mae: 0.0455 - rmsle_cust: 0.0043 - val_loss: 0.0105 - val_mae: 0.0723 - val_rmsle_cust: 0.0076\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 2s 767ms/step - loss: 0.0040 - mae: 0.0456 - rmsle_cust: 0.0041 - val_loss: 0.0104 - val_mae: 0.0714 - val_rmsle_cust: 0.0076\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 2s 758ms/step - loss: 0.0038 - mae: 0.0442 - rmsle_cust: 0.0040 - val_loss: 0.0104 - val_mae: 0.0713 - val_rmsle_cust: 0.0076\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 2s 600ms/step - loss: 0.0037 - mae: 0.0437 - rmsle_cust: 0.0040 - val_loss: 0.0103 - val_mae: 0.0712 - val_rmsle_cust: 0.0076\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 2s 813ms/step - loss: 0.0037 - mae: 0.0433 - rmsle_cust: 0.0040 - val_loss: 0.0103 - val_mae: 0.0716 - val_rmsle_cust: 0.0075\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 2s 597ms/step - loss: 0.0033 - mae: 0.0412 - rmsle_cust: 0.0039 - val_loss: 0.0102 - val_mae: 0.0717 - val_rmsle_cust: 0.0075\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 2s 700ms/step - loss: 0.0033 - mae: 0.0417 - rmsle_cust: 0.0037 - val_loss: 0.0102 - val_mae: 0.0710 - val_rmsle_cust: 0.0075\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 2s 584ms/step - loss: 0.0031 - mae: 0.0402 - rmsle_cust: 0.0037 - val_loss: 0.0103 - val_mae: 0.0713 - val_rmsle_cust: 0.0074\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 2s 702ms/step - loss: 0.0032 - mae: 0.0408 - rmsle_cust: 0.0035 - val_loss: 0.0102 - val_mae: 0.0710 - val_rmsle_cust: 0.0073\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 2s 774ms/step - loss: 0.0030 - mae: 0.0398 - rmsle_cust: 0.0035 - val_loss: 0.0102 - val_mae: 0.0708 - val_rmsle_cust: 0.0073\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 2s 679ms/step - loss: 0.0031 - mae: 0.0404 - rmsle_cust: 0.0036 - val_loss: 0.0102 - val_mae: 0.0706 - val_rmsle_cust: 0.0072\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 2s 689ms/step - loss: 0.0029 - mae: 0.0389 - rmsle_cust: 0.0033 - val_loss: 0.0102 - val_mae: 0.0706 - val_rmsle_cust: 0.0072\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 2s 609ms/step - loss: 0.0029 - mae: 0.0390 - rmsle_cust: 0.0034 - val_loss: 0.0102 - val_mae: 0.0705 - val_rmsle_cust: 0.0072\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 2s 859ms/step - loss: 0.0027 - mae: 0.0377 - rmsle_cust: 0.0033 - val_loss: 0.0101 - val_mae: 0.0703 - val_rmsle_cust: 0.0071\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 2s 756ms/step - loss: 0.0027 - mae: 0.0379 - rmsle_cust: 0.0033 - val_loss: 0.0101 - val_mae: 0.0699 - val_rmsle_cust: 0.0071\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 2s 635ms/step - loss: 0.0026 - mae: 0.0376 - rmsle_cust: 0.0032 - val_loss: 0.0101 - val_mae: 0.0697 - val_rmsle_cust: 0.0071\n",
      "11/11 [==============================] - 1s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, random_state=1, shuffle=True) \n",
    "klstm_output = []\n",
    "for train_index, test_index in kf.split(data_cleaned):\n",
    "#     print(\"Train:\", train_index, \"Validation:\",test_index)\n",
    "    KX_train, KX_test = data_cleaned.iloc[train_index], data_cleaned.iloc[test_index]\n",
    "    KXX_train = get_keras_data(KX_train)\n",
    "    KXX_test = get_keras_data(KX_test)\n",
    "    lstm_model = get_lstm_model(KXX_train)\n",
    "    lstm_model.fit(KXX_train, KX_train.target, epochs=epochs, batch_size=BATCH_SIZE\n",
    "          , validation_data=(KXX_test, KX_test.target)\n",
    "          , verbose=1, callbacks=[history])\n",
    "    val_preds = lstm_model.predict(KXX_test)\n",
    "    val_preds = target_scaler.inverse_transform(val_preds)\n",
    "    val_preds = np.exp(val_preds)+1\n",
    "\n",
    "    # mean_absolute_error, mean_squared_log_error.\n",
    "    y_true = np.array(KX_test.mrp.values)\n",
    "    y_pred = val_preds[:,0]\n",
    "    v_rmsle = rmsle(y_true, y_pred)\n",
    "    v_rmse = mean_squared_error(y_true, y_pred , squared=False)\n",
    "    klstm_output.append([v_rmse, v_rmsle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RMSLE error for LSTM Based Model on Test Data using K-Fold Validation: 0.21941404938125872\n",
      " RMSE error for LSTM Based Model on Test Data using K-Fold Validation: 10.626755304320529\n"
     ]
    }
   ],
   "source": [
    "print(\" RMSLE error for LSTM Based Model on Test Data using K-Fold Validation: \"+str(sum([i[1] for i in klstm_output])/len(klstm_output)))\n",
    "print(\" RMSE error for LSTM Based Model on Test Data using K-Fold Validation: \"+str(sum([i[0] for i in klstm_output])/len(klstm_output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_47\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " item_name (InputLayer)         [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " item_desc (InputLayer)         [(None, 75)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_188 (Embedding)      (None, 10, 50)       195500      ['item_name[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_189 (Embedding)      (None, 75, 50)       195500      ['item_desc[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 10, 50)       2550        ['embedding_188[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 10, 50)       5050        ['embedding_188[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 10, 50)       7550        ['embedding_188[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 75, 50)       2550        ['embedding_189[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 75, 50)       5050        ['embedding_189[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 75, 50)       7550        ['embedding_189[0][0]']          \n",
      "                                                                                                  \n",
      " brand_name (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " product_category (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 50)          0           ['conv1d[0][0]']                 \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " global_max_pooling1d_2 (Global  (None, 50)          0           ['conv1d_2[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_4 (Global  (None, 50)          0           ['conv1d_4[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 50)          0           ['conv1d_1[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_3 (Global  (None, 50)          0           ['conv1d_3[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_5 (Global  (None, 50)          0           ['conv1d_5[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " embedding_190 (Embedding)      (None, 1, 10)        160         ['brand_name[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_191 (Embedding)      (None, 1, 10)        160         ['product_category[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_47 (Concatenate)   (None, 150)          0           ['global_max_pooling1d[0][0]',   \n",
      "                                                                  'global_max_pooling1d_2[0][0]', \n",
      "                                                                  'global_max_pooling1d_4[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_48 (Concatenate)   (None, 150)          0           ['global_max_pooling1d_1[0][0]', \n",
      "                                                                  'global_max_pooling1d_3[0][0]', \n",
      "                                                                  'global_max_pooling1d_5[0][0]'] \n",
      "                                                                                                  \n",
      " flatten_94 (Flatten)           (None, 10)           0           ['embedding_190[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_95 (Flatten)           (None, 10)           0           ['embedding_191[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_49 (Concatenate)   (None, 320)          0           ['concatenate_47[0][0]',         \n",
      "                                                                  'concatenate_48[0][0]',         \n",
      "                                                                  'flatten_94[0][0]',             \n",
      "                                                                  'flatten_95[0][0]']             \n",
      "                                                                                                  \n",
      " dense_141 (Dense)              (None, 256)          82176       ['concatenate_49[0][0]']         \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 256)          0           ['dense_141[0][0]']              \n",
      "                                                                                                  \n",
      " dense_142 (Dense)              (None, 128)          32896       ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 128)          0           ['dense_142[0][0]']              \n",
      "                                                                                                  \n",
      " dense_143 (Dense)              (None, 64)           8256        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 64)           0           ['dense_143[0][0]']              \n",
      "                                                                                                  \n",
      " dense_144 (Dense)              (None, 1)            65          ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 545,013\n",
      "Trainable params: 545,013\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_model(data, lr=0.001, decay=0.0):\n",
    "\n",
    "    # Inputs\n",
    "    item_name = Input(shape=[data[\"item_name\"].shape[1]], name=\"item_name\")\n",
    "    item_desc = Input(shape=[data[\"item_desc\"].shape[1]], name=\"item_desc\")\n",
    "    brand_name = Input(shape=[1], name=\"brand_name\")\n",
    "    product_category = Input(shape=[1], name=\"product_category\")\n",
    "\n",
    "    # Embeddings layers\n",
    "    emb_item_name = Embedding(MAX_TEXT, 50)(item_name)\n",
    "    emb_item_desc = Embedding(MAX_TEXT, 50)(item_desc)\n",
    "    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n",
    "    emb_product_category = Embedding(MAX_CAT, 10)(product_category)\n",
    "\n",
    "    convs1 = []\n",
    "    convs2 = []\n",
    "    \n",
    "    for filter_length in [1,2,3]:\n",
    "        cnn_layer1 = Conv1D(filters=50, kernel_size=filter_length, padding='same', activation='relu', strides=1) (emb_item_name)\n",
    "        cnn_layer2 = Conv1D(filters=50, kernel_size=filter_length, padding='same', activation='relu', strides=1) (emb_item_desc)\n",
    "        \n",
    "        maxpool1 = GlobalMaxPooling1D() (cnn_layer1)\n",
    "        maxpool2 = GlobalMaxPooling1D() (cnn_layer2)\n",
    "        \n",
    "        convs1.append(maxpool1)\n",
    "        convs2.append(maxpool2)\n",
    "\n",
    "    convs1 = concatenate(convs1)\n",
    "    convs2 = concatenate(convs2)\n",
    "    \n",
    "    # main layers\n",
    "    main_l = concatenate([\n",
    "        convs1,\n",
    "        convs2,\n",
    "        Flatten() (emb_brand_name), \n",
    "        Flatten() (emb_product_category)\n",
    "    ])\n",
    "\n",
    "    main_l = Dense(256)(main_l)\n",
    "    main_l = Activation('elu')(main_l)\n",
    "\n",
    "    main_l = Dense(128)(main_l)\n",
    "    main_l = Activation('elu')(main_l)\n",
    "\n",
    "    main_l = Dense(64)(main_l)\n",
    "    main_l = Activation('elu')(main_l)\n",
    "\n",
    "    # the output layer.\n",
    "    output = Dense(1, activation=\"linear\") (main_l)\n",
    "\n",
    "    model = Model([item_name, item_desc, brand_name, product_category], output)\n",
    "\n",
    "#     optimizer = Adam(lr=lr, decay=decay)\n",
    "#     model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", rmsle_cust])\n",
    "\n",
    "    return model\n",
    "\n",
    "cnn = get_cnn_model(X_train)\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting CNN model to training data...\n",
      "Epoch 1/50\n",
      "2/2 - 4s - loss: 0.1401 - mae: 0.3020 - rmsle_cust: 0.0101 - val_loss: 0.0999 - val_mae: 0.2706 - val_rmsle_cust: 0.0077 - 4s/epoch - 2s/step\n",
      "Epoch 2/50\n",
      "2/2 - 2s - loss: 0.0919 - mae: 0.2525 - rmsle_cust: 0.0101 - val_loss: 0.0706 - val_mae: 0.2088 - val_rmsle_cust: 0.0077 - 2s/epoch - 1s/step\n",
      "Epoch 3/50\n",
      "2/2 - 2s - loss: 0.0772 - mae: 0.2209 - rmsle_cust: 0.0101 - val_loss: 0.0653 - val_mae: 0.2031 - val_rmsle_cust: 0.0077 - 2s/epoch - 1s/step\n",
      "Epoch 4/50\n",
      "2/2 - 2s - loss: 0.0640 - mae: 0.2022 - rmsle_cust: 0.0101 - val_loss: 0.0366 - val_mae: 0.1554 - val_rmsle_cust: 0.0077 - 2s/epoch - 930ms/step\n",
      "Epoch 5/50\n",
      "2/2 - 2s - loss: 0.0377 - mae: 0.1553 - rmsle_cust: 0.0101 - val_loss: 0.0470 - val_mae: 0.1813 - val_rmsle_cust: 0.0077 - 2s/epoch - 953ms/step\n",
      "Epoch 6/50\n",
      "2/2 - 2s - loss: 0.0450 - mae: 0.1716 - rmsle_cust: 0.0101 - val_loss: 0.0322 - val_mae: 0.1450 - val_rmsle_cust: 0.0077 - 2s/epoch - 1s/step\n",
      "Epoch 7/50\n",
      "2/2 - 2s - loss: 0.0288 - mae: 0.1305 - rmsle_cust: 0.0101 - val_loss: 0.0241 - val_mae: 0.1167 - val_rmsle_cust: 0.0077 - 2s/epoch - 1s/step\n",
      "Epoch 8/50\n",
      "2/2 - 3s - loss: 0.0224 - mae: 0.1137 - rmsle_cust: 0.0098 - val_loss: 0.0291 - val_mae: 0.1356 - val_rmsle_cust: 0.0075 - 3s/epoch - 1s/step\n",
      "Epoch 9/50\n",
      "2/2 - 3s - loss: 0.0254 - mae: 0.1295 - rmsle_cust: 0.0090 - val_loss: 0.0203 - val_mae: 0.1092 - val_rmsle_cust: 0.0074 - 3s/epoch - 1s/step\n",
      "Epoch 10/50\n",
      "2/2 - 3s - loss: 0.0160 - mae: 0.0968 - rmsle_cust: 0.0087 - val_loss: 0.0180 - val_mae: 0.1026 - val_rmsle_cust: 0.0073 - 3s/epoch - 1s/step\n",
      "Epoch 11/50\n",
      "2/2 - 3s - loss: 0.0153 - mae: 0.0912 - rmsle_cust: 0.0090 - val_loss: 0.0200 - val_mae: 0.1084 - val_rmsle_cust: 0.0072 - 3s/epoch - 1s/step\n",
      "Epoch 12/50\n",
      "2/2 - 3s - loss: 0.0163 - mae: 0.0942 - rmsle_cust: 0.0086 - val_loss: 0.0142 - val_mae: 0.0902 - val_rmsle_cust: 0.0071 - 3s/epoch - 1s/step\n",
      "Epoch 13/50\n",
      "2/2 - 2s - loss: 0.0101 - mae: 0.0732 - rmsle_cust: 0.0076 - val_loss: 0.0152 - val_mae: 0.0947 - val_rmsle_cust: 0.0072 - 2s/epoch - 1s/step\n",
      "Epoch 14/50\n",
      "2/2 - 3s - loss: 0.0103 - mae: 0.0792 - rmsle_cust: 0.0080 - val_loss: 0.0143 - val_mae: 0.0911 - val_rmsle_cust: 0.0071 - 3s/epoch - 1s/step\n",
      "Epoch 15/50\n",
      "2/2 - 2s - loss: 0.0088 - mae: 0.0717 - rmsle_cust: 0.0074 - val_loss: 0.0112 - val_mae: 0.0772 - val_rmsle_cust: 0.0063 - 2s/epoch - 1s/step\n",
      "Epoch 16/50\n",
      "2/2 - 2s - loss: 0.0062 - mae: 0.0542 - rmsle_cust: 0.0062 - val_loss: 0.0120 - val_mae: 0.0829 - val_rmsle_cust: 0.0063 - 2s/epoch - 1s/step\n",
      "Epoch 17/50\n",
      "2/2 - 2s - loss: 0.0070 - mae: 0.0612 - rmsle_cust: 0.0060 - val_loss: 0.0110 - val_mae: 0.0782 - val_rmsle_cust: 0.0061 - 2s/epoch - 1s/step\n",
      "Epoch 18/50\n",
      "2/2 - 3s - loss: 0.0056 - mae: 0.0534 - rmsle_cust: 0.0056 - val_loss: 0.0105 - val_mae: 0.0741 - val_rmsle_cust: 0.0057 - 3s/epoch - 1s/step\n",
      "Epoch 19/50\n",
      "2/2 - 3s - loss: 0.0049 - mae: 0.0500 - rmsle_cust: 0.0056 - val_loss: 0.0110 - val_mae: 0.0761 - val_rmsle_cust: 0.0057 - 3s/epoch - 1s/step\n",
      "Epoch 20/50\n",
      "2/2 - 2s - loss: 0.0050 - mae: 0.0523 - rmsle_cust: 0.0061 - val_loss: 0.0097 - val_mae: 0.0697 - val_rmsle_cust: 0.0055 - 2s/epoch - 1s/step\n",
      "Epoch 21/50\n",
      "2/2 - 3s - loss: 0.0038 - mae: 0.0430 - rmsle_cust: 0.0055 - val_loss: 0.0095 - val_mae: 0.0706 - val_rmsle_cust: 0.0053 - 3s/epoch - 1s/step\n",
      "Epoch 22/50\n",
      "2/2 - 3s - loss: 0.0037 - mae: 0.0441 - rmsle_cust: 0.0050 - val_loss: 0.0092 - val_mae: 0.0689 - val_rmsle_cust: 0.0051 - 3s/epoch - 1s/step\n",
      "Epoch 23/50\n",
      "2/2 - 2s - loss: 0.0034 - mae: 0.0413 - rmsle_cust: 0.0049 - val_loss: 0.0088 - val_mae: 0.0649 - val_rmsle_cust: 0.0052 - 2s/epoch - 1s/step\n",
      "Epoch 24/50\n",
      "2/2 - 3s - loss: 0.0028 - mae: 0.0366 - rmsle_cust: 0.0052 - val_loss: 0.0090 - val_mae: 0.0669 - val_rmsle_cust: 0.0053 - 3s/epoch - 1s/step\n",
      "Epoch 25/50\n",
      "2/2 - 3s - loss: 0.0028 - mae: 0.0390 - rmsle_cust: 0.0053 - val_loss: 0.0085 - val_mae: 0.0629 - val_rmsle_cust: 0.0049 - 3s/epoch - 1s/step\n",
      "Epoch 26/50\n",
      "2/2 - 3s - loss: 0.0023 - mae: 0.0324 - rmsle_cust: 0.0043 - val_loss: 0.0083 - val_mae: 0.0622 - val_rmsle_cust: 0.0048 - 3s/epoch - 1s/step\n",
      "Epoch 27/50\n",
      "2/2 - 2s - loss: 0.0022 - mae: 0.0320 - rmsle_cust: 0.0038 - val_loss: 0.0082 - val_mae: 0.0620 - val_rmsle_cust: 0.0049 - 2s/epoch - 1s/step\n",
      "Epoch 28/50\n",
      "2/2 - 2s - loss: 0.0020 - mae: 0.0299 - rmsle_cust: 0.0035 - val_loss: 0.0081 - val_mae: 0.0601 - val_rmsle_cust: 0.0047 - 2s/epoch - 1s/step\n",
      "Epoch 29/50\n",
      "2/2 - 3s - loss: 0.0017 - mae: 0.0275 - rmsle_cust: 0.0034 - val_loss: 0.0082 - val_mae: 0.0611 - val_rmsle_cust: 0.0046 - 3s/epoch - 1s/step\n",
      "Epoch 30/50\n",
      "2/2 - 2s - loss: 0.0017 - mae: 0.0285 - rmsle_cust: 0.0033 - val_loss: 0.0078 - val_mae: 0.0588 - val_rmsle_cust: 0.0046 - 2s/epoch - 1s/step\n",
      "Epoch 31/50\n",
      "2/2 - 3s - loss: 0.0014 - mae: 0.0245 - rmsle_cust: 0.0030 - val_loss: 0.0077 - val_mae: 0.0596 - val_rmsle_cust: 0.0047 - 3s/epoch - 1s/step\n",
      "Epoch 32/50\n",
      "2/2 - 2s - loss: 0.0014 - mae: 0.0252 - rmsle_cust: 0.0028 - val_loss: 0.0075 - val_mae: 0.0577 - val_rmsle_cust: 0.0045 - 2s/epoch - 1s/step\n",
      "Epoch 33/50\n",
      "2/2 - 3s - loss: 0.0012 - mae: 0.0222 - rmsle_cust: 0.0028 - val_loss: 0.0076 - val_mae: 0.0578 - val_rmsle_cust: 0.0045 - 3s/epoch - 2s/step\n",
      "Epoch 34/50\n",
      "2/2 - 2s - loss: 0.0012 - mae: 0.0227 - rmsle_cust: 0.0029 - val_loss: 0.0075 - val_mae: 0.0569 - val_rmsle_cust: 0.0044 - 2s/epoch - 1s/step\n",
      "Epoch 35/50\n",
      "2/2 - 2s - loss: 0.0010 - mae: 0.0204 - rmsle_cust: 0.0027 - val_loss: 0.0074 - val_mae: 0.0569 - val_rmsle_cust: 0.0044 - 2s/epoch - 1s/step\n",
      "Epoch 36/50\n",
      "2/2 - 2s - loss: 9.8469e-04 - mae: 0.0200 - rmsle_cust: 0.0024 - val_loss: 0.0074 - val_mae: 0.0569 - val_rmsle_cust: 0.0044 - 2s/epoch - 1s/step\n",
      "Epoch 37/50\n",
      "2/2 - 3s - loss: 9.0663e-04 - mae: 0.0189 - rmsle_cust: 0.0023 - val_loss: 0.0075 - val_mae: 0.0561 - val_rmsle_cust: 0.0043 - 3s/epoch - 1s/step\n",
      "Epoch 38/50\n",
      "2/2 - 2s - loss: 8.3789e-04 - mae: 0.0179 - rmsle_cust: 0.0023 - val_loss: 0.0076 - val_mae: 0.0563 - val_rmsle_cust: 0.0043 - 2s/epoch - 1s/step\n",
      "Epoch 39/50\n",
      "2/2 - 3s - loss: 7.9282e-04 - mae: 0.0175 - rmsle_cust: 0.0022 - val_loss: 0.0074 - val_mae: 0.0559 - val_rmsle_cust: 0.0043 - 3s/epoch - 1s/step\n",
      "Epoch 40/50\n",
      "2/2 - 3s - loss: 7.1964e-04 - mae: 0.0163 - rmsle_cust: 0.0021 - val_loss: 0.0074 - val_mae: 0.0562 - val_rmsle_cust: 0.0043 - 3s/epoch - 1s/step\n",
      "Epoch 41/50\n",
      "2/2 - 3s - loss: 6.8542e-04 - mae: 0.0162 - rmsle_cust: 0.0020 - val_loss: 0.0074 - val_mae: 0.0552 - val_rmsle_cust: 0.0043 - 3s/epoch - 2s/step\n",
      "Epoch 42/50\n",
      "2/2 - 3s - loss: 6.3130e-04 - mae: 0.0149 - rmsle_cust: 0.0019 - val_loss: 0.0074 - val_mae: 0.0552 - val_rmsle_cust: 0.0043 - 3s/epoch - 1s/step\n",
      "Epoch 43/50\n",
      "2/2 - 2s - loss: 5.9927e-04 - mae: 0.0148 - rmsle_cust: 0.0018 - val_loss: 0.0073 - val_mae: 0.0549 - val_rmsle_cust: 0.0043 - 2s/epoch - 1s/step\n",
      "Epoch 44/50\n",
      "2/2 - 3s - loss: 5.5820e-04 - mae: 0.0139 - rmsle_cust: 0.0017 - val_loss: 0.0073 - val_mae: 0.0547 - val_rmsle_cust: 0.0043 - 3s/epoch - 1s/step\n",
      "Epoch 45/50\n",
      "2/2 - 2s - loss: 5.3009e-04 - mae: 0.0135 - rmsle_cust: 0.0016 - val_loss: 0.0073 - val_mae: 0.0544 - val_rmsle_cust: 0.0042 - 2s/epoch - 1s/step\n",
      "Epoch 46/50\n",
      "2/2 - 3s - loss: 4.9860e-04 - mae: 0.0129 - rmsle_cust: 0.0016 - val_loss: 0.0073 - val_mae: 0.0542 - val_rmsle_cust: 0.0042 - 3s/epoch - 1s/step\n",
      "Epoch 47/50\n",
      "2/2 - 3s - loss: 4.6674e-04 - mae: 0.0123 - rmsle_cust: 0.0015 - val_loss: 0.0072 - val_mae: 0.0541 - val_rmsle_cust: 0.0042 - 3s/epoch - 1s/step\n",
      "Epoch 48/50\n",
      "2/2 - 3s - loss: 4.4313e-04 - mae: 0.0120 - rmsle_cust: 0.0014 - val_loss: 0.0072 - val_mae: 0.0539 - val_rmsle_cust: 0.0041 - 3s/epoch - 1s/step\n",
      "Epoch 49/50\n",
      "2/2 - 2s - loss: 4.1412e-04 - mae: 0.0113 - rmsle_cust: 0.0014 - val_loss: 0.0072 - val_mae: 0.0537 - val_rmsle_cust: 0.0041 - 2s/epoch - 1s/step\n",
      "Epoch 50/50\n",
      "2/2 - 3s - loss: 3.9706e-04 - mae: 0.0111 - rmsle_cust: 0.0014 - val_loss: 0.0072 - val_mae: 0.0535 - val_rmsle_cust: 0.0041 - 3s/epoch - 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23adee0d2e0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate learning rate decay.\n",
    "exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "steps = int(dtrain.shape[0] / BATCH_SIZE) * epochs\n",
    "lr_init, lr_fin = 0.007, 0.0005\n",
    "lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "\n",
    "cnn_model_ = get_cnn_model(X_train,lr=lr_init, decay=lr_decay)\n",
    "print(\"Fitting CNN model to training data...\")\n",
    "cnn_model_.fit(\n",
    "        X_train, dtrain.target, epochs=epochs, batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_test, dtest.target), verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 8ms/step\n",
      "RMSLE error for CNN Based Model on Test Data: 0.21543820861421722\n",
      "RMSE error for CNN Based Model on Test Data: 7.123950112172428\n"
     ]
    }
   ],
   "source": [
    "val_preds = cnn_model_.predict(X_test)\n",
    "val_preds = target_scaler.inverse_transform(val_preds)\n",
    "val_preds = np.exp(val_preds)+1\n",
    "\n",
    "# mean_absolute_error, mean_squared_log_error.\n",
    "y_true = np.array(dtest.mrp.values)\n",
    "y_pred = val_preds[:,0]\n",
    "v_rmsle = rmsle(y_true, y_pred)\n",
    "v_rmse = mean_squared_error(y_true, y_pred , squared=False)\n",
    "print(\"RMSLE error for CNN Based Model on Test Data: \"+str(v_rmsle))\n",
    "print(\"RMSE error for CNN Based Model on Test Data: \"+str(v_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 155ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>product_category_wide</th>\n",
       "      <th>mrp</th>\n",
       "      <th>clean_description</th>\n",
       "      <th>seq_description</th>\n",
       "      <th>seq_product_name</th>\n",
       "      <th>target</th>\n",
       "      <th>predicted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3619</th>\n",
       "      <td>logo to go low rise thong 631581</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>22.00</td>\n",
       "      <td>soft  sheer lace wraps around waist lightweigh...</td>\n",
       "      <td>[18, 72, 1, 1261, 521, 161, 264, 20, 8, 83, 90]</td>\n",
       "      <td>[35, 87, 65, 21, 16, 8, 1980]</td>\n",
       "      <td>-0.468620</td>\n",
       "      <td>25.409889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3444</th>\n",
       "      <td>lace plunge bra</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>35.00</td>\n",
       "      <td>elegant black plunge bra prettified gorgeous l...</td>\n",
       "      <td>[488, 133, 80, 2, 2973, 238, 1, 711, 109, 324,...</td>\n",
       "      <td>[1, 80, 2]</td>\n",
       "      <td>-0.268490</td>\n",
       "      <td>28.414982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517</th>\n",
       "      <td>classic mesh triangle bralette</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>54.00</td>\n",
       "      <td>classic silhouettes airy stretch mesh</td>\n",
       "      <td>[100, 682, 616, 34, 36]</td>\n",
       "      <td>[100, 36, 68, 19]</td>\n",
       "      <td>-0.079174</td>\n",
       "      <td>50.844784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>happy unlined bandeau bra</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>26.95</td>\n",
       "      <td>hello  happy bras  happiness feeling like real...</td>\n",
       "      <td>[240, 52, 92, 148, 269, 150, 12, 235, 360, 52,...</td>\n",
       "      <td>[52, 169, 312, 2]</td>\n",
       "      <td>-0.381549</td>\n",
       "      <td>26.519339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5635</th>\n",
       "      <td>b.provocative contrast-lace bra 951222</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>40.00</td>\n",
       "      <td>flattering sheer nude panels decorated intrica...</td>\n",
       "      <td>[215, 72, 393, 781, 999, 571, 1250, 325, 3311,...</td>\n",
       "      <td>[67, 686, 216, 1, 2, 3892]</td>\n",
       "      <td>-0.210396</td>\n",
       "      <td>40.111614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                product_name  brand_name  product_category_wide    mrp                                  clean_description                                    seq_description               seq_product_name    target  predicted_price\n",
       "3619        logo to go low rise thong 631581           6                      7  22.00  soft  sheer lace wraps around waist lightweigh...    [18, 72, 1, 1261, 521, 161, 264, 20, 8, 83, 90]  [35, 87, 65, 21, 16, 8, 1980] -0.468620        25.409889\n",
       "3444                         lace plunge bra          12                      4  35.00  elegant black plunge bra prettified gorgeous l...  [488, 133, 80, 2, 2973, 238, 1, 711, 109, 324,...                     [1, 80, 2] -0.268490        28.414982\n",
       "2517          classic mesh triangle bralette           6                      3  54.00             classic silhouettes airy stretch mesh                             [100, 682, 616, 34, 36]              [100, 36, 68, 19] -0.079174        50.844784\n",
       "3076               happy unlined bandeau bra           0                      4  26.95  hello  happy bras  happiness feeling like real...  [240, 52, 92, 148, 269, 150, 12, 235, 360, 52,...              [52, 169, 312, 2] -0.381549        26.519339\n",
       "5635  b.provocative contrast-lace bra 951222           1                      4  40.00  flattering sheer nude panels decorated intrica...  [215, 72, 393, 781, 999, 571, 1250, 325, 3311,...     [67, 686, 216, 1, 2, 3892] -0.210396        40.111614"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the price.\n",
    "preds = lstm_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "preds = target_scaler.inverse_transform(preds)\n",
    "preds = np.exp(preds)-1\n",
    "dtest[\"predicted_price\"] = preds\n",
    "dtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, random_state=1, shuffle=True) \n",
    "kcnn_output = []\n",
    "exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "lr_init, lr_fin = 0.007, 0.0005\n",
    "for train_index, test_index in kf.split(data_cleaned):\n",
    "#     print(\"Train:\", train_index, \"Validation:\",test_index)\n",
    "    KX_train, KX_test = data_cleaned.iloc[train_index], data_cleaned.iloc[test_index]\n",
    "    KXX_train = get_keras_data(KX_train)\n",
    "    KXX_test = get_keras_data(KX_test)\n",
    "\n",
    "    # Calculate learning rate decay.\n",
    "    steps = int(KX_train.shape[0] / BATCH_SIZE) * epochs\n",
    "    lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "    cnn_model_ = get_cnn_model(lr=lr_init, decay=lr_decay, KXX_train)\n",
    "    cnn_model_.fit(\n",
    "            KXX_train, KX_train.target, epochs=epochs, batch_size=BATCH_SIZE,\n",
    "            validation_data=(KXX_test, KX_test.target), verbose=2,\n",
    "    )\n",
    "    val_preds = cnn_model_.predict(X_test)\n",
    "    val_preds = target_scaler.inverse_transform(val_preds)\n",
    "    val_preds = np.exp(val_preds)+1\n",
    "\n",
    "    # mean_absolute_error, mean_squared_log_error.\n",
    "    y_true = np.array(dtest.mrp.values)\n",
    "    y_pred = val_preds[:,0]\n",
    "    v_rmsle = rmsle(y_true, y_pred)\n",
    "    v_rmse = mean_squared_error(y_true, y_pred , squared=False)\n",
    "    kcnn_output.append([v_rmse, v_rmsle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSLE error for CNN Based Model on Test Data using K-Fold Validations: \", sum([i[1] for i in klstm_output])/len(klstm_output))\n",
    "print(\"RMSE error for CNN Based Model on Test Data using K-Fold Validations: \", sum([i[1] for i in klstm_output])/len(klstm_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
